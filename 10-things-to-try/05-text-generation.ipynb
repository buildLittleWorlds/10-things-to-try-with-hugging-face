{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# 5. Text Generation\n",
    "\n",
    "**Estimated Time**: ~2 hours\n",
    "\n",
    "**Prerequisites**: Notebooks 1-4 (understanding of tokenization, pipelines, and encoder-decoder architecture from summarization)\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. **Understand** how autoregressive language models generate text one token at a time\n",
    "2. **Apply** different decoding strategies (greedy, beam search, sampling)\n",
    "3. **Control** creativity vs coherence using temperature, top-k, and top-p parameters\n",
    "4. **Prevent** repetitive text using repetition penalty and no-repeat n-gram settings\n",
    "5. **Build** a creative writing assistant with mood control"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Run this cell first. If you completed Notebooks 1-4, you already have the core packages ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 1: Conceptual Foundation\n",
    "\n",
    "## What is Text Generation?\n",
    "\n",
    "**In plain English**: Text generation is when a model writes new text by predicting one word at a time, using what it's already written to decide what comes next.\n",
    "\n",
    "**Technical definition**: Autoregressive text generation models predict the probability of the next token given all previous tokens, then sample or select from that distribution to extend the sequence.\n",
    "\n",
    "### Visual Example\n",
    "\n",
    "```\n",
    "PROMPT: \"The robot walked into the kitchen and\"\n",
    "\n",
    "Step 1: \"The robot walked into the kitchen and\" → [opened] (most likely next word)\n",
    "Step 2: \"The robot walked into the kitchen and opened\" → [the] \n",
    "Step 3: \"The robot walked into the kitchen and opened the\" → [refrigerator]\n",
    "Step 4: \"The robot walked into the kitchen and opened the refrigerator\" → [.]\n",
    "\n",
    "OUTPUT: \"The robot walked into the kitchen and opened the refrigerator.\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "### Text Generation vs Summarization\n",
    "\n",
    "| Aspect | Summarization (Notebook 4) | Text Generation (This Notebook) |\n",
    "|--------|---------------------------|----------------------------------|\n",
    "| **Goal** | Compress long text to short | Extend short text to longer |\n",
    "| **Architecture** | Encoder-Decoder | Decoder-only (typically) |\n",
    "| **Input** | Complete document | Prompt/starter text |\n",
    "| **Output** | Shorter, faithful summary | Creative continuation |\n",
    "| **Key Challenge** | Preserve key information | Maintain coherence & creativity |\n",
    "\n",
    "```\n",
    "SUMMARIZATION (Notebook 4):               TEXT GENERATION (This Notebook):\n",
    "┌────────────────────────┐                ┌────────────────────────┐\n",
    "│ Long input document    │                │ Short prompt           │\n",
    "│ (500 words)            │                │ (10 words)             │\n",
    "└───────────┬────────────┘                └───────────┬────────────┘\n",
    "            │ COMPRESS                                │ EXPAND\n",
    "            ▼                                         ▼\n",
    "┌────────────────────────┐                ┌────────────────────────┐\n",
    "│ Short summary          │                │ Long continuation      │\n",
    "│ (50 words)             │                │ (200+ words)           │\n",
    "└────────────────────────┘                └────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "### How Autoregressive Generation Works: Decoder-Only\n",
    "\n",
    "Unlike summarization's encoder-decoder, most text generation uses **decoder-only** models:\n",
    "\n",
    "```\n",
    "              DECODER-ONLY MODEL (e.g., GPT-2)\n",
    "    ┌──────────────────────────────────────────────────────┐\n",
    "    │                                                      │\n",
    "    │  \"The robot walked\" → [Predict next] → \"into\"       │\n",
    "    │                                                      │\n",
    "    │  \"The robot walked into\" → [Predict next] → \"the\"   │\n",
    "    │                                                      │\n",
    "    │  \"The robot walked into the\" → [Predict next] → ... │\n",
    "    │                                                      │\n",
    "    └──────────────────────────────────────────────────────┘\n",
    "\n",
    "Each token prediction uses ALL previous tokens as context.\n",
    "This is called \"autoregressive\" - each output depends on previous outputs.\n",
    "```\n",
    "\n",
    "Popular decoder-only models for text generation:\n",
    "- **GPT-2** (OpenAI): The classic open-source text generator\n",
    "- **GPT-Neo/GPT-J** (EleutherAI): Open-source GPT alternatives\n",
    "- **Llama/Mistral** (Meta/Mistral AI): Modern open-weight models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "### Connection to Previous Notebooks\n",
    "\n",
    "| Notebook | Architecture | Direction |\n",
    "|----------|--------------|----------|\n",
    "| 1-3 (MLM, NER, QA) | Encoder-only | Understanding |\n",
    "| 4 (Summarization) | Encoder-Decoder | Input → Shorter output |\n",
    "| **5 (Text Generation)** | **Decoder-only** | **Input → Longer output** |\n",
    "\n",
    "```\n",
    "ENCODER-ONLY (Notebooks 1-3):    ENCODER-DECODER (Notebook 4):    DECODER-ONLY (This Notebook):\n",
    "┌────────────────┐               ┌─────────┐ ┌─────────┐          ┌────────────────┐\n",
    "│    ENCODER     │               │ ENCODER │─│ DECODER │          │    DECODER     │\n",
    "│                │               │         │ │         │          │                │\n",
    "│ Bidirectional  │               │ Encode  │ │ Decode  │          │ Left-to-right  │\n",
    "│ context        │               │ input   │ │ output  │          │ generation     │\n",
    "└────────────────┘               └─────────┘ └─────────┘          └────────────────┘\n",
    "     Tasks:                           Tasks:                           Tasks:\n",
    "     - Fill masks                     - Summarization                  - Story writing\n",
    "     - NER                            - Translation                    - Code completion\n",
    "     - QA extraction                                                   - Chatbots\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "### The Core Challenge: Decoding Strategies\n",
    "\n",
    "At each step, the model outputs **probabilities for all possible next tokens**. How do we choose which one?\n",
    "\n",
    "```\n",
    "After \"The cat sat on the\":\n",
    "\n",
    "Token Probabilities:\n",
    "┌────────────┬───────────┐\n",
    "│ Token      │ Prob      │\n",
    "├────────────┼───────────┤\n",
    "│ mat        │ 0.35      │  ← Highest (greedy would pick this)\n",
    "│ floor      │ 0.25      │\n",
    "│ couch      │ 0.15      │\n",
    "│ bed        │ 0.10      │\n",
    "│ chair      │ 0.08      │\n",
    "│ ...        │ ...       │\n",
    "└────────────┴───────────┘\n",
    "\n",
    "Different strategies choose differently from this distribution!\n",
    "```\n",
    "\n",
    "| Strategy | Description | When to Use |\n",
    "|----------|-------------|-------------|\n",
    "| **Greedy** | Always pick highest probability | Predictable, deterministic |\n",
    "| **Beam Search** | Track multiple candidates | Balanced quality |\n",
    "| **Top-k Sampling** | Random from top k tokens | Creative, varied |\n",
    "| **Top-p (Nucleus)** | Random from smallest set summing to p | Creative, adaptive |\n",
    "| **Temperature** | Adjust probability sharpness | Control randomness |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "### Real-World Applications\n",
    "\n",
    "Text generation powers many practical applications:\n",
    "\n",
    "- **Creative Writing**: Story continuation, poetry, dialogue\n",
    "- **Code Completion**: GitHub Copilot, code suggestions\n",
    "- **Chatbots**: Conversational AI responses\n",
    "- **Content Creation**: Marketing copy, product descriptions\n",
    "- **Autocomplete**: Email suggestions, search queries\n",
    "- **Data Augmentation**: Generate synthetic training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "### Key Terminology\n",
    "\n",
    "| Term | Definition |\n",
    "|------|------------|\n",
    "| **Autoregressive** | Each output depends on all previous outputs |\n",
    "| **Decoding** | The process of converting model outputs to text |\n",
    "| **Temperature** | Controls randomness (lower = focused, higher = random) |\n",
    "| **Top-k** | Limits selection to the k most likely tokens |\n",
    "| **Top-p (Nucleus)** | Limits selection to tokens covering p probability mass |\n",
    "| **Beam Search** | Explores multiple candidate sequences simultaneously |\n",
    "| **Repetition Penalty** | Discourages the model from repeating tokens |\n",
    "| **Prompt** | The input text that starts/guides generation |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "### Check Your Understanding\n",
    "\n",
    "Before moving on, try to answer these questions (answers at the end):\n",
    "\n",
    "1. What does \"autoregressive\" mean in the context of text generation?\n",
    "   - A) The model generates all tokens simultaneously\n",
    "   - B) Each token prediction depends on all previous tokens\n",
    "   - C) The model only looks at the original prompt\n",
    "\n",
    "2. What does higher temperature do to text generation?\n",
    "   - A) Makes output more random and diverse\n",
    "   - B) Makes output more focused and deterministic\n",
    "   - C) Makes generation faster\n",
    "\n",
    "3. Which architecture do most text generation models use?\n",
    "   - A) Encoder-only (like BERT)\n",
    "   - B) Encoder-Decoder (like BART)\n",
    "   - C) Decoder-only (like GPT)\n",
    "\n",
    "4. What is the purpose of top-k sampling?\n",
    "   - A) Generate exactly k tokens\n",
    "   - B) Limit selection to the k most likely next tokens\n",
    "   - C) Run the model k times and average results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: Basic Implementation\n",
    "\n",
    "## Your First Text Generation Pipeline\n",
    "\n",
    "Let's create a text generation pipeline and write some continuations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a text generation pipeline\n",
    "# Using GPT-2, a classic and lightweight text generator\n",
    "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "\n",
    "# Simple prompt to continue\n",
    "prompt = \"In a distant galaxy, a young explorer discovered\"\n",
    "\n",
    "# Generate continuation\n",
    "result = generator(\n",
    "    prompt,\n",
    "    max_new_tokens=50,  # Generate up to 50 new tokens\n",
    "    num_return_sequences=1,  # Return 1 sequence\n",
    "    do_sample=True,  # Use sampling for variety\n",
    "    temperature=0.7,  # Moderate creativity\n",
    ")\n",
    "\n",
    "print(\"Prompt:\")\n",
    "print(f\"  {prompt}\")\n",
    "print(f\"\\n{'='*60}\\n\")\n",
    "print(\"Generated continuation:\")\n",
    "print(f\"  {result[0]['generated_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "### Understanding the Output\n",
    "\n",
    "The text generation pipeline returns a list of dictionaries, each containing:\n",
    "- `generated_text`: The complete text (prompt + generated continuation)\n",
    "\n",
    "Let's examine what was generated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the generation\n",
    "full_text = result[0]['generated_text']\n",
    "generated_only = full_text[len(prompt):]\n",
    "\n",
    "print(\"Generation Statistics:\")\n",
    "print(\"=\"*40)\n",
    "print(f\"  Prompt length:     {len(prompt.split())} words\")\n",
    "print(f\"  Generated length:  {len(generated_only.split())} words\")\n",
    "print(f\"  Total length:      {len(full_text.split())} words\")\n",
    "print(f\"\\nGenerated text only:\")\n",
    "print(f\"  {generated_only.strip()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "### Generating Multiple Sequences\n",
    "\n",
    "One powerful feature is generating multiple different continuations from the same prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate multiple continuations\n",
    "prompt = \"The secret to happiness is\"\n",
    "\n",
    "results = generator(\n",
    "    prompt,\n",
    "    max_new_tokens=30,\n",
    "    num_return_sequences=3,  # Generate 3 different continuations\n",
    "    do_sample=True,\n",
    "    temperature=0.8,\n",
    ")\n",
    "\n",
    "print(f\"Prompt: \\\"{prompt}\\\"\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i, result in enumerate(results, 1):\n",
    "    continuation = result['generated_text'][len(prompt):].strip()\n",
    "    print(f\"\\nVersion {i}:\")\n",
    "    print(f\"  ...{continuation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "### Different Types of Prompts\n",
    "\n",
    "Text generation works with many types of starting prompts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different prompt styles\n",
    "prompts = {\n",
    "    \"Story Opening\": \"Once upon a time, in a castle made of crystal,\",\n",
    "    \"News Headline\": \"Breaking: Scientists announce that\",\n",
    "    \"Technical\": \"To implement a binary search algorithm, first\",\n",
    "    \"Dialogue\": '\"I can\\'t believe you actually did it,\" she said.',\n",
    "    \"Question Start\": \"The most important question we must ask is:\",\n",
    "}\n",
    "\n",
    "print(\"Different Prompt Styles:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for style, prompt in prompts.items():\n",
    "    result = generator(\n",
    "        prompt,\n",
    "        max_new_tokens=25,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        pad_token_id=generator.tokenizer.eos_token_id,\n",
    "    )\n",
    "    \n",
    "    continuation = result[0]['generated_text'][len(prompt):].strip()\n",
    "    print(f\"\\n[{style}]\")\n",
    "    print(f\"  Prompt: {prompt}\")\n",
    "    print(f\"  Generated: ...{continuation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 1: Multiple Story Starters (Guided)\n",
    "\n",
    "**Difficulty**: Basic | **Time**: 10-15 minutes\n",
    "\n",
    "**Your task**: Create a function that generates multiple creative continuations for story prompts.\n",
    "\n",
    "### Step 1: Create a story generator function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_story_continuations(prompt, num_versions=3, length=50):\n",
    "    \"\"\"\n",
    "    Generate multiple story continuations from a prompt.\n",
    "    \n",
    "    Args:\n",
    "        prompt: The story starter\n",
    "        num_versions: How many different continuations to generate\n",
    "        length: Maximum tokens to generate\n",
    "        \n",
    "    Returns:\n",
    "        List of continuation strings (without the prompt)\n",
    "    \"\"\"\n",
    "    results = generator(\n",
    "        prompt,\n",
    "        max_new_tokens=length,\n",
    "        num_return_sequences=num_versions,\n",
    "        do_sample=True,\n",
    "        temperature=0.8,\n",
    "        top_k=50,\n",
    "        pad_token_id=generator.tokenizer.eos_token_id,\n",
    "    )\n",
    "    \n",
    "    continuations = []\n",
    "    for result in results:\n",
    "        continuation = result['generated_text'][len(prompt):].strip()\n",
    "        continuations.append(continuation)\n",
    "    \n",
    "    return continuations\n",
    "\n",
    "\n",
    "# Test the function\n",
    "test_prompt = \"The old lighthouse keeper had a secret that\"\n",
    "\n",
    "stories = generate_story_continuations(test_prompt, num_versions=3)\n",
    "\n",
    "print(f\"Story Prompt: \\\"{test_prompt}\\\"\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i, story in enumerate(stories, 1):\n",
    "    print(f\"\\n--- Version {i} ---\")\n",
    "    print(f\"...{story}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "### Step 2: Try different genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genre-specific prompts\n",
    "genre_prompts = {\n",
    "    \"Mystery\": \"Detective Morgan examined the bloody knife and realized\",\n",
    "    \"Sci-Fi\": \"The spaceship's AI suddenly announced,\",\n",
    "    \"Romance\": \"Their eyes met across the crowded room, and\",\n",
    "    \"Horror\": \"The door creaked open by itself, revealing\",\n",
    "}\n",
    "\n",
    "print(\"Genre-Based Story Generation:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for genre, prompt in genre_prompts.items():\n",
    "    continuations = generate_story_continuations(prompt, num_versions=2, length=40)\n",
    "    \n",
    "    print(f\"\\n[{genre.upper()}]\")\n",
    "    print(f\"Prompt: \\\"{prompt}\\\"\")\n",
    "    for i, cont in enumerate(continuations, 1):\n",
    "        print(f\"  Version {i}: ...{cont[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "### Step 3: Try your own prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Create your own story prompts and generate continuations\n",
    "\n",
    "my_prompt = \"Write your story starter here\"\n",
    "\n",
    "# Uncomment to run:\n",
    "# my_stories = generate_story_continuations(my_prompt, num_versions=3)\n",
    "# for i, story in enumerate(my_stories, 1):\n",
    "#     print(f\"\\nVersion {i}: ...{story}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 3: Intermediate Exploration\n",
    "\n",
    "## Decoding Strategies Deep Dive\n",
    "\n",
    "How we select the next token dramatically affects output quality. Let's explore each strategy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample prompt for experiments\n",
    "experiment_prompt = \"The future of artificial intelligence\"\n",
    "\n",
    "# GREEDY DECODING: Always pick the highest probability token\n",
    "print(\"DECODING STRATEGY COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Prompt: \\\"{experiment_prompt}\\\"\\n\")\n",
    "\n",
    "# Greedy (deterministic)\n",
    "greedy_result = generator(\n",
    "    experiment_prompt,\n",
    "    max_new_tokens=40,\n",
    "    do_sample=False,  # Greedy - no sampling\n",
    "    pad_token_id=generator.tokenizer.eos_token_id,\n",
    ")\n",
    "print(\"[GREEDY DECODING]\")\n",
    "print(\"  Always picks the highest probability token.\")\n",
    "print(\"  Deterministic - same output every time.\")\n",
    "print(f\"  Result: ...{greedy_result[0]['generated_text'][len(experiment_prompt):].strip()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEAM SEARCH: Explore multiple paths, keep best ones\n",
    "beam_result = generator(\n",
    "    experiment_prompt,\n",
    "    max_new_tokens=40,\n",
    "    num_beams=4,  # Explore 4 candidates at each step\n",
    "    do_sample=False,\n",
    "    early_stopping=True,\n",
    "    pad_token_id=generator.tokenizer.eos_token_id,\n",
    ")\n",
    "print(\"\\n[BEAM SEARCH (num_beams=4)]\")\n",
    "print(\"  Explores multiple candidate sequences.\")\n",
    "print(\"  Better quality than greedy, still deterministic.\")\n",
    "print(f\"  Result: ...{beam_result[0]['generated_text'][len(experiment_prompt):].strip()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOP-K SAMPLING: Random selection from top k tokens\n",
    "print(\"\\n[TOP-K SAMPLING (k=50)]\")\n",
    "print(\"  Randomly samples from the 50 most likely tokens.\")\n",
    "print(\"  Different output each time (stochastic).\")\n",
    "\n",
    "for i in range(2):\n",
    "    topk_result = generator(\n",
    "        experiment_prompt,\n",
    "        max_new_tokens=40,\n",
    "        do_sample=True,\n",
    "        top_k=50,\n",
    "        pad_token_id=generator.tokenizer.eos_token_id,\n",
    "    )\n",
    "    print(f\"  Run {i+1}: ...{topk_result[0]['generated_text'][len(experiment_prompt):].strip()[:80]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOP-P (NUCLEUS) SAMPLING: Sample from smallest set covering p probability\n",
    "print(\"\\n[TOP-P SAMPLING (p=0.9)]\")\n",
    "print(\"  Samples from smallest set of tokens covering 90% probability.\")\n",
    "print(\"  Adaptive - uses fewer tokens when model is confident.\")\n",
    "\n",
    "for i in range(2):\n",
    "    topp_result = generator(\n",
    "        experiment_prompt,\n",
    "        max_new_tokens=40,\n",
    "        do_sample=True,\n",
    "        top_p=0.9,\n",
    "        pad_token_id=generator.tokenizer.eos_token_id,\n",
    "    )\n",
    "    print(f\"  Run {i+1}: ...{topp_result[0]['generated_text'][len(experiment_prompt):].strip()[:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-30",
   "metadata": {},
   "source": [
    "### Understanding Temperature\n",
    "\n",
    "Temperature is the most intuitive creativity control. It adjusts how \"sharp\" the probability distribution is:\n",
    "\n",
    "```\n",
    "Original probabilities: [0.5, 0.3, 0.15, 0.05]\n",
    "\n",
    "Low temperature (0.3):  [0.85, 0.12, 0.028, 0.002]  ← More focused\n",
    "High temperature (1.5): [0.35, 0.28, 0.22, 0.15]    ← More uniform\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temperature comparison\n",
    "temp_prompt = \"The meaning of life is\"\n",
    "\n",
    "temperatures = [0.3, 0.7, 1.0, 1.5]\n",
    "\n",
    "print(\"TEMPERATURE COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Prompt: \\\"{temp_prompt}\\\"\\n\")\n",
    "\n",
    "for temp in temperatures:\n",
    "    result = generator(\n",
    "        temp_prompt,\n",
    "        max_new_tokens=35,\n",
    "        do_sample=True,\n",
    "        temperature=temp,\n",
    "        top_k=50,\n",
    "        pad_token_id=generator.tokenizer.eos_token_id,\n",
    "    )\n",
    "    \n",
    "    creativity = \"Very focused\" if temp < 0.5 else \"Balanced\" if temp < 1.0 else \"Creative\" if temp < 1.3 else \"Wild\"\n",
    "    continuation = result[0]['generated_text'][len(temp_prompt):].strip()\n",
    "    \n",
    "    print(f\"[Temperature = {temp}] ({creativity})\")\n",
    "    print(f\"  ...{continuation[:100]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-32",
   "metadata": {},
   "source": [
    "### Preventing Repetition\n",
    "\n",
    "A common problem with text generation is repetitive output. Several parameters help prevent this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repetition problem demonstration\n",
    "rep_prompt = \"I love pizza because pizza is\"\n",
    "\n",
    "print(\"REPETITION PREVENTION\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Prompt: \\\"{rep_prompt}\\\" (designed to encourage repetition)\\n\")\n",
    "\n",
    "# Without any repetition control\n",
    "no_control = generator(\n",
    "    rep_prompt,\n",
    "    max_new_tokens=50,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    pad_token_id=generator.tokenizer.eos_token_id,\n",
    ")\n",
    "print(\"[No Repetition Control]\")\n",
    "print(f\"  ...{no_control[0]['generated_text'][len(rep_prompt):].strip()}\\n\")\n",
    "\n",
    "# With repetition penalty\n",
    "with_penalty = generator(\n",
    "    rep_prompt,\n",
    "    max_new_tokens=50,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    repetition_penalty=1.2,  # Penalize repeated tokens\n",
    "    pad_token_id=generator.tokenizer.eos_token_id,\n",
    ")\n",
    "print(\"[Repetition Penalty = 1.2]\")\n",
    "print(f\"  ...{with_penalty[0]['generated_text'][len(rep_prompt):].strip()}\\n\")\n",
    "\n",
    "# With no_repeat_ngram_size\n",
    "with_ngram = generator(\n",
    "    rep_prompt,\n",
    "    max_new_tokens=50,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    no_repeat_ngram_size=3,  # No 3-gram can repeat\n",
    "    pad_token_id=generator.tokenizer.eos_token_id,\n",
    ")\n",
    "print(\"[No Repeat N-gram = 3]\")\n",
    "print(f\"  ...{with_ngram[0]['generated_text'][len(rep_prompt):].strip()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-34",
   "metadata": {},
   "source": [
    "### Parameter Reference Guide\n",
    "\n",
    "| Parameter | Range | Effect | Recommended |\n",
    "|-----------|-------|--------|-------------|\n",
    "| `temperature` | 0.1 - 2.0 | Creativity level | 0.7-0.9 for creative, 0.3-0.5 for focused |\n",
    "| `top_k` | 1 - 100 | Token pool size | 40-60 |\n",
    "| `top_p` | 0.1 - 1.0 | Dynamic token pool | 0.9-0.95 |\n",
    "| `repetition_penalty` | 1.0 - 2.0 | Discourage repeats | 1.1-1.3 |\n",
    "| `no_repeat_ngram_size` | 2 - 5 | Block n-gram repeats | 2-3 |\n",
    "| `num_beams` | 1 - 10 | Beam search width | 4-6 for quality |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-35",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 2: Temperature Exploration (Semi-guided)\n",
    "\n",
    "**Difficulty**: Intermediate | **Time**: 15-20 minutes\n",
    "\n",
    "**Your task**: Build a function that generates text at multiple temperatures and helps you understand the creativity-coherence tradeoff.\n",
    "\n",
    "**Hints**:\n",
    "1. Generate the same prompt at temperatures from 0.2 to 1.8\n",
    "2. Analyze the outputs for coherence and creativity\n",
    "3. Consider measuring vocabulary diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "def analyze_temperature_effect(prompt, temperatures, samples_per_temp=2):\n",
    "    \"\"\"\n",
    "    Generate text at multiple temperatures and analyze the outputs.\n",
    "    \n",
    "    Args:\n",
    "        prompt: The starting text\n",
    "        temperatures: List of temperature values to test\n",
    "        samples_per_temp: Number of samples per temperature\n",
    "        \n",
    "    Returns:\n",
    "        dict with temperature as key and analysis as value\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for temp in temperatures:\n",
    "        samples = []\n",
    "        all_words = []\n",
    "        \n",
    "        for _ in range(samples_per_temp):\n",
    "            result = generator(\n",
    "                prompt,\n",
    "                max_new_tokens=50,\n",
    "                do_sample=True,\n",
    "                temperature=temp,\n",
    "                top_k=50,\n",
    "                pad_token_id=generator.tokenizer.eos_token_id,\n",
    "            )\n",
    "            text = result[0]['generated_text'][len(prompt):].strip()\n",
    "            samples.append(text)\n",
    "            all_words.extend(text.lower().split())\n",
    "        \n",
    "        # Calculate vocabulary diversity\n",
    "        unique_words = len(set(all_words))\n",
    "        total_words = len(all_words)\n",
    "        diversity = unique_words / total_words if total_words > 0 else 0\n",
    "        \n",
    "        results[temp] = {\n",
    "            'samples': samples,\n",
    "            'diversity': diversity,\n",
    "            'unique_words': unique_words,\n",
    "            'total_words': total_words,\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# Test the function\n",
    "test_prompt = \"The robot looked at the sunset and thought about\"\n",
    "temps_to_test = [0.3, 0.5, 0.7, 1.0, 1.3, 1.6]\n",
    "\n",
    "analysis = analyze_temperature_effect(test_prompt, temps_to_test, samples_per_temp=2)\n",
    "\n",
    "print(\"TEMPERATURE ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Prompt: \\\"{test_prompt}\\\"\\n\")\n",
    "\n",
    "for temp, data in analysis.items():\n",
    "    print(f\"\\n[Temperature = {temp}]\")\n",
    "    print(f\"  Vocabulary diversity: {data['diversity']:.2%}\")\n",
    "    print(f\"  Unique/Total words: {data['unique_words']}/{data['total_words']}\")\n",
    "    for i, sample in enumerate(data['samples'], 1):\n",
    "        print(f\"  Sample {i}: ...{sample[:60]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the creativity-coherence tradeoff\n",
    "print(\"\\nCREATIVITY-COHERENCE TRADEOFF\")\n",
    "print(\"=\"*70)\n",
    "print(\"\")\n",
    "print(\"Temperature |  Diversity  |  Characteristic\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for temp, data in sorted(analysis.items()):\n",
    "    div = data['diversity']\n",
    "    \n",
    "    if temp < 0.5:\n",
    "        char = \"Very predictable, may repeat\"\n",
    "    elif temp < 0.8:\n",
    "        char = \"Balanced, good for most uses\"\n",
    "    elif temp < 1.1:\n",
    "        char = \"Creative, some surprises\"\n",
    "    elif temp < 1.4:\n",
    "        char = \"Highly creative, occasional nonsense\"\n",
    "    else:\n",
    "        char = \"Very random, often incoherent\"\n",
    "    \n",
    "    bar = '*' * int(div * 30)\n",
    "    print(f\"    {temp:.1f}     |  {bar:30s} | {char}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-38",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 4: Advanced Topics\n",
    "\n",
    "## Under the Hood: Token-by-Token Generation\n",
    "\n",
    "Let's see exactly how the model generates text one token at a time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and tokenizer separately for manual inspection\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "print(f\"Model: {model_name}\")\n",
    "print(f\"Vocabulary size: {tokenizer.vocab_size:,}\")\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step-by-step generation visualization\n",
    "prompt = \"The cat sat\"\n",
    "\n",
    "print(\"STEP-BY-STEP GENERATION\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Starting prompt: \\\"{prompt}\\\"\\n\")\n",
    "\n",
    "# Tokenize the prompt\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "print(f\"Step 0 - Tokenized prompt:\")\n",
    "print(f\"  Token IDs: {input_ids[0].tolist()}\")\n",
    "print(f\"  Tokens: {[tokenizer.decode([t]) for t in input_ids[0]]}\")\n",
    "\n",
    "# Generate tokens one at a time\n",
    "current_ids = input_ids.clone()\n",
    "generated_tokens = []\n",
    "\n",
    "print(\"\\nGenerating tokens one at a time:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for step in range(8):  # Generate 8 tokens\n",
    "    with torch.no_grad():\n",
    "        outputs = model(current_ids)\n",
    "        logits = outputs.logits\n",
    "    \n",
    "    # Get probabilities for the next token\n",
    "    next_token_logits = logits[0, -1, :]\n",
    "    probs = torch.softmax(next_token_logits, dim=-1)\n",
    "    \n",
    "    # Get top 5 candidates\n",
    "    top_probs, top_indices = torch.topk(probs, 5)\n",
    "    \n",
    "    print(f\"\\nStep {step + 1}:\")\n",
    "    print(f\"  Current text: \\\"{tokenizer.decode(current_ids[0])}\\\"\")\n",
    "    print(f\"  Top 5 candidates:\")\n",
    "    for prob, idx in zip(top_probs, top_indices):\n",
    "        token = tokenizer.decode([idx])\n",
    "        marker = \" ← SELECTED\" if idx == top_indices[0] else \"\"\n",
    "        print(f\"    \\\"{token}\\\" ({prob:.1%}){marker}\")\n",
    "    \n",
    "    # Select the highest probability token (greedy)\n",
    "    next_token = top_indices[0].unsqueeze(0).unsqueeze(0)\n",
    "    current_ids = torch.cat([current_ids, next_token], dim=-1)\n",
    "    generated_tokens.append(tokenizer.decode([top_indices[0]]))\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"Final text: \\\"{tokenizer.decode(current_ids[0])}\\\"\")\n",
    "print(f\"Generated tokens: {generated_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-41",
   "metadata": {},
   "source": [
    "### How Temperature Affects Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize temperature's effect on probability distribution\n",
    "prompt = \"The weather today is\"\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids)\n",
    "    logits = outputs.logits[0, -1, :]\n",
    "\n",
    "print(\"TEMPERATURE EFFECT ON PROBABILITIES\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Prompt: \\\"{prompt}\\\"\\n\")\n",
    "\n",
    "temperatures = [0.3, 0.7, 1.0, 1.5]\n",
    "\n",
    "for temp in temperatures:\n",
    "    # Apply temperature\n",
    "    scaled_logits = logits / temp\n",
    "    probs = torch.softmax(scaled_logits, dim=-1)\n",
    "    \n",
    "    # Get top 5\n",
    "    top_probs, top_indices = torch.topk(probs, 5)\n",
    "    \n",
    "    print(f\"[Temperature = {temp}]\")\n",
    "    for prob, idx in zip(top_probs, top_indices):\n",
    "        token = tokenizer.decode([idx]).strip()\n",
    "        bar = '*' * int(prob * 40)\n",
    "        print(f\"  {token:12s} {prob:6.1%} {bar}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-43",
   "metadata": {},
   "source": [
    "### Constrained Generation\n",
    "\n",
    "Sometimes you want to control what the model generates more precisely:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using stopping criteria\n",
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "class StopOnPunctuation(StoppingCriteria):\n",
    "    \"\"\"Stop generation when we hit certain punctuation.\"\"\"\n",
    "    def __init__(self, tokenizer, stop_tokens):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.stop_tokens = stop_tokens\n",
    "    \n",
    "    def __call__(self, input_ids, scores, **kwargs):\n",
    "        last_token = self.tokenizer.decode(input_ids[0, -1])\n",
    "        return any(stop in last_token for stop in self.stop_tokens)\n",
    "\n",
    "\n",
    "# Generate until end of sentence\n",
    "stopper = StopOnPunctuation(tokenizer, ['.', '!', '?'])\n",
    "stopping_criteria = StoppingCriteriaList([stopper])\n",
    "\n",
    "prompt = \"The scientist discovered that\"\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "\n",
    "output = model.generate(\n",
    "    input_ids,\n",
    "    max_new_tokens=100,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    stopping_criteria=stopping_criteria,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "print(\"CONSTRAINED GENERATION (Stop at sentence end)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Prompt: \\\"{prompt}\\\"\")\n",
    "print(f\"Generated: {tokenizer.decode(output[0], skip_special_tokens=True)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-45",
   "metadata": {},
   "source": [
    "### Limitations of Text Generation\n",
    "\n",
    "| Limitation | Description | Mitigation |\n",
    "|------------|-------------|------------|\n",
    "| **Factual errors** | Models can generate plausible but false information | Verify facts, use for creative not factual tasks |\n",
    "| **Repetition** | Long generations may loop | Use repetition_penalty, no_repeat_ngram |\n",
    "| **Coherence drift** | Loses track of context over long generations | Use shorter generations, better prompts |\n",
    "| **Bias** | Reflects biases in training data | Be aware, review outputs |\n",
    "| **Context limit** | Limited by model's max context length | Truncate or summarize context |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-46",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 3: Controlled Generation (Independent)\n",
    "\n",
    "**Difficulty**: Advanced | **Time**: 15-20 minutes\n",
    "\n",
    "**Your task**: Build a class that generates text with different \"styles\" by adjusting parameters.\n",
    "\n",
    "**Requirements**:\n",
    "1. Support at least 3 generation styles (e.g., \"creative\", \"focused\", \"balanced\")\n",
    "2. Each style should have appropriate parameter settings\n",
    "3. Allow for custom parameter overrides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "class StyledTextGenerator:\n",
    "    \"\"\"Generate text with different style presets.\"\"\"\n",
    "    \n",
    "    STYLES = {\n",
    "        'focused': {\n",
    "            'description': 'Predictable, coherent output',\n",
    "            'params': {\n",
    "                'temperature': 0.3,\n",
    "                'top_k': 30,\n",
    "                'top_p': 0.85,\n",
    "                'repetition_penalty': 1.1,\n",
    "            }\n",
    "        },\n",
    "        'balanced': {\n",
    "            'description': 'Good balance of creativity and coherence',\n",
    "            'params': {\n",
    "                'temperature': 0.7,\n",
    "                'top_k': 50,\n",
    "                'top_p': 0.9,\n",
    "                'repetition_penalty': 1.2,\n",
    "            }\n",
    "        },\n",
    "        'creative': {\n",
    "            'description': 'More surprising and varied output',\n",
    "            'params': {\n",
    "                'temperature': 1.0,\n",
    "                'top_k': 80,\n",
    "                'top_p': 0.95,\n",
    "                'repetition_penalty': 1.3,\n",
    "            }\n",
    "        },\n",
    "        'wild': {\n",
    "            'description': 'Highly creative, may be incoherent',\n",
    "            'params': {\n",
    "                'temperature': 1.4,\n",
    "                'top_k': 100,\n",
    "                'top_p': 0.98,\n",
    "                'repetition_penalty': 1.4,\n",
    "            }\n",
    "        },\n",
    "    }\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize with the text generation pipeline.\"\"\"\n",
    "        self.generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "    \n",
    "    def list_styles(self):\n",
    "        \"\"\"List available styles.\"\"\"\n",
    "        print(\"Available Styles:\")\n",
    "        print(\"=\"*50)\n",
    "        for name, config in self.STYLES.items():\n",
    "            print(f\"  {name:12s} - {config['description']}\")\n",
    "    \n",
    "    def generate(self, prompt, style='balanced', max_tokens=50, **overrides):\n",
    "        \"\"\"\n",
    "        Generate text with a specific style.\n",
    "        \n",
    "        Args:\n",
    "            prompt: Starting text\n",
    "            style: One of the predefined styles\n",
    "            max_tokens: Maximum new tokens to generate\n",
    "            **overrides: Override specific parameters\n",
    "            \n",
    "        Returns:\n",
    "            dict with generated text and metadata\n",
    "        \"\"\"\n",
    "        if style not in self.STYLES:\n",
    "            raise ValueError(f\"Unknown style: {style}. Use list_styles() to see options.\")\n",
    "        \n",
    "        # Get style parameters and apply overrides\n",
    "        params = self.STYLES[style]['params'].copy()\n",
    "        params.update(overrides)\n",
    "        \n",
    "        # Generate\n",
    "        result = self.generator(\n",
    "            prompt,\n",
    "            max_new_tokens=max_tokens,\n",
    "            do_sample=True,\n",
    "            pad_token_id=self.generator.tokenizer.eos_token_id,\n",
    "            **params\n",
    "        )\n",
    "        \n",
    "        full_text = result[0]['generated_text']\n",
    "        continuation = full_text[len(prompt):].strip()\n",
    "        \n",
    "        return {\n",
    "            'prompt': prompt,\n",
    "            'style': style,\n",
    "            'continuation': continuation,\n",
    "            'full_text': full_text,\n",
    "            'params': params,\n",
    "        }\n",
    "    \n",
    "    def compare_styles(self, prompt, max_tokens=40):\n",
    "        \"\"\"Generate the same prompt with all styles for comparison.\"\"\"\n",
    "        results = {}\n",
    "        for style in self.STYLES:\n",
    "            results[style] = self.generate(prompt, style, max_tokens)\n",
    "        return results\n",
    "\n",
    "\n",
    "# Create the generator\n",
    "styled_gen = StyledTextGenerator()\n",
    "\n",
    "# List available styles\n",
    "styled_gen.list_styles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all styles on the same prompt\n",
    "test_prompt = \"In the year 2150, humanity had finally\"\n",
    "\n",
    "comparison = styled_gen.compare_styles(test_prompt)\n",
    "\n",
    "print(\"STYLE COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Prompt: \\\"{test_prompt}\\\"\\n\")\n",
    "\n",
    "for style, result in comparison.items():\n",
    "    desc = styled_gen.STYLES[style]['description']\n",
    "    print(f\"[{style.upper()}] - {desc}\")\n",
    "    print(f\"  Temperature: {result['params']['temperature']}\")\n",
    "    print(f\"  Output: ...{result['continuation'][:100]}...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try with custom overrides\n",
    "custom_result = styled_gen.generate(\n",
    "    \"The mysterious package contained\",\n",
    "    style='creative',\n",
    "    max_tokens=60,\n",
    "    temperature=0.9,  # Override the creative style's temperature\n",
    ")\n",
    "\n",
    "print(\"CUSTOM GENERATION (creative style with temperature override)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Prompt: \\\"{custom_result['prompt']}\\\"\")\n",
    "print(f\"Style: {custom_result['style']}\")\n",
    "print(f\"Parameters: {custom_result['params']}\")\n",
    "print(f\"\\nGenerated: ...{custom_result['continuation']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-50",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 5: Mini-Project\n",
    "\n",
    "## Project: Creative Writing Assistant\n",
    "\n",
    "**Scenario**: You're building a creative writing tool that helps authors overcome writer's block by generating story continuations with different moods.\n",
    "\n",
    "**Your goal**: Build a `CreativeWritingAssistant` class that:\n",
    "1. Generates story continuations with specified moods (mysterious, cheerful, tense, etc.)\n",
    "2. Provides multiple variations for each generation\n",
    "3. Allows mood blending (e.g., 70% mysterious, 30% cheerful)\n",
    "4. Estimates the \"mood match\" of generated text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MINI-PROJECT: Creative Writing Assistant\n",
    "# ========================================\n",
    "\n",
    "class CreativeWritingAssistant:\n",
    "    \"\"\"\n",
    "    Generates story continuations with mood control.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Mood configurations with generation parameters and seed words\n",
    "    MOODS = {\n",
    "        'mysterious': {\n",
    "            'description': 'Dark, enigmatic, suspenseful',\n",
    "            'params': {'temperature': 0.8, 'top_p': 0.92},\n",
    "            'seed_words': ['shadow', 'secret', 'hidden', 'whisper', 'unknown', 'strange', 'mysterious'],\n",
    "            'prompt_prefix': 'In a mysterious tone: ',\n",
    "        },\n",
    "        'cheerful': {\n",
    "            'description': 'Happy, optimistic, light-hearted',\n",
    "            'params': {'temperature': 0.9, 'top_p': 0.95},\n",
    "            'seed_words': ['bright', 'happy', 'joy', 'smile', 'laugh', 'wonderful', 'delightful'],\n",
    "            'prompt_prefix': 'In a cheerful tone: ',\n",
    "        },\n",
    "        'tense': {\n",
    "            'description': 'Suspenseful, urgent, dramatic',\n",
    "            'params': {'temperature': 0.7, 'top_p': 0.9},\n",
    "            'seed_words': ['sudden', 'heart', 'racing', 'danger', 'fear', 'quickly', 'desperate'],\n",
    "            'prompt_prefix': 'In a tense, dramatic tone: ',\n",
    "        },\n",
    "        'romantic': {\n",
    "            'description': 'Emotional, tender, passionate',\n",
    "            'params': {'temperature': 0.85, 'top_p': 0.93},\n",
    "            'seed_words': ['heart', 'love', 'gentle', 'tender', 'eyes', 'touch', 'beautiful'],\n",
    "            'prompt_prefix': 'In a romantic tone: ',\n",
    "        },\n",
    "        'melancholic': {\n",
    "            'description': 'Sad, reflective, nostalgic',\n",
    "            'params': {'temperature': 0.75, 'top_p': 0.9},\n",
    "            'seed_words': ['remember', 'lost', 'gone', 'faded', 'memory', 'sigh', 'alone'],\n",
    "            'prompt_prefix': 'In a melancholic, reflective tone: ',\n",
    "        },\n",
    "    }\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the writing assistant.\"\"\"\n",
    "        self.generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "    \n",
    "    def list_moods(self):\n",
    "        \"\"\"Display available moods.\"\"\"\n",
    "        print(\"Available Moods:\")\n",
    "        print(\"=\"*50)\n",
    "        for mood, config in self.MOODS.items():\n",
    "            print(f\"  {mood:12s} - {config['description']}\")\n",
    "    \n",
    "    def generate(self, story_start, mood='mysterious', num_variations=3, max_tokens=60):\n",
    "        \"\"\"\n",
    "        Generate story continuations with a specific mood.\n",
    "        \n",
    "        Args:\n",
    "            story_start: The beginning of the story\n",
    "            mood: The desired mood for continuation\n",
    "            num_variations: Number of different continuations to generate\n",
    "            max_tokens: Maximum new tokens per continuation\n",
    "            \n",
    "        Returns:\n",
    "            dict with variations and mood analysis\n",
    "        \"\"\"\n",
    "        if mood not in self.MOODS:\n",
    "            raise ValueError(f\"Unknown mood: {mood}. Use list_moods() to see options.\")\n",
    "        \n",
    "        mood_config = self.MOODS[mood]\n",
    "        \n",
    "        # Generate variations\n",
    "        variations = []\n",
    "        for _ in range(num_variations):\n",
    "            result = self.generator(\n",
    "                story_start,\n",
    "                max_new_tokens=max_tokens,\n",
    "                num_return_sequences=1,\n",
    "                do_sample=True,\n",
    "                repetition_penalty=1.2,\n",
    "                no_repeat_ngram_size=3,\n",
    "                pad_token_id=self.generator.tokenizer.eos_token_id,\n",
    "                **mood_config['params']\n",
    "            )\n",
    "            \n",
    "            continuation = result[0]['generated_text'][len(story_start):].strip()\n",
    "            mood_score = self._calculate_mood_match(continuation, mood)\n",
    "            \n",
    "            variations.append({\n",
    "                'text': continuation,\n",
    "                'mood_score': mood_score,\n",
    "                'word_count': len(continuation.split()),\n",
    "            })\n",
    "        \n",
    "        # Sort by mood match\n",
    "        variations.sort(key=lambda x: x['mood_score'], reverse=True)\n",
    "        \n",
    "        return {\n",
    "            'story_start': story_start,\n",
    "            'mood': mood,\n",
    "            'mood_description': mood_config['description'],\n",
    "            'variations': variations,\n",
    "        }\n",
    "    \n",
    "    def _calculate_mood_match(self, text, mood):\n",
    "        \"\"\"\n",
    "        Estimate how well the text matches the target mood.\n",
    "        Simple approach: count mood-related words.\n",
    "        \"\"\"\n",
    "        text_lower = text.lower()\n",
    "        seed_words = self.MOODS[mood]['seed_words']\n",
    "        \n",
    "        # Count matches\n",
    "        matches = sum(1 for word in seed_words if word in text_lower)\n",
    "        \n",
    "        # Normalize to 0-1 scale\n",
    "        max_possible = min(len(seed_words), len(text.split()) // 5)  # Reasonable max\n",
    "        score = matches / max(max_possible, 1)\n",
    "        \n",
    "        return min(score, 1.0)  # Cap at 1.0\n",
    "    \n",
    "    def blend_moods(self, story_start, primary_mood, secondary_mood, \n",
    "                    primary_weight=0.7, max_tokens=60):\n",
    "        \"\"\"\n",
    "        Generate with blended mood parameters.\n",
    "        \n",
    "        Args:\n",
    "            story_start: The beginning of the story\n",
    "            primary_mood: Main mood\n",
    "            secondary_mood: Secondary mood to blend in\n",
    "            primary_weight: Weight for primary mood (0-1)\n",
    "            max_tokens: Maximum new tokens\n",
    "            \n",
    "        Returns:\n",
    "            Generated continuation with blended mood\n",
    "        \"\"\"\n",
    "        if primary_mood not in self.MOODS or secondary_mood not in self.MOODS:\n",
    "            raise ValueError(\"Invalid mood specified.\")\n",
    "        \n",
    "        secondary_weight = 1 - primary_weight\n",
    "        \n",
    "        # Blend parameters\n",
    "        p1 = self.MOODS[primary_mood]['params']\n",
    "        p2 = self.MOODS[secondary_mood]['params']\n",
    "        \n",
    "        blended_params = {\n",
    "            'temperature': p1['temperature'] * primary_weight + p2['temperature'] * secondary_weight,\n",
    "            'top_p': p1['top_p'] * primary_weight + p2['top_p'] * secondary_weight,\n",
    "        }\n",
    "        \n",
    "        # Generate\n",
    "        result = self.generator(\n",
    "            story_start,\n",
    "            max_new_tokens=max_tokens,\n",
    "            do_sample=True,\n",
    "            repetition_penalty=1.2,\n",
    "            pad_token_id=self.generator.tokenizer.eos_token_id,\n",
    "            **blended_params\n",
    "        )\n",
    "        \n",
    "        continuation = result[0]['generated_text'][len(story_start):].strip()\n",
    "        \n",
    "        return {\n",
    "            'story_start': story_start,\n",
    "            'primary_mood': primary_mood,\n",
    "            'secondary_mood': secondary_mood,\n",
    "            'blend': f\"{primary_weight:.0%}/{secondary_weight:.0%}\",\n",
    "            'blended_params': blended_params,\n",
    "            'continuation': continuation,\n",
    "            'primary_score': self._calculate_mood_match(continuation, primary_mood),\n",
    "            'secondary_score': self._calculate_mood_match(continuation, secondary_mood),\n",
    "        }\n",
    "    \n",
    "    def format_output(self, result):\n",
    "        \"\"\"Format generation results for display.\"\"\"\n",
    "        lines = []\n",
    "        lines.append(\"=\"*70)\n",
    "        lines.append(f\"Story Start: \\\"{result['story_start']}\\\"\")\n",
    "        lines.append(f\"Target Mood: {result['mood']} ({result['mood_description']})\")\n",
    "        lines.append(\"=\"*70)\n",
    "        \n",
    "        for i, var in enumerate(result['variations'], 1):\n",
    "            lines.append(f\"\\n--- Variation {i} (mood match: {var['mood_score']:.0%}) ---\")\n",
    "            lines.append(f\"...{var['text']}\")\n",
    "        \n",
    "        return '\\n'.join(lines)\n",
    "\n",
    "\n",
    "# Create the assistant\n",
    "writer = CreativeWritingAssistant()\n",
    "\n",
    "# List available moods\n",
    "writer.list_moods()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate with a specific mood\n",
    "story_start = \"The old mansion stood at the end of the overgrown path, and\"\n",
    "\n",
    "result = writer.generate(\n",
    "    story_start,\n",
    "    mood='mysterious',\n",
    "    num_variations=3,\n",
    "    max_tokens=50\n",
    ")\n",
    "\n",
    "print(writer.format_output(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try different moods on the same story start\n",
    "print(\"\\nCOMPARING MOODS ON SAME STORY START\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "story = \"She opened the letter and read the first line:\"\n",
    "print(f\"Story: \\\"{story}\\\"\\n\")\n",
    "\n",
    "for mood in ['mysterious', 'cheerful', 'tense', 'romantic']:\n",
    "    result = writer.generate(story, mood=mood, num_variations=1, max_tokens=40)\n",
    "    best = result['variations'][0]\n",
    "    print(f\"[{mood.upper()}] ({result['mood_description']})\")\n",
    "    print(f\"  ...{best['text'][:100]}...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try mood blending\n",
    "print(\"\\nMOOD BLENDING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "story = \"As the sun set behind the mountains, she thought about\"\n",
    "\n",
    "blended = writer.blend_moods(\n",
    "    story,\n",
    "    primary_mood='romantic',\n",
    "    secondary_mood='melancholic',\n",
    "    primary_weight=0.6,\n",
    ")\n",
    "\n",
    "print(f\"Story: \\\"{blended['story_start']}\\\"\")\n",
    "print(f\"\\nMood Blend: {blended['primary_mood']} ({blended['blend'].split('/')[0]}) + {blended['secondary_mood']} ({blended['blend'].split('/')[1]})\")\n",
    "print(f\"Blended params: {blended['blended_params']}\")\n",
    "print(f\"\\nContinuation:\")\n",
    "print(f\"  ...{blended['continuation']}\")\n",
    "print(f\"\\nMood Scores:\")\n",
    "print(f\"  {blended['primary_mood']}: {blended['primary_score']:.0%}\")\n",
    "print(f\"  {blended['secondary_mood']}: {blended['secondary_score']:.0%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try your own story and mood\n",
    "# Uncomment and modify:\n",
    "\n",
    "# my_story = \"Your story start here\"\n",
    "# my_result = writer.generate(my_story, mood='cheerful', num_variations=3)\n",
    "# print(writer.format_output(my_result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-56",
   "metadata": {},
   "source": [
    "### Extension Ideas\n",
    "\n",
    "If you want to extend this project further:\n",
    "\n",
    "1. **Genre presets**: Add genre-specific settings (sci-fi, fantasy, thriller)\n",
    "2. **Character voice**: Generate dialogue in character-specific voices\n",
    "3. **Plot suggestions**: Generate plot twist ideas based on current story\n",
    "4. **Style analysis**: Analyze writing style and suggest improvements\n",
    "5. **Continuation chains**: Build longer stories by chaining generations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-57",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 6: Wrap-Up\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Autoregressive generation** predicts one token at a time, using previous tokens as context\n",
    "\n",
    "2. **Decoding strategies** dramatically affect output quality:\n",
    "   - Greedy: predictable but potentially repetitive\n",
    "   - Beam search: better quality, still deterministic\n",
    "   - Sampling (top-k, top-p): creative and varied\n",
    "\n",
    "3. **Temperature** is the key creativity control:\n",
    "   - Lower (0.3-0.5): focused, predictable\n",
    "   - Medium (0.7-0.9): balanced, recommended for most uses\n",
    "   - Higher (1.0+): creative but may become incoherent\n",
    "\n",
    "4. **Repetition prevention** is crucial for longer generations:\n",
    "   - repetition_penalty: discourages token repetition\n",
    "   - no_repeat_ngram_size: blocks n-gram repetition\n",
    "\n",
    "5. **Prompt engineering** significantly affects output quality and direction\n",
    "\n",
    "## Common Mistakes to Avoid\n",
    "\n",
    "| Mistake | Why It's a Problem |\n",
    "|---------|-------------------|\n",
    "| Very high temperature (>1.5) | Output becomes incoherent and random |\n",
    "| No repetition control | Long generations loop and repeat |\n",
    "| Treating output as factual | Models confidently generate false information |\n",
    "| Ignoring prompt design | Poor prompts lead to off-topic generations |\n",
    "\n",
    "## What's Next?\n",
    "\n",
    "In **Notebook 6: Zero-Shot Classification**, you'll learn:\n",
    "- How models classify text into categories they weren't trained on\n",
    "- The power of Natural Language Inference for classification\n",
    "- How to design effective label sets for your tasks\n",
    "\n",
    "This builds on text generation - both demonstrate the flexibility of language models beyond their original training!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-58",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Solutions\n",
    "\n",
    "### Check Your Understanding (Quiz Answers)\n",
    "\n",
    "1. **B) Each token prediction depends on all previous tokens** - This is what \"autoregressive\" means\n",
    "2. **A) Makes output more random and diverse** - Higher temperature flattens the probability distribution\n",
    "3. **C) Decoder-only (like GPT)** - Most text generation models use decoder-only architecture\n",
    "4. **B) Limit selection to the k most likely next tokens** - Reduces randomness while allowing variety\n",
    "\n",
    "### Exercise 2: Temperature Analysis (Key Insights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key insights from temperature exploration:\n",
    "\n",
    "# 1. Very low temperature (0.2-0.4):\n",
    "#    - Highly predictable output\n",
    "#    - Good for factual completions\n",
    "#    - May be repetitive\n",
    "\n",
    "# 2. Medium temperature (0.6-0.8):\n",
    "#    - Best balance for most use cases\n",
    "#    - Creative but coherent\n",
    "#    - Recommended default\n",
    "\n",
    "# 3. High temperature (1.0-1.3):\n",
    "#    - Good for brainstorming\n",
    "#    - More surprising word choices\n",
    "#    - May occasionally produce odd phrases\n",
    "\n",
    "# 4. Very high temperature (1.5+):\n",
    "#    - Often incoherent\n",
    "#    - Useful only for very experimental generation\n",
    "\n",
    "recommended_settings = {\n",
    "    'factual_completion': {'temperature': 0.3, 'top_p': 0.85},\n",
    "    'general_writing': {'temperature': 0.7, 'top_p': 0.9},\n",
    "    'creative_writing': {'temperature': 0.9, 'top_p': 0.95},\n",
    "    'brainstorming': {'temperature': 1.1, 'top_p': 0.98},\n",
    "}\n",
    "\n",
    "print(\"Recommended settings by use case:\")\n",
    "print(\"=\"*50)\n",
    "for use_case, params in recommended_settings.items():\n",
    "    print(f\"  {use_case:20s}: temp={params['temperature']}, top_p={params['top_p']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-60",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "- [Hugging Face Text Generation Docs](https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
    "- [GPT-2 Paper](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) - Language Models are Unsupervised Multitask Learners\n",
    "- [The Curious Case of Neural Text Degeneration](https://arxiv.org/abs/1904.09751) - Top-p (nucleus) sampling paper\n",
    "- [How to Generate Text](https://huggingface.co/blog/how-to-generate) - Excellent Hugging Face blog post\n",
    "- [CTRL Paper](https://arxiv.org/abs/1909.05858) - Conditional generation with control codes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
