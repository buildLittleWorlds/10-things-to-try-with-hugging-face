{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Fill-Mask: Predict Missing Words\n",
    "\n",
    "**Estimated Time**: ~2 hours\n",
    "\n",
    "**Prerequisites**: None (this is the first notebook)\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. **Explain** what masked language modeling (MLM) is and why it matters for NLP\n",
    "2. **Use** the Hugging Face `fill-mask` pipeline to predict missing words\n",
    "3. **Interpret** confidence scores and understand probability distributions\n",
    "4. **Compare** predictions from different BERT-family models\n",
    "5. **Identify** appropriate use cases for fill-mask models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Run this cell first to import required libraries. The first time you run fill-mask, it will download a model (~400MB) which is then cached for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForMaskedLM\n",
    "import torch\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 1: Conceptual Foundation\n",
    "\n",
    "## What is Masked Language Modeling?\n",
    "\n",
    "**In plain English**: Masked Language Modeling (MLM) is like a \"fill-in-the-blank\" test for AI. You give the model a sentence with a word hidden (masked), and it predicts what word should go there.\n",
    "\n",
    "**Technical definition**: MLM is a self-supervised pre-training objective where the model learns to predict randomly masked tokens based on their surrounding context.\n",
    "\n",
    "### The \"Cloze Test\" Analogy\n",
    "\n",
    "You've probably done fill-in-the-blank exercises in school:\n",
    "\n",
    "> \"The cat sat on the ____.\"\n",
    "\n",
    "Your brain uses context (\"cat\", \"sat\", \"on\", \"the\") to predict likely words: \"mat\", \"floor\", \"couch\", etc.\n",
    "\n",
    "MLM models do the same thing, but they've read billions of sentences and learned patterns about how words relate to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How BERT Was Trained\n",
    "\n",
    "BERT (Bidirectional Encoder Representations from Transformers) was trained on a massive corpus using this approach:\n",
    "\n",
    "1. **Take a sentence**: \"The cat sat on the mat.\"\n",
    "2. **Randomly mask ~15% of tokens**: \"The cat [MASK] on the mat.\"\n",
    "3. **Train the model to predict the masked word**: Model learns that \"sat\" fits here\n",
    "4. **Repeat billions of times** across Wikipedia and books\n",
    "\n",
    "### Why \"Bidirectional\" Matters\n",
    "\n",
    "```\n",
    "Left context:  \"The cat\"     ‚Üí  [MASK]  ‚Üê  \"on the mat\"  :Right context\n",
    "```\n",
    "\n",
    "Unlike older models that only read left-to-right (like GPT), BERT looks at words on **both sides** of the mask. This bidirectional context helps it make much better predictions.\n",
    "\n",
    "**Example**: In \"The bank was flooded after the [MASK] broke\", knowing \"flooded\" comes later helps predict \"dam\" (not \"robber\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real-World Applications\n",
    "\n",
    "Fill-mask models are the foundation for many NLP applications:\n",
    "\n",
    "- **Autocomplete**: Suggesting words as you type\n",
    "- **Grammar checking**: Identifying words that don't fit the context\n",
    "- **Data augmentation**: Generating variations of training data\n",
    "- **Transfer learning**: Pre-trained knowledge is fine-tuned for specific tasks\n",
    "\n",
    "### Key Terminology\n",
    "\n",
    "| Term | Definition |\n",
    "|------|------------|\n",
    "| **Token** | A piece of text (usually a word or subword) that the model processes |\n",
    "| **[MASK]** | The special token that tells BERT \"predict what goes here\" |\n",
    "| **Logits** | Raw model output scores (before converting to probabilities) |\n",
    "| **Softmax** | Function that converts logits to probabilities (0-1, summing to 1) |\n",
    "| **Top-k** | The k most likely predictions |\n",
    "| **Confidence score** | Probability assigned to a prediction (higher = more confident) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Your Understanding\n",
    "\n",
    "Before moving on, try to answer these questions (answers at the end of the notebook):\n",
    "\n",
    "1. Why is BERT called \"bidirectional\"?\n",
    "   - A) It can translate in both directions\n",
    "   - B) It looks at context on both sides of a masked word\n",
    "   - C) It was trained on two datasets\n",
    "\n",
    "2. What percentage of tokens were masked during BERT's training?\n",
    "   - A) 50%\n",
    "   - B) 15%\n",
    "   - C) 5%\n",
    "\n",
    "3. What does a higher confidence score mean?\n",
    "   - A) The model is more certain about its prediction\n",
    "   - B) The word is longer\n",
    "   - C) The prediction is always correct\n",
    "\n",
    "4. Which is NOT a use case for fill-mask models?\n",
    "   - A) Autocomplete\n",
    "   - B) Grammar checking\n",
    "   - C) Real-time translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: Basic Implementation\n",
    "\n",
    "## Your First Fill-Mask Pipeline\n",
    "\n",
    "Hugging Face provides a simple `pipeline` abstraction that handles all the complexity for you. Let's start with the most basic usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a fill-mask pipeline (uses bert-base-uncased by default)\n",
    "fill_mask = pipeline(\"fill-mask\")\n",
    "\n",
    "# Run prediction on a simple sentence\n",
    "# Note: The [MASK] token tells the model which word to predict\n",
    "result = fill_mask(\"The capital of France is [MASK].\")\n",
    "\n",
    "# Print the results\n",
    "print(\"Top predictions for: 'The capital of France is [MASK].'\\n\")\n",
    "for prediction in result:\n",
    "    print(f\"  {prediction['token_str']:12} ‚Üí {prediction['score']:.2%} confidence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Output\n",
    "\n",
    "The pipeline returns a list of dictionaries, each containing:\n",
    "\n",
    "- `token_str`: The predicted word\n",
    "- `score`: Confidence score (probability between 0 and 1)\n",
    "- `token`: The token ID (internal representation)\n",
    "- `sequence`: The full sentence with the prediction filled in\n",
    "\n",
    "Let's examine one prediction in detail:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the first (highest confidence) prediction in detail\n",
    "top_prediction = result[0]\n",
    "\n",
    "print(\"Detailed view of top prediction:\")\n",
    "print(f\"  Predicted word: '{top_prediction['token_str']}'\")\n",
    "print(f\"  Confidence:     {top_prediction['score']:.4f} ({top_prediction['score']:.2%})\")\n",
    "print(f\"  Token ID:       {top_prediction['token']}\")\n",
    "print(f\"  Full sentence:  '{top_prediction['sequence']}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How Context Changes Predictions\n",
    "\n",
    "The same [MASK] position can have completely different predictions based on surrounding words. Let's see this in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare predictions for similar sentences with different contexts\n",
    "sentences = [\n",
    "    \"The [MASK] is a popular pet.\",\n",
    "    \"The [MASK] is a wild animal.\",\n",
    "    \"The [MASK] is a farm animal.\",\n",
    "    \"The [MASK] is an endangered species.\"\n",
    "]\n",
    "\n",
    "print(\"Same position, different contexts:\\n\")\n",
    "for sentence in sentences:\n",
    "    predictions = fill_mask(sentence)\n",
    "    # Get top 3 predictions\n",
    "    top_3 = [f\"{p['token_str']}({p['score']:.0%})\" for p in predictions[:3]]\n",
    "    print(f\"'{sentence}'\")\n",
    "    print(f\"  ‚Üí {', '.join(top_3)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the predictions change based on context:\n",
    "- \"popular pet\" ‚Üí likely dogs, cats\n",
    "- \"wild animal\" ‚Üí likely lions, wolves\n",
    "- \"farm animal\" ‚Üí likely cows, pigs\n",
    "- \"endangered species\" ‚Üí likely tigers, rhinos\n",
    "\n",
    "This demonstrates that the model truly understands context, not just word frequency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Controlling the Number of Predictions\n",
    "\n",
    "By default, the pipeline returns 5 predictions. You can change this with the `top_k` parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get more predictions\n",
    "sentence = \"I love to eat [MASK] for breakfast.\"\n",
    "\n",
    "# Top 10 predictions\n",
    "results = fill_mask(sentence, top_k=10)\n",
    "\n",
    "print(f\"Top 10 predictions for: '{sentence}'\\n\")\n",
    "for i, pred in enumerate(results, 1):\n",
    "    print(f\"  {i:2}. {pred['token_str']:15} {pred['score']:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 1: Context Experiments (Guided)\n",
    "\n",
    "**Difficulty**: Basic | **Time**: 10-15 minutes\n",
    "\n",
    "**Your task**: Explore how different contexts affect predictions for the same masked position.\n",
    "\n",
    "### Step 1: Run the code below and observe the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The word \"bank\" has multiple meanings. Let's see if the model can distinguish them.\n",
    "\n",
    "# Context 1: Financial institution\n",
    "financial = fill_mask(\"I went to the bank to deposit my [MASK].\")\n",
    "print(\"Financial context: 'I went to the bank to deposit my [MASK].'\")\n",
    "print(f\"  Top prediction: {financial[0]['token_str']} ({financial[0]['score']:.2%})\\n\")\n",
    "\n",
    "# Context 2: River bank\n",
    "river = fill_mask(\"The children played by the river [MASK].\")\n",
    "print(\"River context: 'The children played by the river [MASK].'\")\n",
    "print(f\"  Top prediction: {river[0]['token_str']} ({river[0]['score']:.2%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Try your own experiments\n",
    "\n",
    "Create 3 sentences with [MASK] in the same position, but with different contexts that should lead to different predictions.\n",
    "\n",
    "**Example theme**: Professions\n",
    "- \"The [MASK] performed surgery on the patient.\" (expect: doctor, surgeon)\n",
    "- \"The [MASK] argued the case in court.\" (expect: lawyer, attorney)\n",
    "- \"The [MASK] designed the new building.\" (expect: architect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Create 3 sentences with different contexts\n",
    "\n",
    "my_sentences = [\n",
    "    # Replace these with your own sentences\n",
    "    \"The [MASK] performed surgery on the patient.\",\n",
    "    \"The [MASK] argued the case in court.\",\n",
    "    \"The [MASK] designed the new building.\"\n",
    "]\n",
    "\n",
    "# Run predictions\n",
    "for sentence in my_sentences:\n",
    "    result = fill_mask(sentence)\n",
    "    print(f\"'{sentence}'\")\n",
    "    print(f\"  ‚Üí {result[0]['token_str']} ({result[0]['score']:.2%})\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Reflection Questions\n",
    "\n",
    "After running your experiments, consider:\n",
    "1. Were the predictions what you expected?\n",
    "2. Did any predictions surprise you?\n",
    "3. Can you find a sentence where the model makes a clearly wrong prediction?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 3: Intermediate Exploration\n",
    "\n",
    "## Comparing Different Models\n",
    "\n",
    "BERT isn't the only masked language model. Let's compare predictions from different models to see how they differ.\n",
    "\n",
    "### Important: Different models use different mask tokens!\n",
    "\n",
    "| Model | Mask Token | Case-sensitive? |\n",
    "|-------|------------|----------------|\n",
    "| BERT (base-uncased) | [MASK] | No (lowercased) |\n",
    "| BERT (base-cased) | [MASK] | Yes |\n",
    "| RoBERTa | `<mask>` | Yes |\n",
    "| DistilBERT | [MASK] | No (uncased version) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pipelines for different models\n",
    "# Note: First run will download each model (~250-500MB each)\n",
    "\n",
    "print(\"Loading models (this may take a moment on first run)...\\n\")\n",
    "\n",
    "# BERT base (uncased - converts everything to lowercase)\n",
    "bert_uncased = pipeline(\"fill-mask\", model=\"bert-base-uncased\")\n",
    "\n",
    "# DistilBERT (smaller, faster, 97% of BERT's accuracy)\n",
    "distilbert = pipeline(\"fill-mask\", model=\"distilbert-base-uncased\")\n",
    "\n",
    "print(\"Models loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare predictions across models\n",
    "test_sentence = \"The scientist made an important [MASK] in the laboratory.\"\n",
    "\n",
    "print(f\"Sentence: '{test_sentence}'\\n\")\n",
    "\n",
    "models = [\n",
    "    (\"BERT (base-uncased)\", bert_uncased),\n",
    "    (\"DistilBERT\", distilbert),\n",
    "]\n",
    "\n",
    "for model_name, model_pipeline in models:\n",
    "    predictions = model_pipeline(test_sentence)\n",
    "    top_3 = [(p['token_str'], p['score']) for p in predictions[:3]]\n",
    "    print(f\"{model_name}:\")\n",
    "    for word, score in top_3:\n",
    "        print(f\"  {word:15} {score:.2%}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Model Differences\n",
    "\n",
    "Different models may give different predictions because:\n",
    "\n",
    "1. **Training data**: Models are trained on different text corpora\n",
    "2. **Model size**: Larger models generally capture more nuance\n",
    "3. **Architecture**: Different designs lead to different representations\n",
    "4. **Vocabulary**: Each model has its own tokenizer and vocabulary\n",
    "\n",
    "### Confidence Score Analysis\n",
    "\n",
    "Let's look at how confidence scores are distributed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get more predictions to see confidence distribution\n",
    "sentence = \"Python is a popular [MASK] language.\"\n",
    "predictions = fill_mask(sentence, top_k=15)\n",
    "\n",
    "print(f\"Confidence distribution for: '{sentence}'\\n\")\n",
    "print(\"Word            Confidence    Visual\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for pred in predictions:\n",
    "    # Create a visual bar\n",
    "    bar_length = int(pred['score'] * 50)\n",
    "    bar = '‚ñà' * bar_length\n",
    "    print(f\"{pred['token_str']:15} {pred['score']:6.2%}    {bar}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how confidence scores typically follow a **long-tail distribution**:\n",
    "- Top 1-2 predictions have high confidence\n",
    "- Confidence drops sharply for subsequent predictions\n",
    "- Many plausible words have very low confidence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Ambiguous Contexts\n",
    "\n",
    "Some sentences are genuinely ambiguous - multiple words would work equally well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examples of ambiguous vs. clear contexts\n",
    "\n",
    "# Ambiguous: Many words could fit\n",
    "ambiguous = \"I like to [MASK].\"\n",
    "amb_results = fill_mask(ambiguous, top_k=5)\n",
    "\n",
    "# Clear: Strong contextual constraints\n",
    "clear = \"Water freezes at zero degrees [MASK].\"\n",
    "clear_results = fill_mask(clear, top_k=5)\n",
    "\n",
    "print(\"AMBIGUOUS CONTEXT:\")\n",
    "print(f\"'{ambiguous}'\\n\")\n",
    "for p in amb_results:\n",
    "    print(f\"  {p['token_str']:10} {p['score']:.2%}\")\n",
    "\n",
    "print(f\"\\nConfidence gap (1st vs 2nd): {amb_results[0]['score'] - amb_results[1]['score']:.2%}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "print(\"CLEAR CONTEXT:\")\n",
    "print(f\"'{clear}'\\n\")\n",
    "for p in clear_results:\n",
    "    print(f\"  {p['token_str']:10} {p['score']:.2%}\")\n",
    "\n",
    "print(f\"\\nConfidence gap (1st vs 2nd): {clear_results[0]['score'] - clear_results[1]['score']:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **confidence gap** between 1st and 2nd predictions indicates how certain the model is:\n",
    "- **Large gap**: Model is confident in top prediction\n",
    "- **Small gap**: Multiple words are equally plausible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 2: Model Comparison (Semi-guided)\n",
    "\n",
    "**Difficulty**: Intermediate | **Time**: 15-20 minutes\n",
    "\n",
    "**Your task**: Find sentences where BERT and DistilBERT give notably different predictions.\n",
    "\n",
    "**Hints**:\n",
    "1. Try sentences with technical or specialized vocabulary\n",
    "2. Try sentences with cultural references\n",
    "3. Compare confidence scores, not just the top prediction\n",
    "\n",
    "**Expected output**: At least 2 sentences where the models disagree on the top prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Create a function to compare models on a sentence\n",
    "\n",
    "def compare_models(sentence):\n",
    "    \"\"\"Compare BERT and DistilBERT predictions for a sentence.\"\"\"\n",
    "    print(f\"Sentence: '{sentence}'\\n\")\n",
    "    \n",
    "    bert_pred = bert_uncased(sentence, top_k=3)\n",
    "    distil_pred = distilbert(sentence, top_k=3)\n",
    "    \n",
    "    print(\"BERT predictions:\")\n",
    "    for p in bert_pred:\n",
    "        print(f\"  {p['token_str']:15} {p['score']:.2%}\")\n",
    "    \n",
    "    print(\"\\nDistilBERT predictions:\")\n",
    "    for p in distil_pred:\n",
    "        print(f\"  {p['token_str']:15} {p['score']:.2%}\")\n",
    "    \n",
    "    # Check if top predictions differ\n",
    "    if bert_pred[0]['token_str'] != distil_pred[0]['token_str']:\n",
    "        print(\"\\n‚ö° Models DISAGREE on top prediction!\")\n",
    "    else:\n",
    "        print(\"\\n‚úì Models agree on top prediction\")\n",
    "    print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "# Test with your own sentences\n",
    "test_sentences = [\n",
    "    # Add your own sentences here\n",
    "    \"The quarterback threw the [MASK] for a touchdown.\",\n",
    "    \"The programmer fixed the [MASK] in the code.\",\n",
    "]\n",
    "\n",
    "for sentence in test_sentences:\n",
    "    compare_models(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 4: Advanced Topics\n",
    "\n",
    "## Under the Hood: What the Pipeline Actually Does\n",
    "\n",
    "The `pipeline` function is a convenience wrapper. Let's see what happens inside:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer and model separately\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_name)\n",
    "\n",
    "print(f\"Model: {model_name}\")\n",
    "print(f\"Vocabulary size: {tokenizer.vocab_size:,} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step-by-step: What happens when you call fill_mask()\n",
    "\n",
    "sentence = \"The capital of France is [MASK].\"\n",
    "print(f\"Input: '{sentence}'\\n\")\n",
    "\n",
    "# STEP 1: Tokenization\n",
    "# Convert text to token IDs\n",
    "inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "\n",
    "print(\"STEP 1 - Tokenization:\")\n",
    "print(f\"  Tokens: {tokens}\")\n",
    "print(f\"  Token IDs: {inputs['input_ids'][0].tolist()}\")\n",
    "\n",
    "# Find the mask position\n",
    "mask_token_id = tokenizer.mask_token_id\n",
    "mask_position = (inputs['input_ids'][0] == mask_token_id).nonzero(as_tuple=True)[0].item()\n",
    "print(f\"  [MASK] is at position: {mask_position}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2: Model Inference\n",
    "# Pass tokens through the model\n",
    "print(\"STEP 2 - Model Inference:\")\n",
    "\n",
    "with torch.no_grad():  # Disable gradient computation (faster, less memory)\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# outputs.logits shape: [batch_size, sequence_length, vocab_size]\n",
    "print(f\"  Output shape: {outputs.logits.shape}\")\n",
    "print(f\"  This means: {outputs.logits.shape[1]} positions √ó {outputs.logits.shape[2]:,} possible tokens\")\n",
    "\n",
    "# Get logits for the masked position\n",
    "mask_logits = outputs.logits[0, mask_position, :]\n",
    "print(f\"  Logits for [MASK] position: {mask_logits.shape} (one score per vocabulary token)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 3: Convert to Probabilities\n",
    "# Apply softmax to get probabilities\n",
    "print(\"STEP 3 - Softmax (logits ‚Üí probabilities):\")\n",
    "\n",
    "probabilities = torch.softmax(mask_logits, dim=0)\n",
    "\n",
    "print(f\"  Sum of all probabilities: {probabilities.sum():.4f} (should be ~1.0)\")\n",
    "print(f\"  Min probability: {probabilities.min():.2e}\")\n",
    "print(f\"  Max probability: {probabilities.max():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 4: Get Top Predictions\n",
    "print(\"STEP 4 - Top-k Selection:\")\n",
    "\n",
    "# Get top 5 predictions\n",
    "top_k = 5\n",
    "top_probs, top_indices = torch.topk(probabilities, top_k)\n",
    "\n",
    "print(f\"\\nTop {top_k} predictions:\")\n",
    "for prob, idx in zip(top_probs, top_indices):\n",
    "    token = tokenizer.decode([idx])\n",
    "    print(f\"  {token:15} {prob:.4f} ({prob:.2%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Considerations\n",
    "\n",
    "When using fill-mask models in production:\n",
    "\n",
    "| Consideration | Recommendation |\n",
    "|---------------|----------------|\n",
    "| **Batch processing** | Process multiple sentences at once for efficiency |\n",
    "| **Model size** | Use DistilBERT for speed (66% smaller, 60% faster) |\n",
    "| **GPU** | Use CUDA if available: `pipeline(\"fill-mask\", device=0)` |\n",
    "| **Caching** | Models are cached locally after first download |\n",
    "| **Memory** | Each model uses ~400MB-1GB RAM |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Batch processing multiple sentences\n",
    "sentences = [\n",
    "    \"The [MASK] jumped over the lazy dog.\",\n",
    "    \"I enjoy reading [MASK] in my free time.\",\n",
    "    \"The weather today is [MASK].\"\n",
    "]\n",
    "\n",
    "# Process all sentences at once (more efficient than one at a time)\n",
    "results = fill_mask(sentences)\n",
    "\n",
    "for sentence, prediction in zip(sentences, results):\n",
    "    top = prediction[0]  # Results are nested: list of lists\n",
    "    print(f\"'{sentence}'\")\n",
    "    print(f\"  ‚Üí {top['token_str']} ({top['score']:.2%})\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitations of Fill-Mask Models\n",
    "\n",
    "Fill-mask models have important limitations to be aware of:\n",
    "\n",
    "1. **Single mask prediction**: Most models predict one mask at a time. Multiple [MASK] tokens are predicted independently, not jointly.\n",
    "\n",
    "2. **Vocabulary constraints**: Can only predict words in the vocabulary. Rare words or new terms may not be predicted.\n",
    "\n",
    "3. **Biases**: Models inherit biases from training data (gender, racial, cultural biases).\n",
    "\n",
    "4. **No reasoning**: Models match patterns, not logic. They might predict grammatically correct but factually wrong answers.\n",
    "\n",
    "Let's see some of these limitations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limitation 1: Factual errors (pattern matching, not reasoning)\n",
    "print(\"LIMITATION: Pattern matching vs. reasoning\\n\")\n",
    "\n",
    "# The model might predict based on common patterns, not facts\n",
    "factual_test = \"The largest planet in our solar system is [MASK].\"\n",
    "result = fill_mask(factual_test)\n",
    "\n",
    "print(f\"'{factual_test}'\")\n",
    "print(f\"Top 3 predictions:\")\n",
    "for p in result[:3]:\n",
    "    correct = \"‚úì\" if p['token_str'].lower() == \"jupiter\" else \"‚úó\"\n",
    "    print(f\"  {correct} {p['token_str']} ({p['score']:.2%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limitation 2: Multiple masks are independent\n",
    "print(\"LIMITATION: Multiple masks are predicted independently\\n\")\n",
    "\n",
    "# Each [MASK] is filled without considering the other\n",
    "multi_mask = \"The [MASK] and the [MASK] are friends.\"\n",
    "result = fill_mask(multi_mask)\n",
    "\n",
    "print(f\"'{multi_mask}'\")\n",
    "print(\"\\nNote: The model fills each [MASK] independently.\")\n",
    "print(\"You might get 'dog' and 'dog' instead of two different animals.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 3: Word Rank Checker (Independent)\n",
    "\n",
    "**Difficulty**: Advanced | **Time**: 15-20 minutes\n",
    "\n",
    "**Your task**: Create a function that checks how \"natural\" a specific word is in a given sentence context.\n",
    "\n",
    "**Requirements**:\n",
    "1. Take a sentence and a word to check\n",
    "2. Mask the word's position\n",
    "3. Get predictions and find the rank of the target word\n",
    "4. Return the rank and probability\n",
    "\n",
    "**Example**:\n",
    "- Input: \"The cat sat on the mat\", word=\"mat\"\n",
    "- Expected: The word \"mat\" ranks in the top 10 (or not, which would be interesting!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "def check_word_naturalness(sentence, target_word, top_k=100):\n",
    "    \"\"\"\n",
    "    Check how natural a word is in a sentence context.\n",
    "    \n",
    "    Args:\n",
    "        sentence: A sentence containing the target word\n",
    "        target_word: The word to check\n",
    "        top_k: How many predictions to consider\n",
    "    \n",
    "    Returns:\n",
    "        dict with rank, probability, and whether it's in top predictions\n",
    "    \"\"\"\n",
    "    # TODO: Implement this function\n",
    "    # 1. Replace target_word with [MASK] in the sentence\n",
    "    # 2. Get predictions from fill_mask\n",
    "    # 3. Find the rank of target_word in predictions\n",
    "    # 4. Return results\n",
    "    \n",
    "    pass  # Remove this and add your code\n",
    "\n",
    "\n",
    "# Test your function\n",
    "test_cases = [\n",
    "    (\"The cat sat on the mat.\", \"mat\"),\n",
    "    (\"The cat sat on the mat.\", \"elephant\"),  # Should rank poorly\n",
    "    (\"Water freezes at zero degrees Celsius.\", \"Celsius\"),\n",
    "]\n",
    "\n",
    "# Uncomment to test after implementing:\n",
    "# for sentence, word in test_cases:\n",
    "#     result = check_word_naturalness(sentence, word)\n",
    "#     print(f\"'{word}' in '{sentence}'\")\n",
    "#     print(f\"  Rank: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 5: Mini-Project\n",
    "\n",
    "## Project: Word Fitness Scorer\n",
    "\n",
    "**Scenario**: You're building a writing assistant that helps users choose better words. When a user highlights a word in their text, the tool should tell them if the word fits well and suggest alternatives.\n",
    "\n",
    "**Your goal**: Build a `WordFitnessScorer` class that:\n",
    "1. Takes a sentence and a highlighted word\n",
    "2. Scores how well the word fits (based on its prediction rank)\n",
    "3. Suggests alternatives if the word doesn't rank highly\n",
    "4. Provides a human-readable assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MINI-PROJECT: Word Fitness Scorer\n",
    "# =================================\n",
    "\n",
    "class WordFitnessScorer:\n",
    "    \"\"\"\n",
    "    A tool that evaluates how well a word fits in a sentence\n",
    "    and suggests alternatives.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name=\"bert-base-uncased\"):\n",
    "        \"\"\"Initialize the scorer with a fill-mask pipeline.\"\"\"\n",
    "        # TODO 1: Create a fill-mask pipeline\n",
    "        self.fill_mask = pipeline(\"fill-mask\", model=model_name)\n",
    "        self.mask_token = \"[MASK]\"\n",
    "    \n",
    "    def _create_masked_sentence(self, sentence, target_word):\n",
    "        \"\"\"\n",
    "        Replace the target word with [MASK].\n",
    "        Handles case-insensitive matching.\n",
    "        \"\"\"\n",
    "        # TODO 2: Replace target_word with [MASK]\n",
    "        # Hint: Handle case sensitivity carefully\n",
    "        words = sentence.split()\n",
    "        masked_words = []\n",
    "        for word in words:\n",
    "            # Remove punctuation for comparison\n",
    "            clean_word = word.strip('.,!?;:')\n",
    "            if clean_word.lower() == target_word.lower():\n",
    "                # Preserve punctuation after the mask\n",
    "                punct = word[len(clean_word):]\n",
    "                masked_words.append(self.mask_token + punct)\n",
    "            else:\n",
    "                masked_words.append(word)\n",
    "        return ' '.join(masked_words)\n",
    "    \n",
    "    def score_word(self, sentence, target_word, num_alternatives=5):\n",
    "        \"\"\"\n",
    "        Score how well a word fits in the sentence.\n",
    "        \n",
    "        Args:\n",
    "            sentence: The complete sentence\n",
    "            target_word: The word to evaluate\n",
    "            num_alternatives: How many alternatives to suggest\n",
    "            \n",
    "        Returns:\n",
    "            dict with fitness score, rank, and alternatives\n",
    "        \"\"\"\n",
    "        # TODO 3: Create the masked sentence\n",
    "        masked = self._create_masked_sentence(sentence, target_word)\n",
    "        \n",
    "        # TODO 4: Get predictions (get more than we need to find the rank)\n",
    "        predictions = self.fill_mask(masked, top_k=50)\n",
    "        \n",
    "        # TODO 5: Find the rank of the target word\n",
    "        target_lower = target_word.lower()\n",
    "        rank = None\n",
    "        probability = 0.0\n",
    "        \n",
    "        for i, pred in enumerate(predictions):\n",
    "            if pred['token_str'].lower().strip() == target_lower:\n",
    "                rank = i + 1  # 1-indexed rank\n",
    "                probability = pred['score']\n",
    "                break\n",
    "        \n",
    "        # TODO 6: Calculate fitness score (simple approach: inverse of rank)\n",
    "        if rank:\n",
    "            fitness = 1.0 / rank\n",
    "        else:\n",
    "            fitness = 0.0  # Word not in top 50\n",
    "            rank = \">50\"\n",
    "        \n",
    "        # TODO 7: Get top alternatives\n",
    "        alternatives = [\n",
    "            {\"word\": p['token_str'], \"confidence\": p['score']}\n",
    "            for p in predictions[:num_alternatives]\n",
    "        ]\n",
    "        \n",
    "        return {\n",
    "            \"target_word\": target_word,\n",
    "            \"rank\": rank,\n",
    "            \"probability\": probability,\n",
    "            \"fitness_score\": fitness,\n",
    "            \"alternatives\": alternatives,\n",
    "            \"masked_sentence\": masked\n",
    "        }\n",
    "    \n",
    "    def get_assessment(self, sentence, target_word):\n",
    "        \"\"\"\n",
    "        Get a human-readable assessment of word fitness.\n",
    "        \n",
    "        Returns:\n",
    "            str: A formatted assessment message\n",
    "        \"\"\"\n",
    "        result = self.score_word(sentence, target_word)\n",
    "        \n",
    "        # TODO 8: Create human-readable assessment\n",
    "        assessment = []\n",
    "        assessment.append(f\"Word Fitness Assessment\")\n",
    "        assessment.append(f\"=\" * 50)\n",
    "        assessment.append(f\"Sentence: '{sentence}'\")\n",
    "        assessment.append(f\"Target word: '{target_word}'\")\n",
    "        assessment.append(f\"\")\n",
    "        \n",
    "        # Interpret the rank\n",
    "        rank = result['rank']\n",
    "        if isinstance(rank, int):\n",
    "            if rank == 1:\n",
    "                verdict = \"Excellent! This is the top predicted word.\"\n",
    "                emoji = \"üü¢\"\n",
    "            elif rank <= 5:\n",
    "                verdict = \"Good fit. The word is highly probable in this context.\"\n",
    "                emoji = \"üü¢\"\n",
    "            elif rank <= 15:\n",
    "                verdict = \"Acceptable. The word works but alternatives might be stronger.\"\n",
    "                emoji = \"üü°\"\n",
    "            else:\n",
    "                verdict = \"Unusual choice. Consider the alternatives below.\"\n",
    "                emoji = \"üü†\"\n",
    "        else:\n",
    "            verdict = \"This word is unexpected in this context. Strongly consider alternatives.\"\n",
    "            emoji = \"üî¥\"\n",
    "        \n",
    "        assessment.append(f\"Rank: #{rank}\")\n",
    "        assessment.append(f\"Probability: {result['probability']:.2%}\")\n",
    "        assessment.append(f\"\")\n",
    "        assessment.append(f\"{emoji} {verdict}\")\n",
    "        assessment.append(f\"\")\n",
    "        assessment.append(f\"Top alternatives:\")\n",
    "        \n",
    "        for i, alt in enumerate(result['alternatives'], 1):\n",
    "            assessment.append(f\"  {i}. {alt['word']} ({alt['confidence']:.2%})\")\n",
    "        \n",
    "        return '\\n'.join(assessment)\n",
    "\n",
    "\n",
    "# Test the Word Fitness Scorer\n",
    "print(\"Creating Word Fitness Scorer...\\n\")\n",
    "scorer = WordFitnessScorer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with various sentences\n",
    "test_cases = [\n",
    "    (\"The cat sat on the mat.\", \"mat\"),\n",
    "    (\"The cat sat on the mat.\", \"elephant\"),\n",
    "    (\"She decided to pursue a career in medicine.\", \"medicine\"),\n",
    "    (\"She decided to pursue a career in medicine.\", \"dancing\"),\n",
    "]\n",
    "\n",
    "for sentence, word in test_cases:\n",
    "    print(scorer.get_assessment(sentence, word))\n",
    "    print(\"\\n\" + \"=\"*60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extension Ideas\n",
    "\n",
    "If you want to extend this project further:\n",
    "\n",
    "1. **Batch processing**: Evaluate multiple words in a document\n",
    "2. **Contextual suggestions**: Suggest words that match the document's tone\n",
    "3. **Comparison mode**: Compare multiple candidate words\n",
    "4. **Synonym detection**: Check if the word has synonyms that rank higher\n",
    "5. **Writing style analysis**: Build a profile of a user's word choices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 6: Wrap-Up\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Masked Language Modeling** is a pre-training objective where models learn to predict hidden words based on context\n",
    "\n",
    "2. **Bidirectional context** (looking at words on both sides) is what makes BERT different from earlier models\n",
    "\n",
    "3. **Confidence scores** tell you how certain the model is - but high confidence doesn't mean the prediction is factually correct\n",
    "\n",
    "4. **Different models give different results** - model choice matters based on your use case\n",
    "\n",
    "5. **The pipeline hides complexity** - under the hood, it tokenizes text, runs inference, and processes outputs\n",
    "\n",
    "## Common Mistakes to Avoid\n",
    "\n",
    "| Mistake | Why It's a Problem |\n",
    "|---------|-------------------|\n",
    "| Using wrong mask token | RoBERTa uses `<mask>`, BERT uses `[MASK]` |\n",
    "| Trusting high confidence blindly | Models can be confidently wrong |\n",
    "| Ignoring case sensitivity | Uncased models convert everything to lowercase |\n",
    "| Processing one sentence at a time | Batching is much more efficient |\n",
    "\n",
    "## What's Next?\n",
    "\n",
    "In **Notebook 2: Named Entity Recognition**, you'll learn:\n",
    "- How to extract people, organizations, and locations from text\n",
    "- The same BERT architecture used here, but for a different task\n",
    "- How models can label entire sequences, not just single positions\n",
    "\n",
    "The concepts you learned about tokenization, model inference, and confidence scores will directly apply!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Solutions\n",
    "\n",
    "### Check Your Understanding (Quiz Answers)\n",
    "\n",
    "1. **B) It looks at context on both sides of a masked word**\n",
    "2. **B) 15%**\n",
    "3. **A) The model is more certain about its prediction**\n",
    "4. **C) Real-time translation** (Translation uses encoder-decoder models, not fill-mask)\n",
    "\n",
    "### Exercise 3: Word Rank Checker (Sample Solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample solution for Exercise 3\n",
    "\n",
    "def check_word_naturalness_solution(sentence, target_word, top_k=100):\n",
    "    \"\"\"\n",
    "    Check how natural a word is in a sentence context.\n",
    "    \"\"\"\n",
    "    # Create masked sentence\n",
    "    words = sentence.split()\n",
    "    masked_words = []\n",
    "    target_lower = target_word.lower()\n",
    "    \n",
    "    for word in words:\n",
    "        clean = word.strip('.,!?;:')\n",
    "        if clean.lower() == target_lower:\n",
    "            punct = word[len(clean):]\n",
    "            masked_words.append('[MASK]' + punct)\n",
    "        else:\n",
    "            masked_words.append(word)\n",
    "    \n",
    "    masked_sentence = ' '.join(masked_words)\n",
    "    \n",
    "    # Get predictions\n",
    "    predictions = fill_mask(masked_sentence, top_k=top_k)\n",
    "    \n",
    "    # Find rank of target word\n",
    "    rank = None\n",
    "    probability = 0.0\n",
    "    \n",
    "    for i, pred in enumerate(predictions):\n",
    "        if pred['token_str'].lower().strip() == target_lower:\n",
    "            rank = i + 1\n",
    "            probability = pred['score']\n",
    "            break\n",
    "    \n",
    "    return {\n",
    "        'word': target_word,\n",
    "        'rank': rank if rank else f'>{top_k}',\n",
    "        'probability': probability,\n",
    "        'masked_sentence': masked_sentence,\n",
    "        'in_top_10': rank is not None and rank <= 10\n",
    "    }\n",
    "\n",
    "# Test the solution\n",
    "print(\"Testing solution:\\n\")\n",
    "test_cases = [\n",
    "    (\"The cat sat on the mat.\", \"mat\"),\n",
    "    (\"The cat sat on the elephant.\", \"elephant\"),\n",
    "]\n",
    "\n",
    "for sentence, word in test_cases:\n",
    "    result = check_word_naturalness_solution(sentence, word)\n",
    "    print(f\"'{word}' in '{sentence}'\")\n",
    "    print(f\"  Rank: {result['rank']}, Probability: {result['probability']:.2%}\")\n",
    "    print(f\"  In top 10: {result['in_top_10']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "- [BERT Paper](https://arxiv.org/abs/1810.04805): Original BERT research paper\n",
    "- [Hugging Face fill-mask docs](https://huggingface.co/docs/transformers/main_classes/pipelines#transformers.FillMaskPipeline)\n",
    "- [BERT Model Card](https://huggingface.co/bert-base-uncased): Model details and limitations\n",
    "- [DistilBERT](https://huggingface.co/distilbert-base-uncased): Smaller, faster alternative"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
