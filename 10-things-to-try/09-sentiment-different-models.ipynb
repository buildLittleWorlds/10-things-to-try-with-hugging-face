{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 9. Sentiment Analysis with Different Models\n",
        "\n",
        "**Estimated Time**: ~2 hours\n",
        "\n",
        "**Prerequisites**: Notebooks 1-8 (especially Zero-Shot Classification and Text Similarity)\n",
        "\n",
        "---\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "By the end of this notebook, you will be able to:\n",
        "\n",
        "1. **Understand** why different models produce different sentiment results for the same text\n",
        "2. **Compare** binary sentiment (positive/negative) vs. multi-class sentiment (5-star ratings)\n",
        "3. **Analyze** how training data influences model behavior and biases\n",
        "4. **Read** model cards to understand model capabilities and limitations\n",
        "5. **Build** a multi-model sentiment dashboard that aggregates predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n",
        "\n",
        "Run this cell first. If you completed previous notebooks, you already have the core packages ready."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Core imports\n",
        "from transformers import pipeline, AutoModelForSequenceClassification, AutoTokenizer\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# Suppress warnings for cleaner output\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"Setup complete!\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Part 1: Conceptual Foundation\n",
        "\n",
        "## What is Sentiment Analysis?\n",
        "\n",
        "**In plain English**: Sentiment analysis determines the emotional tone or opinion expressed in text - is it positive, negative, or somewhere in between?\n",
        "\n",
        "**Technical definition**: Sentiment analysis is a text classification task that assigns emotional polarity or intensity scores to text, using models trained on labeled datasets of opinions and reviews.\n",
        "\n",
        "### Why Do Different Models Give Different Results?\n",
        "\n",
        "```\n",
        "THE OBSERVATION:\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚  Input: \"The movie was okay, I guess.\"                        â”‚\n",
        "â”‚                                                                â”‚\n",
        "â”‚  Model A (Movie Reviews):   Negative (70%)                    â”‚\n",
        "â”‚  Model B (Product Reviews): Neutral  (65%)                    â”‚\n",
        "â”‚  Model C (Social Media):    Positive (55%)                    â”‚\n",
        "â”‚                                                                â”‚\n",
        "â”‚  Same text, completely different results! Why?                â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "\n",
        "THE REASON: TRAINING DATA SHAPES MODEL BEHAVIOR\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚  Each model learned from different examples:                  â”‚\n",
        "â”‚                                                                â”‚\n",
        "â”‚  Model A: Movie reviews (1-10 stars â†’ binary)                 â”‚\n",
        "â”‚     \"okay\" in movie context often means disappointing        â”‚\n",
        "â”‚                                                                â”‚\n",
        "â”‚  Model B: Product reviews (1-5 stars â†’ 5 classes)             â”‚\n",
        "â”‚     \"okay\" maps to middle rating (3 stars)                    â”‚\n",
        "â”‚                                                                â”‚\n",
        "â”‚  Model C: Tweets (casual language)                            â”‚\n",
        "â”‚     \"okay\" can be understated positivity                      â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Types of Sentiment Classification\n",
        "\n",
        "```\n",
        "BINARY SENTIMENT:\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚  Two classes: POSITIVE or NEGATIVE                         â”‚\n",
        "â”‚                                                             â”‚\n",
        "â”‚  \"I loved this!\"     â†’ POSITIVE (98%)                      â”‚\n",
        "â”‚  \"Terrible waste.\"   â†’ NEGATIVE (95%)                      â”‚\n",
        "â”‚  \"It was okay.\"      â†’ ??? (Model must choose one)         â”‚\n",
        "â”‚                                                             â”‚\n",
        "â”‚  Pros: Simple, clear-cut                                    â”‚\n",
        "â”‚  Cons: Loses nuance, forced choice on neutral text          â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "\n",
        "MULTI-CLASS SENTIMENT (3 classes):\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚  Three classes: POSITIVE, NEUTRAL, NEGATIVE                â”‚\n",
        "â”‚                                                             â”‚\n",
        "â”‚  \"I loved this!\"     â†’ POSITIVE (95%)                      â”‚\n",
        "â”‚  \"Terrible waste.\"   â†’ NEGATIVE (92%)                      â”‚\n",
        "â”‚  \"It was okay.\"      â†’ NEUTRAL  (78%)                      â”‚\n",
        "â”‚                                                             â”‚\n",
        "â”‚  Pros: Captures \"in-between\" sentiment                      â”‚\n",
        "â”‚  Cons: Still coarse-grained                                 â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "\n",
        "FINE-GRAINED SENTIMENT (5 classes):\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚  Five classes: â˜…â˜†â˜†â˜†â˜†  â˜…â˜…â˜†â˜†â˜†  â˜…â˜…â˜…â˜†â˜†  â˜…â˜…â˜…â˜…â˜†  â˜…â˜…â˜…â˜…â˜…           â”‚\n",
        "â”‚                                                             â”‚\n",
        "â”‚  \"Best thing ever!\"  â†’ â˜…â˜…â˜…â˜…â˜… (5 stars, 89%)                â”‚\n",
        "â”‚  \"Pretty good!\"      â†’ â˜…â˜…â˜…â˜…â˜† (4 stars, 72%)                â”‚\n",
        "â”‚  \"It was okay.\"      â†’ â˜…â˜…â˜…â˜†â˜† (3 stars, 65%)                â”‚\n",
        "â”‚  \"Not great.\"        â†’ â˜…â˜…â˜†â˜†â˜† (2 stars, 58%)                â”‚\n",
        "â”‚  \"Awful!\"            â†’ â˜…â˜†â˜†â˜†â˜† (1 star,  91%)                â”‚\n",
        "â”‚                                                             â”‚\n",
        "â”‚  Pros: Most nuanced, matches review systems                 â”‚\n",
        "â”‚  Cons: Harder to train, boundaries between classes blur     â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Connection to Previous Notebooks\n",
        "\n",
        "| Notebook | What You Learned | Relevance to Sentiment |\n",
        "|----------|------------------|------------------------|\n",
        "| 6 (Zero-Shot) | NLI-based classification | Sentiment as hypothesis testing |\n",
        "| 8 (Embeddings) | Comparing model outputs | Comparing sentiment models |\n",
        "| **9 (This notebook)** | **Model differences** | **Why models disagree** |\n",
        "\n",
        "In Notebook 6, you saw how zero-shot classification works via NLI. Sentiment models work similarly but are fine-tuned specifically for opinion/emotion detection."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Understanding Model Cards\n",
        "\n",
        "**Model cards** are documentation that accompanies ML models, describing:\n",
        "\n",
        "```\n",
        "MODEL CARD CONTENTS:\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚  1. MODEL DETAILS                                              â”‚\n",
        "â”‚     - Architecture (BERT, RoBERTa, DistilBERT...)             â”‚\n",
        "â”‚     - Base model it was fine-tuned from                        â”‚\n",
        "â”‚                                                                â”‚\n",
        "â”‚  2. TRAINING DATA                                              â”‚\n",
        "â”‚     - What dataset was used (SST-2, IMDB, Amazon reviews...)  â”‚\n",
        "â”‚     - How many examples                                        â”‚\n",
        "â”‚     - What domain (movies, products, social media...)          â”‚\n",
        "â”‚                                                                â”‚\n",
        "â”‚  3. INTENDED USE                                               â”‚\n",
        "â”‚     - What the model is good for                               â”‚\n",
        "â”‚     - What it should NOT be used for                           â”‚\n",
        "â”‚                                                                â”‚\n",
        "â”‚  4. PERFORMANCE METRICS                                        â”‚\n",
        "â”‚     - Accuracy on test sets                                    â”‚\n",
        "â”‚     - Known limitations                                        â”‚\n",
        "â”‚                                                                â”‚\n",
        "â”‚  5. BIASES & LIMITATIONS                                       â”‚\n",
        "â”‚     - Known biases in training data                            â”‚\n",
        "â”‚     - Edge cases where model struggles                         â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```\n",
        "\n",
        "**Always check the model card before using a model in production!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Key Terminology\n",
        "\n",
        "| Term | Definition |\n",
        "|------|------------|\n",
        "| **Sentiment** | The emotional tone or opinion in text |\n",
        "| **Polarity** | The direction of sentiment (positive/negative) |\n",
        "| **Binary classification** | Two-class prediction (pos/neg) |\n",
        "| **Multi-class** | Three or more classes (pos/neu/neg, 5-star) |\n",
        "| **Fine-grained** | Sentiment with many levels (5+ classes) |\n",
        "| **Model card** | Documentation describing model training and use |\n",
        "| **Domain** | The subject area of training data (movies, products, etc.) |\n",
        "| **Ensemble** | Combining predictions from multiple models |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Check Your Understanding\n",
        "\n",
        "Before moving on, try to answer these questions (answers at the end):\n",
        "\n",
        "1. Why might the same text get different sentiment scores from different models?\n",
        "   - A) Models use different random seeds\n",
        "   - B) Models are trained on different data from different domains\n",
        "   - C) One model is broken\n",
        "\n",
        "2. What is the main disadvantage of binary sentiment classification?\n",
        "   - A) It's too slow\n",
        "   - B) It forces neutral text into positive or negative\n",
        "   - C) It requires too much data\n",
        "\n",
        "3. What should you check in a model card before using a sentiment model?\n",
        "   - A) The model's favorite color\n",
        "   - B) The training data domain and known limitations\n",
        "   - C) How many downloads it has\n",
        "\n",
        "4. What is an \"ensemble\" approach to sentiment analysis?\n",
        "   - A) Using one very large model\n",
        "   - B) Combining predictions from multiple models\n",
        "   - C) Training a model on ensemble music reviews"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Part 2: Basic Implementation\n",
        "\n",
        "## Loading Different Sentiment Models\n",
        "\n",
        "Let's load several sentiment models with different characteristics:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model 1: Default sentiment model (DistilBERT on SST-2 - movie reviews, binary)\n",
        "print(\"Loading Model 1: Default sentiment (binary, movie reviews)...\")\n",
        "sentiment_default = pipeline(\"sentiment-analysis\")\n",
        "print(\"  Loaded: distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "\n",
        "# Model 2: RoBERTa trained on tweets (3-class: positive/neutral/negative)\n",
        "print(\"\\nLoading Model 2: Twitter RoBERTa (3-class, social media)...\")\n",
        "sentiment_twitter = pipeline(\n",
        "    \"sentiment-analysis\", \n",
        "    model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
        ")\n",
        "print(\"  Loaded: cardiffnlp/twitter-roberta-base-sentiment-latest\")\n",
        "\n",
        "# Model 3: Fine-grained sentiment (5-star ratings)\n",
        "print(\"\\nLoading Model 3: 5-star sentiment (product reviews)...\")\n",
        "sentiment_5star = pipeline(\n",
        "    \"sentiment-analysis\",\n",
        "    model=\"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
        ")\n",
        "print(\"  Loaded: nlptown/bert-base-multilingual-uncased-sentiment\")\n",
        "\n",
        "print(\"\\nAll models loaded!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model Comparison Table\n",
        "\n",
        "| Model | Training Data | Classes | Best For |\n",
        "|-------|---------------|---------|----------|\n",
        "| Default (DistilBERT-SST2) | Stanford Sentiment (movies) | 2 (pos/neg) | Movie reviews |\n",
        "| Twitter-RoBERTa | 124M tweets | 3 (pos/neu/neg) | Social media |\n",
        "| BERT-Multilingual | Product reviews | 5 (1-5 stars) | Product/service reviews |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test with a simple sentence\n",
        "test_text = \"I absolutely loved this product, it exceeded my expectations!\"\n",
        "\n",
        "print(f\"Test text: \\\"{test_text}\\\"\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Get predictions from each model\n",
        "result_default = sentiment_default(test_text)[0]\n",
        "result_twitter = sentiment_twitter(test_text)[0]\n",
        "result_5star = sentiment_5star(test_text)[0]\n",
        "\n",
        "print(f\"\\nModel 1 (Binary/Movies):   {result_default['label']:15s} ({result_default['score']:.1%})\")\n",
        "print(f\"Model 2 (Twitter 3-class): {result_twitter['label']:15s} ({result_twitter['score']:.1%})\")\n",
        "print(f\"Model 3 (5-star):          {result_5star['label']:15s} ({result_5star['score']:.1%})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Understanding Different Label Formats\n",
        "\n",
        "Each model uses different label conventions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def explain_label(model_name, label):\n",
        "    \"\"\"Explain what different model labels mean.\"\"\"\n",
        "    explanations = {\n",
        "        'default': {\n",
        "            'POSITIVE': 'Positive sentiment detected',\n",
        "            'NEGATIVE': 'Negative sentiment detected',\n",
        "        },\n",
        "        'twitter': {\n",
        "            'positive': 'Positive sentiment',\n",
        "            'neutral': 'Neutral/no strong sentiment',\n",
        "            'negative': 'Negative sentiment',\n",
        "        },\n",
        "        '5star': {\n",
        "            '1 star': 'Very negative (1/5)',\n",
        "            '2 stars': 'Negative (2/5)',\n",
        "            '3 stars': 'Neutral/mixed (3/5)',\n",
        "            '4 stars': 'Positive (4/5)',\n",
        "            '5 stars': 'Very positive (5/5)',\n",
        "        }\n",
        "    }\n",
        "    return explanations.get(model_name, {}).get(label, 'Unknown label')\n",
        "\n",
        "\n",
        "print(\"Label Format Reference:\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"\\nModel 1 (Binary):\")\n",
        "for label in ['POSITIVE', 'NEGATIVE']:\n",
        "    print(f\"  {label:15s} â†’ {explain_label('default', label)}\")\n",
        "\n",
        "print(\"\\nModel 2 (Twitter 3-class):\")\n",
        "for label in ['positive', 'neutral', 'negative']:\n",
        "    print(f\"  {label:15s} â†’ {explain_label('twitter', label)}\")\n",
        "\n",
        "print(\"\\nModel 3 (5-star):\")\n",
        "for label in ['1 star', '2 stars', '3 stars', '4 stars', '5 stars']:\n",
        "    print(f\"  {label:15s} â†’ {explain_label('5star', label)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Comparing All Three Models on Sample Texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compare_models(text, models_dict):\n",
        "    \"\"\"\n",
        "    Compare sentiment predictions from multiple models.\n",
        "    \n",
        "    Args:\n",
        "        text: Input text to analyze\n",
        "        models_dict: Dict of {name: pipeline}\n",
        "        \n",
        "    Returns:\n",
        "        Dict of results\n",
        "    \"\"\"\n",
        "    results = {'text': text}\n",
        "    \n",
        "    for name, model in models_dict.items():\n",
        "        prediction = model(text)[0]\n",
        "        results[name] = {\n",
        "            'label': prediction['label'],\n",
        "            'score': prediction['score']\n",
        "        }\n",
        "    \n",
        "    return results\n",
        "\n",
        "\n",
        "def format_comparison(result):\n",
        "    \"\"\"Format comparison result for display.\"\"\"\n",
        "    lines = []\n",
        "    lines.append(f\"\\nText: \\\"{result['text']}\\\"\")\n",
        "    lines.append(\"-\" * 60)\n",
        "    \n",
        "    for name, pred in result.items():\n",
        "        if name == 'text':\n",
        "            continue\n",
        "        bar = '*' * int(pred['score'] * 20)\n",
        "        lines.append(f\"  {name:20s}: {pred['label']:15s} {pred['score']:.1%} {bar}\")\n",
        "    \n",
        "    return '\\n'.join(lines)\n",
        "\n",
        "\n",
        "# Set up models dict\n",
        "models = {\n",
        "    'Binary (Movies)': sentiment_default,\n",
        "    'Twitter (3-class)': sentiment_twitter,\n",
        "    '5-Star (Products)': sentiment_5star,\n",
        "}\n",
        "\n",
        "# Test texts covering various sentiments\n",
        "test_texts = [\n",
        "    \"This is absolutely amazing! Best purchase ever!\",\n",
        "    \"Terrible experience, I want my money back.\",\n",
        "    \"It was okay, nothing special.\",\n",
        "    \"Pretty good, but could be better.\",\n",
        "    \"Not bad, I guess.\",\n",
        "]\n",
        "\n",
        "print(\"Model Comparison Results:\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "for text in test_texts:\n",
        "    result = compare_models(text, models)\n",
        "    print(format_comparison(result))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Exercise 1: Model Behavior Exploration (Guided)\n",
        "\n",
        "**Difficulty**: Basic | **Time**: 10-15 minutes\n",
        "\n",
        "**Your task**: Explore how the models handle different types of text.\n",
        "\n",
        "### Step 1: Test with ambiguous sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ambiguous texts that models might interpret differently\n",
        "ambiguous_texts = [\n",
        "    \"I didn't hate it.\",  # Negative phrasing, neutral/positive meaning\n",
        "    \"It could have been worse.\",  # Backhanded compliment\n",
        "    \"Interesting.\",  # Single word, context-dependent\n",
        "    \"Well, that happened.\",  # Sarcasm-like\n",
        "    \"I've seen better, I've seen worse.\",  # Mixed\n",
        "]\n",
        "\n",
        "print(\"Testing Ambiguous Texts:\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "for text in ambiguous_texts:\n",
        "    result = compare_models(text, models)\n",
        "    print(format_comparison(result))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2: Notice the patterns\n",
        "\n",
        "**Questions to consider:**\n",
        "- Which model is most likely to say \"neutral\"?\n",
        "- Which model seems most confident (highest scores)?\n",
        "- Do any models struggle with negation (\"didn't hate\")?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Let's analyze the pattern more systematically\n",
        "def analyze_model_tendencies(texts, models_dict):\n",
        "    \"\"\"Analyze how models tend to classify texts.\"\"\"\n",
        "    stats = {name: {'labels': [], 'scores': []} for name in models_dict.keys()}\n",
        "    \n",
        "    for text in texts:\n",
        "        for name, model in models_dict.items():\n",
        "            pred = model(text)[0]\n",
        "            stats[name]['labels'].append(pred['label'])\n",
        "            stats[name]['scores'].append(pred['score'])\n",
        "    \n",
        "    print(\"Model Tendency Analysis:\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    for name, data in stats.items():\n",
        "        labels = data['labels']\n",
        "        scores = data['scores']\n",
        "        \n",
        "        # Count label distribution\n",
        "        label_counts = {}\n",
        "        for label in labels:\n",
        "            label_counts[label] = label_counts.get(label, 0) + 1\n",
        "        \n",
        "        print(f\"\\n{name}:\")\n",
        "        print(f\"  Average confidence: {np.mean(scores):.1%}\")\n",
        "        print(f\"  Label distribution:\")\n",
        "        for label, count in sorted(label_counts.items(), key=lambda x: -x[1]):\n",
        "            pct = count / len(labels) * 100\n",
        "            print(f\"    {label:15s}: {count:2d} ({pct:.0f}%)\")\n",
        "\n",
        "# Combine all texts for analysis\n",
        "all_test_texts = test_texts + ambiguous_texts\n",
        "analyze_model_tendencies(all_test_texts, models)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 3: Try your own test cases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "# Add your own test texts and compare models\n",
        "\n",
        "my_test_texts = [\n",
        "    \"Your text here\",\n",
        "    # Add more...\n",
        "]\n",
        "\n",
        "# Uncomment to run:\n",
        "# for text in my_test_texts:\n",
        "#     result = compare_models(text, models)\n",
        "#     print(format_comparison(result))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Part 3: Intermediate Exploration\n",
        "\n",
        "## Getting All Class Probabilities\n",
        "\n",
        "So far we've only seen the top prediction. Let's see all class probabilities:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create pipelines that return all scores\n",
        "sentiment_default_full = pipeline(\n",
        "    \"sentiment-analysis\",\n",
        "    top_k=None  # Return all classes\n",
        ")\n",
        "\n",
        "sentiment_twitter_full = pipeline(\n",
        "    \"sentiment-analysis\",\n",
        "    model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\",\n",
        "    top_k=None\n",
        ")\n",
        "\n",
        "sentiment_5star_full = pipeline(\n",
        "    \"sentiment-analysis\",\n",
        "    model=\"nlptown/bert-base-multilingual-uncased-sentiment\",\n",
        "    top_k=None\n",
        ")\n",
        "\n",
        "print(\"Full probability pipelines loaded!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def show_all_probabilities(text):\n",
        "    \"\"\"Show full probability distribution from each model.\"\"\"\n",
        "    print(f\"\\nText: \\\"{text}\\\"\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    # Binary model\n",
        "    print(\"\\nBinary Model (distilbert-sst2):\")\n",
        "    probs = sentiment_default_full(text)[0]\n",
        "    for p in sorted(probs, key=lambda x: -x['score']):\n",
        "        bar = '*' * int(p['score'] * 40)\n",
        "        print(f\"  {p['label']:15s}: {p['score']:.1%} {bar}\")\n",
        "    \n",
        "    # Twitter 3-class\n",
        "    print(\"\\nTwitter 3-class Model:\")\n",
        "    probs = sentiment_twitter_full(text)[0]\n",
        "    for p in sorted(probs, key=lambda x: -x['score']):\n",
        "        bar = '*' * int(p['score'] * 40)\n",
        "        print(f\"  {p['label']:15s}: {p['score']:.1%} {bar}\")\n",
        "    \n",
        "    # 5-star\n",
        "    print(\"\\n5-Star Model:\")\n",
        "    probs = sentiment_5star_full(text)[0]\n",
        "    for p in sorted(probs, key=lambda x: -x['score']):\n",
        "        bar = '*' * int(p['score'] * 40)\n",
        "        print(f\"  {p['label']:15s}: {p['score']:.1%} {bar}\")\n",
        "\n",
        "\n",
        "# Test with a neutral-ish text\n",
        "show_all_probabilities(\"It was okay, nothing special.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare probability distributions for different sentiment levels\n",
        "spectrum_texts = [\n",
        "    \"This is the worst thing I've ever experienced. Absolutely terrible.\",\n",
        "    \"Not great, pretty disappointing actually.\",\n",
        "    \"It's fine, I guess. Nothing special.\",\n",
        "    \"Pretty good! I enjoyed it.\",\n",
        "    \"Absolutely incredible! Best ever! Highly recommend!\",\n",
        "]\n",
        "\n",
        "print(\"Sentiment Spectrum Analysis:\")\n",
        "for text in spectrum_texts:\n",
        "    show_all_probabilities(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Normalizing to a Common Scale\n",
        "\n",
        "To compare models fairly, let's convert all outputs to a common -1 to +1 scale:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def normalize_sentiment(model_type, predictions):\n",
        "    \"\"\"\n",
        "    Convert model predictions to a -1 to +1 scale.\n",
        "    \n",
        "    -1 = Most negative\n",
        "     0 = Neutral\n",
        "    +1 = Most positive\n",
        "    \"\"\"\n",
        "    probs = {p['label']: p['score'] for p in predictions}\n",
        "    \n",
        "    if model_type == 'binary':\n",
        "        # Binary: POSITIVE and NEGATIVE\n",
        "        pos = probs.get('POSITIVE', 0)\n",
        "        neg = probs.get('NEGATIVE', 0)\n",
        "        return pos - neg  # Range: -1 to +1\n",
        "    \n",
        "    elif model_type == 'twitter':\n",
        "        # Twitter: positive, neutral, negative\n",
        "        pos = probs.get('positive', 0)\n",
        "        neu = probs.get('neutral', 0)\n",
        "        neg = probs.get('negative', 0)\n",
        "        return pos - neg  # Ignore neutral for simplicity\n",
        "    \n",
        "    elif model_type == '5star':\n",
        "        # 5-star: convert to weighted average\n",
        "        weights = {\n",
        "            '1 star': -1.0,\n",
        "            '2 stars': -0.5,\n",
        "            '3 stars': 0.0,\n",
        "            '4 stars': 0.5,\n",
        "            '5 stars': 1.0,\n",
        "        }\n",
        "        score = sum(probs.get(label, 0) * weight for label, weight in weights.items())\n",
        "        return score\n",
        "    \n",
        "    return 0\n",
        "\n",
        "\n",
        "def get_normalized_scores(text):\n",
        "    \"\"\"Get normalized sentiment scores from all models.\"\"\"\n",
        "    results = {}\n",
        "    \n",
        "    results['Binary'] = normalize_sentiment(\n",
        "        'binary', \n",
        "        sentiment_default_full(text)[0]\n",
        "    )\n",
        "    results['Twitter'] = normalize_sentiment(\n",
        "        'twitter', \n",
        "        sentiment_twitter_full(text)[0]\n",
        "    )\n",
        "    results['5-Star'] = normalize_sentiment(\n",
        "        '5star', \n",
        "        sentiment_5star_full(text)[0]\n",
        "    )\n",
        "    \n",
        "    return results\n",
        "\n",
        "\n",
        "# Compare normalized scores\n",
        "print(\"Normalized Sentiment Scores (-1 to +1):\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "for text in spectrum_texts:\n",
        "    scores = get_normalized_scores(text)\n",
        "    avg = np.mean(list(scores.values()))\n",
        "    \n",
        "    print(f\"\\n\\\"{text[:50]}...\\\"\" if len(text) > 50 else f\"\\n\\\"{text}\\\"\")\n",
        "    print(f\"  Binary:  {scores['Binary']:+.3f}\")\n",
        "    print(f\"  Twitter: {scores['Twitter']:+.3f}\")\n",
        "    print(f\"  5-Star:  {scores['5-Star']:+.3f}\")\n",
        "    print(f\"  Average: {avg:+.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualizing Model Agreement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_sentiment_comparison(text):\n",
        "    \"\"\"Create a visual comparison of model sentiments.\"\"\"\n",
        "    scores = get_normalized_scores(text)\n",
        "    \n",
        "    print(f\"\\n\\\"{text}\\\"\")\n",
        "    print()\n",
        "    \n",
        "    # Scale: -1 to +1, mapped to 0 to 40 characters\n",
        "    def score_to_position(score):\n",
        "        return int((score + 1) * 20)  # 0 to 40\n",
        "    \n",
        "    # Draw scale\n",
        "    print(\"         Negative          Neutral          Positive\")\n",
        "    print(\"         â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º\")\n",
        "    print(\"        -1                  0                 +1\")\n",
        "    print()\n",
        "    \n",
        "    for name, score in scores.items():\n",
        "        pos = score_to_position(score)\n",
        "        line = [' '] * 41\n",
        "        line[20] = 'â”‚'  # Center marker\n",
        "        line[pos] = 'â—'\n",
        "        \n",
        "        print(f\"  {name:10s} {''.join(line)} ({score:+.2f})\")\n",
        "    \n",
        "    # Show agreement metric\n",
        "    values = list(scores.values())\n",
        "    spread = max(values) - min(values)\n",
        "    agreement = 1 - (spread / 2)  # 0 to 1 scale\n",
        "    \n",
        "    print()\n",
        "    print(f\"  Model Agreement: {agreement:.0%}\" + \n",
        "          (\" (High)\" if agreement > 0.7 else \" (Low)\" if agreement < 0.4 else \" (Medium)\"))\n",
        "\n",
        "\n",
        "# Test visualization\n",
        "test_vis_texts = [\n",
        "    \"I love this so much!\",\n",
        "    \"It's fine.\",\n",
        "    \"Worst experience ever.\",\n",
        "    \"I didn't hate it.\",\n",
        "]\n",
        "\n",
        "print(\"Visual Sentiment Comparison:\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "for text in test_vis_texts:\n",
        "    visualize_sentiment_comparison(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Exercise 2: Find Edge Cases (Semi-guided)\n",
        "\n",
        "**Difficulty**: Intermediate | **Time**: 15-20 minutes\n",
        "\n",
        "**Your task**: Find texts where models strongly disagree.\n",
        "\n",
        "**Hints**:\n",
        "- Try negation (\"I don't think it's bad\")\n",
        "- Try sarcasm (\"Oh great, another delay\")\n",
        "- Try domain-specific language\n",
        "- Try mixed sentiment (\"Good product, terrible service\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def find_disagreements(texts, threshold=0.5):\n",
        "    \"\"\"\n",
        "    Find texts where models disagree significantly.\n",
        "    \n",
        "    Args:\n",
        "        texts: List of texts to test\n",
        "        threshold: Minimum spread to count as disagreement\n",
        "        \n",
        "    Returns:\n",
        "        List of (text, scores, spread) for disagreements\n",
        "    \"\"\"\n",
        "    disagreements = []\n",
        "    \n",
        "    for text in texts:\n",
        "        scores = get_normalized_scores(text)\n",
        "        values = list(scores.values())\n",
        "        spread = max(values) - min(values)\n",
        "        \n",
        "        if spread >= threshold:\n",
        "            disagreements.append((text, scores, spread))\n",
        "    \n",
        "    # Sort by spread (highest first)\n",
        "    disagreements.sort(key=lambda x: x[2], reverse=True)\n",
        "    \n",
        "    return disagreements\n",
        "\n",
        "\n",
        "# Test with challenging texts\n",
        "challenging_texts = [\n",
        "    # Negation\n",
        "    \"I don't think it's bad.\",\n",
        "    \"Not the worst I've seen.\",\n",
        "    \"I can't say I disliked it.\",\n",
        "    \n",
        "    # Sarcasm/Irony\n",
        "    \"Oh great, another meeting.\",\n",
        "    \"Wow, what a surprise. Not.\",\n",
        "    \"Yeah, that's exactly what I wanted.\",\n",
        "    \n",
        "    # Mixed sentiment\n",
        "    \"Great product, terrible customer service.\",\n",
        "    \"Loved the food, hated the wait.\",\n",
        "    \"Beautiful design but falls apart quickly.\",\n",
        "    \n",
        "    # Domain-specific\n",
        "    \"The plot twist was sick!\",  # Slang positive\n",
        "    \"This code is wicked complex.\",  # Slang\n",
        "    \"That's a bad movie in the best way.\",  # \"So bad it's good\"\n",
        "]\n",
        "\n",
        "print(\"Searching for Model Disagreements:\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "disagreements = find_disagreements(challenging_texts, threshold=0.4)\n",
        "\n",
        "if disagreements:\n",
        "    print(f\"\\nFound {len(disagreements)} texts with significant disagreement:\\n\")\n",
        "    for text, scores, spread in disagreements:\n",
        "        visualize_sentiment_comparison(text)\n",
        "        print(f\"  Spread: {spread:.2f}\")\n",
        "        print()\n",
        "else:\n",
        "    print(\"No significant disagreements found. Try more challenging texts!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "# Try to find texts with maximum disagreement\n",
        "\n",
        "my_challenging_texts = [\n",
        "    \"Your challenging text here\",\n",
        "    # Add more...\n",
        "]\n",
        "\n",
        "# Uncomment to test:\n",
        "# for text in my_challenging_texts:\n",
        "#     visualize_sentiment_comparison(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Part 4: Advanced Topics\n",
        "\n",
        "## Building a Sentiment Ensemble\n",
        "\n",
        "Combine multiple models for more robust predictions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SentimentEnsemble:\n",
        "    \"\"\"\n",
        "    Combines multiple sentiment models for more robust predictions.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        \"\"\"Initialize ensemble with multiple models.\"\"\"\n",
        "        self.models = {}\n",
        "        self.model_types = {}\n",
        "        \n",
        "    def add_model(self, name, model, model_type):\n",
        "        \"\"\"\n",
        "        Add a model to the ensemble.\n",
        "        \n",
        "        Args:\n",
        "            name: Display name for the model\n",
        "            model: Pipeline with top_k=None\n",
        "            model_type: 'binary', 'twitter', or '5star' for normalization\n",
        "        \"\"\"\n",
        "        self.models[name] = model\n",
        "        self.model_types[name] = model_type\n",
        "    \n",
        "    def predict(self, text, strategy='average'):\n",
        "        \"\"\"\n",
        "        Get ensemble prediction.\n",
        "        \n",
        "        Args:\n",
        "            text: Input text\n",
        "            strategy: 'average', 'majority', or 'weighted'\n",
        "            \n",
        "        Returns:\n",
        "            dict with ensemble and individual predictions\n",
        "        \"\"\"\n",
        "        individual_scores = {}\n",
        "        \n",
        "        for name, model in self.models.items():\n",
        "            preds = model(text)[0]\n",
        "            normalized = normalize_sentiment(self.model_types[name], preds)\n",
        "            individual_scores[name] = normalized\n",
        "        \n",
        "        # Calculate ensemble score\n",
        "        scores_list = list(individual_scores.values())\n",
        "        \n",
        "        if strategy == 'average':\n",
        "            ensemble_score = np.mean(scores_list)\n",
        "        elif strategy == 'majority':\n",
        "            # Count positive vs negative\n",
        "            positives = sum(1 for s in scores_list if s > 0)\n",
        "            negatives = sum(1 for s in scores_list if s < 0)\n",
        "            if positives > negatives:\n",
        "                ensemble_score = abs(np.mean([s for s in scores_list if s > 0]))\n",
        "            elif negatives > positives:\n",
        "                ensemble_score = -abs(np.mean([s for s in scores_list if s < 0]))\n",
        "            else:\n",
        "                ensemble_score = 0\n",
        "        else:  # weighted by confidence\n",
        "            ensemble_score = np.mean(scores_list)  # Default to average\n",
        "        \n",
        "        # Determine label\n",
        "        if ensemble_score > 0.3:\n",
        "            label = 'POSITIVE'\n",
        "        elif ensemble_score < -0.3:\n",
        "            label = 'NEGATIVE'\n",
        "        else:\n",
        "            label = 'NEUTRAL'\n",
        "        \n",
        "        # Calculate agreement\n",
        "        spread = max(scores_list) - min(scores_list)\n",
        "        agreement = 1 - (spread / 2)\n",
        "        \n",
        "        return {\n",
        "            'text': text,\n",
        "            'ensemble_score': ensemble_score,\n",
        "            'ensemble_label': label,\n",
        "            'agreement': agreement,\n",
        "            'individual_scores': individual_scores,\n",
        "            'strategy': strategy,\n",
        "        }\n",
        "    \n",
        "    def format_prediction(self, result):\n",
        "        \"\"\"Format prediction for display.\"\"\"\n",
        "        lines = []\n",
        "        lines.append(\"â”Œ\" + \"â”€\"*66 + \"â”\")\n",
        "        \n",
        "        text_display = result['text'][:60]\n",
        "        lines.append(f\"â”‚ Text: {text_display:60s} â”‚\")\n",
        "        lines.append(\"â”œ\" + \"â”€\"*66 + \"â”¤\")\n",
        "        \n",
        "        # Individual scores\n",
        "        lines.append(\"â”‚ Individual Models:\" + \" \"*47 + \"â”‚\")\n",
        "        for name, score in result['individual_scores'].items():\n",
        "            bar_pos = int((score + 1) * 15)  # 0 to 30\n",
        "            bar = [' '] * 31\n",
        "            bar[15] = 'â”‚'\n",
        "            bar[bar_pos] = 'â—'\n",
        "            lines.append(f\"â”‚   {name:18s} {''.join(bar)} {score:+.2f}\" + \" \"*3 + \"â”‚\")\n",
        "        \n",
        "        lines.append(\"â”œ\" + \"â”€\"*66 + \"â”¤\")\n",
        "        \n",
        "        # Ensemble result\n",
        "        score = result['ensemble_score']\n",
        "        label = result['ensemble_label']\n",
        "        agreement = result['agreement']\n",
        "        \n",
        "        lines.append(f\"â”‚ ENSEMBLE RESULT: {label:10s}  Score: {score:+.2f}\" + \" \"*23 + \"â”‚\")\n",
        "        lines.append(f\"â”‚ Model Agreement: {agreement:.0%}\" + \" \"*46 + \"â”‚\")\n",
        "        \n",
        "        lines.append(\"â””\" + \"â”€\"*66 + \"â”˜\")\n",
        "        \n",
        "        return '\\n'.join(lines)\n",
        "\n",
        "\n",
        "# Create ensemble\n",
        "ensemble = SentimentEnsemble()\n",
        "ensemble.add_model('Binary (SST2)', sentiment_default_full, 'binary')\n",
        "ensemble.add_model('Twitter', sentiment_twitter_full, 'twitter')\n",
        "ensemble.add_model('5-Star', sentiment_5star_full, '5star')\n",
        "\n",
        "print(\"Sentiment Ensemble created with 3 models!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test the ensemble\n",
        "test_ensemble_texts = [\n",
        "    \"Absolutely loved it! Highly recommend!\",\n",
        "    \"Terrible waste of money.\",\n",
        "    \"It was okay, nothing special.\",\n",
        "    \"I didn't hate it.\",\n",
        "    \"Great product but awful customer service.\",\n",
        "]\n",
        "\n",
        "print(\"Ensemble Predictions:\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "for text in test_ensemble_texts:\n",
        "    result = ensemble.predict(text)\n",
        "    print(ensemble.format_prediction(result))\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### When to Use Ensembles\n",
        "\n",
        "| Scenario | Ensemble? | Why |\n",
        "|----------|-----------|-----|\n",
        "| High-stakes decisions | Yes | Reduces single-model errors |\n",
        "| Mixed domains | Yes | Different models for different text types |\n",
        "| Speed-critical | No | Multiple models = slower |\n",
        "| Simple classification | No | One good model is usually enough |\n",
        "| Detecting edge cases | Yes | Disagreement signals uncertainty |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Domain-Specific Considerations\n",
        "\n",
        "Different domains have different sentiment patterns:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Domain-specific test cases\n",
        "domain_texts = {\n",
        "    'Movie Reviews': [\n",
        "        \"The cinematography was stunning but the plot was thin.\",\n",
        "        \"Oscar-worthy performance!\",\n",
        "        \"A total flop at the box office.\",\n",
        "    ],\n",
        "    'Product Reviews': [\n",
        "        \"Works as advertised. Good value.\",\n",
        "        \"Broke after 2 weeks. Cheap quality.\",\n",
        "        \"Exactly what I needed!\",\n",
        "    ],\n",
        "    'Social Media': [\n",
        "        \"lol this is so random ðŸ˜‚\",\n",
        "        \"ugh monday again ðŸ™„\",\n",
        "        \"this slaps! ðŸ”¥\",\n",
        "    ],\n",
        "    'Financial News': [\n",
        "        \"Stock plunged 15% after earnings miss.\",\n",
        "        \"Company announces record quarterly profits.\",\n",
        "        \"Market remained flat amid uncertainty.\",\n",
        "    ],\n",
        "}\n",
        "\n",
        "print(\"Domain-Specific Analysis:\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "for domain, texts in domain_texts.items():\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"DOMAIN: {domain}\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    for text in texts:\n",
        "        result = ensemble.predict(text)\n",
        "        print(f\"\\n\\\"{text}\\\"\")\n",
        "        print(f\"  Ensemble: {result['ensemble_label']:10s} ({result['ensemble_score']:+.2f})\")\n",
        "        print(f\"  Agreement: {result['agreement']:.0%}\")\n",
        "        \n",
        "        # Show which model is most confident\n",
        "        scores = result['individual_scores']\n",
        "        most_confident = max(scores.keys(), key=lambda k: abs(scores[k]))\n",
        "        print(f\"  Most confident: {most_confident}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Exercise 3: Model Selection Advisor (Independent)\n",
        "\n",
        "**Difficulty**: Advanced | **Time**: 15-20 minutes\n",
        "\n",
        "**Your task**: Build a function that recommends which sentiment model to use based on the input text characteristics.\n",
        "\n",
        "**Requirements**:\n",
        "1. Detect domain indicators (e.g., movie terms, product terms, emojis)\n",
        "2. Recommend the most appropriate model\n",
        "3. Explain the recommendation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "\n",
        "def recommend_model(text):\n",
        "    \"\"\"\n",
        "    Recommend the best sentiment model for a given text.\n",
        "    \n",
        "    Args:\n",
        "        text: Input text to analyze\n",
        "        \n",
        "    Returns:\n",
        "        dict with recommendation and explanation\n",
        "    \"\"\"\n",
        "    text_lower = text.lower()\n",
        "    \n",
        "    # Domain indicators\n",
        "    movie_indicators = ['movie', 'film', 'actor', 'actress', 'director', 'oscar', \n",
        "                        'cinema', 'scene', 'plot', 'character', 'sequel']\n",
        "    product_indicators = ['product', 'quality', 'price', 'shipping', 'delivery',\n",
        "                         'bought', 'purchase', 'worth', 'money', 'item']\n",
        "    social_indicators = ['lol', 'lmao', 'omg', 'bruh', 'ngl', 'tbh', 'idk']\n",
        "    \n",
        "    # Check for emojis (simplified check)\n",
        "    has_emoji = any(ord(char) > 127 for char in text)\n",
        "    \n",
        "    # Count indicator matches\n",
        "    movie_score = sum(1 for ind in movie_indicators if ind in text_lower)\n",
        "    product_score = sum(1 for ind in product_indicators if ind in text_lower)\n",
        "    social_score = sum(1 for ind in social_indicators if ind in text_lower)\n",
        "    \n",
        "    if has_emoji:\n",
        "        social_score += 2  # Emojis suggest social media\n",
        "    \n",
        "    # Short informal text suggests social media\n",
        "    if len(text) < 50 and not any([movie_score, product_score]):\n",
        "        social_score += 1\n",
        "    \n",
        "    # Make recommendation\n",
        "    scores = {\n",
        "        'Binary (Movies)': movie_score + 1,  # +1 as it's the default\n",
        "        'Twitter (3-class)': social_score,\n",
        "        '5-Star (Products)': product_score,\n",
        "    }\n",
        "    \n",
        "    recommended = max(scores.keys(), key=lambda k: scores[k])\n",
        "    \n",
        "    # Generate explanation\n",
        "    explanations = {\n",
        "        'Binary (Movies)': 'Best for general/movie-related text with clear sentiment.',\n",
        "        'Twitter (3-class)': 'Best for informal text, social media, or text with emojis.',\n",
        "        '5-Star (Products)': 'Best for product/service reviews with nuanced ratings.',\n",
        "    }\n",
        "    \n",
        "    return {\n",
        "        'text': text,\n",
        "        'recommended_model': recommended,\n",
        "        'explanation': explanations[recommended],\n",
        "        'confidence_scores': scores,\n",
        "    }\n",
        "\n",
        "\n",
        "# Test the advisor\n",
        "advisor_texts = [\n",
        "    \"The movie's cinematography was breathtaking!\",\n",
        "    \"Great product, fast shipping, would buy again!\",\n",
        "    \"lol this is so funny ðŸ˜‚ðŸ˜‚\",\n",
        "    \"Not bad for the price.\",\n",
        "    \"The sequel was even better than the original film!\",\n",
        "]\n",
        "\n",
        "print(\"Model Recommendation Advisor:\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "for text in advisor_texts:\n",
        "    rec = recommend_model(text)\n",
        "    print(f\"\\nText: \\\"{text}\\\"\")\n",
        "    print(f\"  Recommended: {rec['recommended_model']}\")\n",
        "    print(f\"  Reason: {rec['explanation']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Part 5: Mini-Project\n",
        "\n",
        "## Project: Multi-Model Sentiment Dashboard\n",
        "\n",
        "**Scenario**: You're building a content moderation system that analyzes user reviews and social media posts. You need to understand sentiment from multiple perspectives and flag cases where models strongly disagree.\n",
        "\n",
        "**Your goal**: Build a `SentimentDashboard` class that:\n",
        "1. Analyzes text with multiple models\n",
        "2. Provides a summary with agreement metrics\n",
        "3. Flags uncertain or controversial predictions\n",
        "4. Generates actionable insights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# MINI-PROJECT: Multi-Model Sentiment Dashboard\n",
        "# ============================================\n",
        "\n",
        "class SentimentDashboard:\n",
        "    \"\"\"\n",
        "    A comprehensive sentiment analysis dashboard using multiple models.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, agreement_threshold=0.6, uncertainty_threshold=0.3):\n",
        "        \"\"\"\n",
        "        Initialize the dashboard.\n",
        "        \n",
        "        Args:\n",
        "            agreement_threshold: Below this, flag as low agreement\n",
        "            uncertainty_threshold: Score magnitude below this is uncertain\n",
        "        \"\"\"\n",
        "        self.agreement_threshold = agreement_threshold\n",
        "        self.uncertainty_threshold = uncertainty_threshold\n",
        "        \n",
        "        # Initialize models\n",
        "        self.models = {\n",
        "            'binary': sentiment_default_full,\n",
        "            'twitter': sentiment_twitter_full,\n",
        "            '5star': sentiment_5star_full,\n",
        "        }\n",
        "        \n",
        "        # Track analysis history\n",
        "        self.history = []\n",
        "    \n",
        "    def analyze(self, text):\n",
        "        \"\"\"\n",
        "        Perform comprehensive sentiment analysis.\n",
        "        \n",
        "        Returns:\n",
        "            dict with detailed analysis results\n",
        "        \"\"\"\n",
        "        results = {'text': text, 'models': {}}\n",
        "        normalized_scores = []\n",
        "        \n",
        "        # Get predictions from each model\n",
        "        for model_type, pipeline in self.models.items():\n",
        "            preds = pipeline(text)[0]\n",
        "            top_pred = max(preds, key=lambda x: x['score'])\n",
        "            normalized = normalize_sentiment(model_type, preds)\n",
        "            \n",
        "            results['models'][model_type] = {\n",
        "                'label': top_pred['label'],\n",
        "                'confidence': top_pred['score'],\n",
        "                'normalized_score': normalized,\n",
        "                'all_probs': {p['label']: p['score'] for p in preds}\n",
        "            }\n",
        "            normalized_scores.append(normalized)\n",
        "        \n",
        "        # Calculate aggregate metrics\n",
        "        avg_score = np.mean(normalized_scores)\n",
        "        spread = max(normalized_scores) - min(normalized_scores)\n",
        "        agreement = 1 - (spread / 2)\n",
        "        \n",
        "        # Determine consensus label\n",
        "        if avg_score > self.uncertainty_threshold:\n",
        "            consensus = 'POSITIVE'\n",
        "        elif avg_score < -self.uncertainty_threshold:\n",
        "            consensus = 'NEGATIVE'\n",
        "        else:\n",
        "            consensus = 'NEUTRAL'\n",
        "        \n",
        "        # Generate flags\n",
        "        flags = []\n",
        "        if agreement < self.agreement_threshold:\n",
        "            flags.append('LOW_AGREEMENT')\n",
        "        if abs(avg_score) < self.uncertainty_threshold:\n",
        "            flags.append('UNCERTAIN')\n",
        "        if any(m['confidence'] < 0.5 for m in results['models'].values()):\n",
        "            flags.append('LOW_CONFIDENCE')\n",
        "        \n",
        "        results['summary'] = {\n",
        "            'consensus_label': consensus,\n",
        "            'average_score': avg_score,\n",
        "            'agreement': agreement,\n",
        "            'spread': spread,\n",
        "            'flags': flags,\n",
        "            'needs_review': len(flags) > 0,\n",
        "        }\n",
        "        \n",
        "        # Store in history\n",
        "        self.history.append(results)\n",
        "        \n",
        "        return results\n",
        "    \n",
        "    def format_analysis(self, result):\n",
        "        \"\"\"Format analysis for display.\"\"\"\n",
        "        lines = []\n",
        "        \n",
        "        # Header\n",
        "        lines.append(\"â•”\" + \"â•\"*68 + \"â•—\")\n",
        "        lines.append(\"â•‘\" + \" SENTIMENT ANALYSIS DASHBOARD \".center(68) + \"â•‘\")\n",
        "        lines.append(\"â• \" + \"â•\"*68 + \"â•£\")\n",
        "        \n",
        "        # Text\n",
        "        text_display = result['text'][:64]\n",
        "        lines.append(f\"â•‘ Text: {text_display:62s} â•‘\")\n",
        "        if len(result['text']) > 64:\n",
        "            lines.append(f\"â•‘       {result['text'][64:126]:62s} â•‘\")\n",
        "        lines.append(\"â• \" + \"â•\"*68 + \"â•£\")\n",
        "        \n",
        "        # Individual model results\n",
        "        lines.append(\"â•‘\" + \" Individual Model Predictions: \".ljust(68) + \"â•‘\")\n",
        "        lines.append(\"â•‘\" + \"-\"*68 + \"â•‘\")\n",
        "        \n",
        "        model_names = {'binary': 'Binary (SST2)', 'twitter': 'Twitter', '5star': '5-Star'}\n",
        "        for model_type, data in result['models'].items():\n",
        "            name = model_names[model_type]\n",
        "            label = data['label']\n",
        "            conf = data['confidence']\n",
        "            norm = data['normalized_score']\n",
        "            lines.append(f\"â•‘  {name:18s} â”‚ {label:15s} â”‚ Conf: {conf:.0%} â”‚ Score: {norm:+.2f} â•‘\")\n",
        "        \n",
        "        lines.append(\"â• \" + \"â•\"*68 + \"â•£\")\n",
        "        \n",
        "        # Summary\n",
        "        summary = result['summary']\n",
        "        lines.append(\"â•‘\" + \" SUMMARY \".center(68, 'â”€') + \"â•‘\")\n",
        "        lines.append(f\"â•‘  Consensus: {summary['consensus_label']:15s}  â”‚  Score: {summary['average_score']:+.2f}\" + \" \"*21 + \"â•‘\")\n",
        "        lines.append(f\"â•‘  Agreement: {summary['agreement']:.0%}\" + \" \"*(56 - len(f\"{summary['agreement']:.0%}\")) + \"â•‘\")\n",
        "        \n",
        "        # Flags\n",
        "        if summary['flags']:\n",
        "            flags_str = \", \".join(summary['flags'])\n",
        "            lines.append(\"â• \" + \"â•\"*68 + \"â•£\")\n",
        "            lines.append(f\"â•‘  âš ï¸  FLAGS: {flags_str:55s} â•‘\")\n",
        "            \n",
        "            # Recommendations based on flags\n",
        "            if 'LOW_AGREEMENT' in summary['flags']:\n",
        "                lines.append(\"â•‘       â†’ Models disagree significantly - manual review suggested\" + \" \"*3 + \"â•‘\")\n",
        "            if 'UNCERTAIN' in summary['flags']:\n",
        "                lines.append(\"â•‘       â†’ Sentiment is ambiguous or neutral\" + \" \"*24 + \"â•‘\")\n",
        "            if 'LOW_CONFIDENCE' in summary['flags']:\n",
        "                lines.append(\"â•‘       â†’ At least one model has low confidence\" + \" \"*20 + \"â•‘\")\n",
        "        else:\n",
        "            lines.append(\"â• \" + \"â•\"*68 + \"â•£\")\n",
        "            lines.append(\"â•‘  âœ“  No flags - High confidence prediction\" + \" \"*26 + \"â•‘\")\n",
        "        \n",
        "        lines.append(\"â•š\" + \"â•\"*68 + \"â•\")\n",
        "        \n",
        "        return '\\n'.join(lines)\n",
        "    \n",
        "    def batch_analyze(self, texts):\n",
        "        \"\"\"\n",
        "        Analyze multiple texts and return summary statistics.\n",
        "        \"\"\"\n",
        "        results = [self.analyze(text) for text in texts]\n",
        "        \n",
        "        # Calculate batch statistics\n",
        "        n = len(results)\n",
        "        needs_review = sum(1 for r in results if r['summary']['needs_review'])\n",
        "        \n",
        "        sentiment_counts = {'POSITIVE': 0, 'NEGATIVE': 0, 'NEUTRAL': 0}\n",
        "        for r in results:\n",
        "            sentiment_counts[r['summary']['consensus_label']] += 1\n",
        "        \n",
        "        avg_agreement = np.mean([r['summary']['agreement'] for r in results])\n",
        "        \n",
        "        return {\n",
        "            'total_analyzed': n,\n",
        "            'needs_review': needs_review,\n",
        "            'sentiment_distribution': sentiment_counts,\n",
        "            'average_agreement': avg_agreement,\n",
        "            'results': results,\n",
        "        }\n",
        "    \n",
        "    def format_batch_summary(self, batch_result):\n",
        "        \"\"\"Format batch analysis summary.\"\"\"\n",
        "        lines = []\n",
        "        \n",
        "        lines.append(\"â•”\" + \"â•\"*50 + \"â•—\")\n",
        "        lines.append(\"â•‘\" + \" BATCH ANALYSIS SUMMARY \".center(50) + \"â•‘\")\n",
        "        lines.append(\"â• \" + \"â•\"*50 + \"â•£\")\n",
        "        \n",
        "        lines.append(f\"â•‘  Total texts analyzed: {batch_result['total_analyzed']:25d} â•‘\")\n",
        "        lines.append(f\"â•‘  Flagged for review:   {batch_result['needs_review']:25d} â•‘\")\n",
        "        lines.append(f\"â•‘  Average agreement:    {batch_result['average_agreement']:24.0%} â•‘\")\n",
        "        \n",
        "        lines.append(\"â• \" + \"â•\"*50 + \"â•£\")\n",
        "        lines.append(\"â•‘\" + \" Sentiment Distribution: \".ljust(50) + \"â•‘\")\n",
        "        \n",
        "        dist = batch_result['sentiment_distribution']\n",
        "        total = sum(dist.values())\n",
        "        for sentiment, count in dist.items():\n",
        "            pct = count / total * 100 if total > 0 else 0\n",
        "            bar = 'â–ˆ' * int(pct / 5)\n",
        "            lines.append(f\"â•‘  {sentiment:10s}: {bar:20s} {count:3d} ({pct:.0f}%) â•‘\")\n",
        "        \n",
        "        lines.append(\"â•š\" + \"â•\"*50 + \"â•\")\n",
        "        \n",
        "        return '\\n'.join(lines)\n",
        "\n",
        "\n",
        "# Create dashboard\n",
        "dashboard = SentimentDashboard()\n",
        "print(\"Sentiment Dashboard initialized!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test individual analysis\n",
        "test_dashboard_texts = [\n",
        "    \"Absolutely love this product! Best purchase I've made all year!\",\n",
        "    \"Complete waste of money. Don't buy this.\",\n",
        "    \"It's okay I guess. Nothing special but does the job.\",\n",
        "    \"The service was great but the product broke after a week.\",\n",
        "]\n",
        "\n",
        "print(\"Individual Analysis Results:\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "for text in test_dashboard_texts:\n",
        "    result = dashboard.analyze(text)\n",
        "    print(dashboard.format_analysis(result))\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test batch analysis with simulated reviews\n",
        "sample_reviews = [\n",
        "    \"Five stars! Amazing quality and fast shipping!\",\n",
        "    \"Good product, would recommend to others.\",\n",
        "    \"Exactly as described. Happy with purchase.\",\n",
        "    \"Arrived late and was damaged. Very disappointed.\",\n",
        "    \"Meh. It's fine.\",\n",
        "    \"DO NOT BUY! Total scam!\",\n",
        "    \"Pretty good for the price.\",\n",
        "    \"Not what I expected but still useful.\",\n",
        "    \"Terrible customer service!\",\n",
        "    \"I love it! My whole family uses it now.\",\n",
        "    \"Could be better but it works.\",\n",
        "    \"Waste of money, broke immediately.\",\n",
        "]\n",
        "\n",
        "# Run batch analysis\n",
        "batch_result = dashboard.batch_analyze(sample_reviews)\n",
        "\n",
        "print(\"\\nBatch Analysis:\")\n",
        "print(\"=\"*70)\n",
        "print(dashboard.format_batch_summary(batch_result))\n",
        "\n",
        "# Show items flagged for review\n",
        "print(\"\\n\\nItems Flagged for Review:\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "flagged = [r for r in batch_result['results'] if r['summary']['needs_review']]\n",
        "for result in flagged:\n",
        "    print(f\"\\n\\\"{result['text']}\\\"\")\n",
        "    print(f\"  Flags: {', '.join(result['summary']['flags'])}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Interactive analysis - try your own texts\n",
        "# Uncomment and modify:\n",
        "\n",
        "# my_text = \"Your text here\"\n",
        "# result = dashboard.analyze(my_text)\n",
        "# print(dashboard.format_analysis(result))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Extension Ideas\n",
        "\n",
        "If you want to extend this project further:\n",
        "\n",
        "1. **Export to CSV/JSON**: Save analysis results for external reporting\n",
        "2. **Trend analysis**: Track sentiment changes over time\n",
        "3. **Category breakdown**: Analyze sentiment by product category\n",
        "4. **Language detection**: Route to appropriate language models\n",
        "5. **Aspect-based sentiment**: Detect sentiment for different aspects (quality, price, service)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Part 6: Wrap-Up\n",
        "\n",
        "## Key Takeaways\n",
        "\n",
        "1. **Different models give different results** because they're trained on different data from different domains\n",
        "\n",
        "2. **Training data shapes model behavior**:\n",
        "   - Movie review models see \"okay\" as disappointment\n",
        "   - Product review models have nuanced star ratings\n",
        "   - Social media models understand informal language\n",
        "\n",
        "3. **Always check model cards** to understand:\n",
        "   - What data the model was trained on\n",
        "   - What domains it works well for\n",
        "   - Known limitations and biases\n",
        "\n",
        "4. **Ensemble methods** combine multiple models for more robust predictions\n",
        "\n",
        "5. **Model agreement** is a useful signal:\n",
        "   - High agreement = confident prediction\n",
        "   - Low agreement = text may be ambiguous or edge case\n",
        "\n",
        "## Common Mistakes to Avoid\n",
        "\n",
        "| Mistake | Why It's a Problem |\n",
        "|---------|-------------------|\n",
        "| Using one model for all domains | Results may be unreliable for some text types |\n",
        "| Ignoring neutral sentiment | Binary models force neutral into pos/neg |\n",
        "| Not checking model cards | May use model on inappropriate domain |\n",
        "| Treating confidence as accuracy | High confidence â‰  correct prediction |\n",
        "| Ignoring sarcasm and negation | Models often struggle with these |\n",
        "\n",
        "## What's Next?\n",
        "\n",
        "In **Notebook 10: Pipeline Internals (Capstone)**, you'll learn:\n",
        "- What happens inside a pipeline (tokenization â†’ inference â†’ post-processing)\n",
        "- How to implement pipeline components manually\n",
        "- How to customize and extend pipelines\n",
        "\n",
        "This capstone notebook will tie together everything you've learned across all notebooks!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Solutions\n",
        "\n",
        "### Check Your Understanding (Quiz Answers)\n",
        "\n",
        "1. **B) Models are trained on different data from different domains** - Training data determines how a model interprets language\n",
        "2. **B) It forces neutral text into positive or negative** - Binary classification has no neutral option\n",
        "3. **B) The training data domain and known limitations** - These help you know if the model is appropriate\n",
        "4. **B) Combining predictions from multiple models** - Ensembles aggregate multiple opinions for robustness\n",
        "\n",
        "### Exercise 2: Key Insights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Key patterns that cause model disagreement:\n",
        "\n",
        "disagreement_patterns = {\n",
        "    'Negation': {\n",
        "        'examples': [\"I don't hate it\", \"Not bad\", \"Can't complain\"],\n",
        "        'why': 'Models may focus on negative words (\"hate\", \"bad\") and miss the negation',\n",
        "    },\n",
        "    'Sarcasm': {\n",
        "        'examples': [\"Oh great, just what I needed\", \"Wow, what a surprise\"],\n",
        "        'why': 'Literal interpretation conflicts with intended meaning',\n",
        "    },\n",
        "    'Mixed Sentiment': {\n",
        "        'examples': [\"Great product, terrible service\", \"Love the look, hate the price\"],\n",
        "        'why': 'Different aspects have different sentiments',\n",
        "    },\n",
        "    'Domain-Specific Language': {\n",
        "        'examples': [\"This code is sick!\", \"That movie was a bomb\"],\n",
        "        'why': 'Slang meanings differ from literal meanings',\n",
        "    },\n",
        "    'Hedged Statements': {\n",
        "        'examples': [\"I suppose it's okay\", \"Could have been worse\"],\n",
        "        'why': 'Weak positive/negative signals, models disagree on interpretation',\n",
        "    },\n",
        "}\n",
        "\n",
        "print(\"Patterns That Cause Model Disagreement:\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for pattern, info in disagreement_patterns.items():\n",
        "    print(f\"\\n{pattern}:\")\n",
        "    print(f\"  Examples: {', '.join(info['examples'])}\")\n",
        "    print(f\"  Why: {info['why']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Additional Resources\n",
        "\n",
        "- [Hugging Face Model Cards](https://huggingface.co/docs/hub/model-cards) - Understanding model documentation\n",
        "- [Stanford Sentiment Treebank](https://nlp.stanford.edu/sentiment/) - Original SST dataset\n",
        "- [SemEval Sentiment Tasks](https://semeval.github.io/) - Academic sentiment benchmarks\n",
        "- [TweetEval Benchmark](https://github.com/cardiffnlp/tweeteval) - Social media NLP benchmarks\n",
        "- [Ensemble Methods in NLP](https://arxiv.org/abs/2004.00790) - Academic overview of ensemble approaches"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
