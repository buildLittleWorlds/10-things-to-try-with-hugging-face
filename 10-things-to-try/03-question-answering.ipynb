{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Question Answering\n",
    "\n",
    "**Estimated Time**: ~2 hours\n",
    "\n",
    "**Prerequisites**: Notebooks 1-2 (understanding of tokenization, pipelines, confidence scores, and span extraction concepts from NER)\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. **Distinguish** between extractive and generative question answering approaches\n",
    "2. **Use** the `question-answering` pipeline to find answers within context\n",
    "3. **Interpret** start/end positions and confidence scores in QA output\n",
    "4. **Handle** unanswerable questions and low-confidence predictions\n",
    "5. **Build** a practical document Q&A system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Run this cell first. If you completed Notebooks 1-2, you already have the core packages ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForQuestionAnswering\n",
    "import torch\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 1: Conceptual Foundation\n",
    "\n",
    "## What is Question Answering?\n",
    "\n",
    "**In plain English**: Question Answering (QA) is the task of finding the answer to a question within a given text passage.\n",
    "\n",
    "**Technical definition**: QA models take a question and a context, then identify the span (start and end positions) in the context that answers the question.\n",
    "\n",
    "### Visual Example\n",
    "\n",
    "```\n",
    "Context: \"Albert Einstein was born in Ulm, Germany, in 1879. He developed \n",
    "          the theory of relativity and won the Nobel Prize in Physics in 1921.\"\n",
    "\n",
    "Question: \"Where was Einstein born?\"\n",
    "\n",
    "Answer:   \"Ulm, Germany\" (extracted from context at positions 27-39)\n",
    "           ↑___________↑\n",
    "           start       end\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two Types of Question Answering\n",
    "\n",
    "| Type | How It Works | Output | Example Models |\n",
    "|------|--------------|--------|----------------|\n",
    "| **Extractive QA** | Finds the answer span within the given context | Exact substring from context | BERT, RoBERTa, DistilBERT |\n",
    "| **Generative QA** | Generates an answer (may use context or not) | Newly generated text | GPT, T5, BART |\n",
    "\n",
    "This notebook focuses on **Extractive QA** - the model extracts answers directly from the context.\n",
    "\n",
    "```\n",
    "EXTRACTIVE QA (This Notebook):\n",
    "┌────────────────────────────────────────┐\n",
    "│ Context: \"The Eiffel Tower is 330m...\" │\n",
    "│ Question: \"How tall is Eiffel Tower?\"  │\n",
    "└────────────────┬───────────────────────┘\n",
    "                 │\n",
    "                 ▼\n",
    "          ┌────────────┐\n",
    "          │  \"330m\"    │  ← Extracted directly\n",
    "          └────────────┘\n",
    "\n",
    "GENERATIVE QA (Different approach):\n",
    "┌────────────────────────────────────────┐\n",
    "│ Question: \"How tall is Eiffel Tower?\"  │\n",
    "└────────────────┬───────────────────────┘\n",
    "                 │\n",
    "                 ▼\n",
    "┌─────────────────────────────────────────────┐\n",
    "│  \"The Eiffel Tower is approximately 330     │  ← Generated new text\n",
    "│   meters (1,083 feet) tall.\"                │\n",
    "└─────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How Extractive QA Works: Start/End Prediction\n",
    "\n",
    "Remember from Notebook 2 how NER finds entity spans? QA works similarly - but instead of finding all entities, it finds the **single span** that answers the question.\n",
    "\n",
    "```\n",
    "Context tokens:  [CLS] The Eiffel Tower is 330 meters tall . [SEP]\n",
    "                   0    1     2      3    4   5    6     7  8   9\n",
    "\n",
    "Question: \"How tall is the Eiffel Tower?\"\n",
    "\n",
    "Model predicts:\n",
    "  Start position: 5 (\"330\")\n",
    "  End position:   7 (\"tall\")\n",
    "\n",
    "Answer: tokens 5-7 = \"330 meters tall\"\n",
    "```\n",
    "\n",
    "The model outputs **two probability distributions**:\n",
    "- One for the start position of the answer\n",
    "- One for the end position of the answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connection to Notebook 2: Span Extraction\n",
    "\n",
    "Both NER and QA are **span extraction** tasks:\n",
    "\n",
    "```\n",
    "NER (Notebook 2):                    QA (This Notebook):\n",
    "┌──────────────────┐                 ┌──────────────────┐\n",
    "│   BERT Encoder   │                 │   BERT Encoder   │\n",
    "│  (same weights)  │                 │  (same weights)  │\n",
    "└────────┬─────────┘                 └────────┬─────────┘\n",
    "         │                                    │\n",
    "         ▼                                    ▼\n",
    "┌──────────────────┐                 ┌──────────────────┐\n",
    "│  Token Class Head│                 │  Start/End Head  │\n",
    "│  (B-PER, I-PER..)│                 │  (position pred) │\n",
    "└──────────────────┘                 └──────────────────┘\n",
    "         │                                    │\n",
    "         ▼                                    ▼\n",
    "  Multiple spans                       Single span\n",
    "  (many entities)                      (one answer)\n",
    "```\n",
    "\n",
    "The key difference: NER finds **many** spans, QA finds **one** answer span."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real-World Applications\n",
    "\n",
    "QA powers many practical applications:\n",
    "\n",
    "- **Customer Support**: Automatically answer FAQs from documentation\n",
    "- **Search Engines**: Extract direct answers from web pages (\"featured snippets\")\n",
    "- **Legal/Medical Research**: Find specific information in long documents\n",
    "- **Virtual Assistants**: Answer factual questions from knowledge bases\n",
    "- **Education**: Auto-grade reading comprehension questions\n",
    "- **Enterprise Search**: Find answers in company wikis and documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Terminology\n",
    "\n",
    "| Term | Definition |\n",
    "|------|------------|\n",
    "| **Context** | The passage of text containing the answer |\n",
    "| **Question** | What we're asking about the context |\n",
    "| **Answer Span** | The substring from context that answers the question |\n",
    "| **Start/End Position** | Character indices where the answer begins and ends |\n",
    "| **Extractive QA** | Finding answers by extracting spans from context |\n",
    "| **Generative QA** | Creating answers by generating new text |\n",
    "| **Unanswerable** | Question cannot be answered from the given context |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Your Understanding\n",
    "\n",
    "Before moving on, try to answer these questions (answers at the end):\n",
    "\n",
    "1. What type of QA extracts answers directly from the given context?\n",
    "   - A) Generative QA\n",
    "   - B) Extractive QA\n",
    "   - C) Creative QA\n",
    "\n",
    "2. What does a QA model predict to find an answer?\n",
    "   - A) The exact answer text\n",
    "   - B) Start and end positions within the context\n",
    "   - C) Whether the context is relevant\n",
    "\n",
    "3. How does QA relate to NER from Notebook 2?\n",
    "   - A) They're completely unrelated\n",
    "   - B) Both are span extraction tasks using similar architectures\n",
    "   - C) QA always uses NER as a preprocessing step\n",
    "\n",
    "4. What happens when a question cannot be answered from the context?\n",
    "   - A) The model always makes up an answer\n",
    "   - B) The model returns an empty string\n",
    "   - C) The model may return low confidence or empty/incorrect span"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: Basic Implementation\n",
    "\n",
    "## Your First QA Pipeline\n",
    "\n",
    "Let's create a QA pipeline and find answers within context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a QA pipeline\n",
    "# The default model is distilbert-base-cased-distilled-squad\n",
    "qa = pipeline(\"question-answering\")\n",
    "\n",
    "# Define context and question\n",
    "context = \"\"\"\n",
    "Albert Einstein was born in Ulm, Germany, on March 14, 1879. He is widely regarded \n",
    "as one of the greatest physicists of all time. Einstein developed the theory of \n",
    "relativity, one of the two pillars of modern physics. He received the Nobel Prize \n",
    "in Physics in 1921 for his explanation of the photoelectric effect.\n",
    "\"\"\"\n",
    "\n",
    "question = \"Where was Einstein born?\"\n",
    "\n",
    "# Get the answer\n",
    "result = qa(question=question, context=context)\n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {result['answer']}\")\n",
    "print(f\"Confidence: {result['score']:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Output\n",
    "\n",
    "The QA pipeline returns a dictionary with:\n",
    "\n",
    "- `answer`: The extracted text that answers the question\n",
    "- `score`: Confidence score (0 to 1)\n",
    "- `start`: Character position where answer begins\n",
    "- `end`: Character position where answer ends\n",
    "\n",
    "Let's examine the result in detail:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the result in detail\n",
    "print(\"Detailed QA Result:\")\n",
    "print(\"=\"*40)\n",
    "for key, value in result.items():\n",
    "    if key == 'score':\n",
    "        print(f\"  {key:8}: {value:.4f} ({value:.2%})\")\n",
    "    else:\n",
    "        print(f\"  {key:8}: {value}\")\n",
    "\n",
    "# Verify the span is correct\n",
    "print(f\"\\nVerification: context[{result['start']}:{result['end']}]\")\n",
    "print(f\"  = '{context[result['start']:result['end']]}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Asking Multiple Questions\n",
    "\n",
    "Let's ask several questions about the same context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple questions about Einstein\n",
    "questions = [\n",
    "    \"Where was Einstein born?\",\n",
    "    \"When was Einstein born?\",\n",
    "    \"What did Einstein develop?\",\n",
    "    \"When did Einstein receive the Nobel Prize?\",\n",
    "    \"What was the Nobel Prize for?\",\n",
    "]\n",
    "\n",
    "print(f\"Context: {context.strip()[:100]}...\\n\")\n",
    "print(\"Questions and Answers:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for q in questions:\n",
    "    result = qa(question=q, context=context)\n",
    "    confidence = \"High\" if result['score'] > 0.8 else \"Medium\" if result['score'] > 0.5 else \"Low\"\n",
    "    print(f\"Q: {q}\")\n",
    "    print(f\"A: {result['answer']} ({result['score']:.0%} - {confidence})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confidence Score Interpretation\n",
    "\n",
    "Like in previous notebooks, confidence scores tell us how certain the model is:\n",
    "\n",
    "| Score Range | Interpretation | Action |\n",
    "|-------------|----------------|--------|\n",
    "| **0.8 - 1.0** | High confidence | Trust the answer |\n",
    "| **0.5 - 0.8** | Medium confidence | Verify if critical |\n",
    "| **0.2 - 0.5** | Low confidence | Answer may be wrong |\n",
    "| **< 0.2** | Very low | Question likely unanswerable |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate confidence levels with different question types\n",
    "test_questions = [\n",
    "    # Clear answer in context\n",
    "    (\"What year was Einstein born?\", \"Clearly answerable\"),\n",
    "    # Answer requires inference\n",
    "    (\"How old was Einstein when he got the Nobel Prize?\", \"Requires calculation\"),\n",
    "    # Not in context\n",
    "    (\"What was Einstein's favorite food?\", \"Not in context\"),\n",
    "]\n",
    "\n",
    "print(\"Confidence Analysis:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for q, note in test_questions:\n",
    "    result = qa(question=q, context=context)\n",
    "    bar = \"█\" * int(result['score'] * 20)\n",
    "    print(f\"Q: {q}\")\n",
    "    print(f\"   Type: {note}\")\n",
    "    print(f\"   A: '{result['answer']}' | {result['score']:.0%} {bar}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 1: Wikipedia QA (Guided)\n",
    "\n",
    "**Difficulty**: Basic | **Time**: 10-15 minutes\n",
    "\n",
    "**Your task**: Ask questions about a Wikipedia-style passage and analyze the confidence scores.\n",
    "\n",
    "### Step 1: Process this passage about the Moon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wikipedia-style passage about the Moon\n",
    "moon_context = \"\"\"\n",
    "The Moon is Earth's only natural satellite. It orbits at an average distance of \n",
    "384,400 kilometers from Earth. The Moon's diameter is 3,474 kilometers, making it \n",
    "the fifth-largest satellite in the Solar System. The Moon was first visited by the \n",
    "Soviet spacecraft Luna 2 in 1959. The first crewed lunar landing was Apollo 11 in \n",
    "1969, when Neil Armstrong became the first person to walk on the Moon. The Moon's \n",
    "gravitational influence produces Earth's tides and slightly lengthens Earth's day.\n",
    "\"\"\"\n",
    "\n",
    "# Questions to ask\n",
    "moon_questions = [\n",
    "    \"How far is the Moon from Earth?\",\n",
    "    \"What is the Moon's diameter?\",\n",
    "    \"When was the Moon first visited by spacecraft?\",\n",
    "    \"Who was the first person to walk on the Moon?\",\n",
    "    \"What does the Moon's gravity affect?\",\n",
    "]\n",
    "\n",
    "print(\"Moon Q&A Session:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "results = []\n",
    "for q in moon_questions:\n",
    "    result = qa(question=q, context=moon_context)\n",
    "    results.append(result)\n",
    "    print(f\"Q: {q}\")\n",
    "    print(f\"A: {result['answer']} ({result['score']:.0%})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Analyze confidence distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze confidence distribution\n",
    "print(\"Confidence Distribution:\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "for q, r in zip(moon_questions, results):\n",
    "    bar = \"█\" * int(r['score'] * 30)\n",
    "    print(f\"{r['score']:.0%} {bar}\")\n",
    "\n",
    "avg_confidence = sum(r['score'] for r in results) / len(results)\n",
    "print(f\"\\nAverage confidence: {avg_confidence:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Try your own questions\n",
    "\n",
    "Add 3 of your own questions about the Moon passage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Add your own questions about the Moon\n",
    "my_questions = [\n",
    "    # Replace with your own questions\n",
    "    \"Your question 1 here\",\n",
    "    \"Your question 2 here\",\n",
    "    \"Your question 3 here\",\n",
    "]\n",
    "\n",
    "for q in my_questions:\n",
    "    result = qa(question=q, context=moon_context)\n",
    "    print(f\"Q: {q}\")\n",
    "    print(f\"A: {result['answer']} ({result['score']:.0%})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 3: Intermediate Exploration\n",
    "\n",
    "## Handling Unanswerable Questions\n",
    "\n",
    "A major challenge in QA: what happens when the answer isn't in the context?\n",
    "\n",
    "Some models are trained on SQuAD 2.0, which includes unanswerable questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with unanswerable questions\n",
    "context = \"\"\"\n",
    "Python is a high-level programming language created by Guido van Rossum and \n",
    "first released in 1991. It emphasizes code readability and allows programmers \n",
    "to express concepts in fewer lines of code than languages like C++ or Java.\n",
    "\"\"\"\n",
    "\n",
    "questions = [\n",
    "    # Answerable\n",
    "    (\"Who created Python?\", True),\n",
    "    (\"When was Python released?\", True),\n",
    "    # Not answerable from context\n",
    "    (\"What company does Guido work for?\", False),\n",
    "    (\"What is the latest Python version?\", False),\n",
    "    (\"How many Python developers exist?\", False),\n",
    "]\n",
    "\n",
    "print(\"Testing Answerable vs Unanswerable Questions:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for q, is_answerable in questions:\n",
    "    result = qa(question=q, context=context)\n",
    "    status = \"✓ Answerable\" if is_answerable else \"✗ Not in context\"\n",
    "    warning = \"\" if result['score'] > 0.3 else \" ⚠️ LOW CONFIDENCE\"\n",
    "    \n",
    "    print(f\"[{status}]\")\n",
    "    print(f\"  Q: {q}\")\n",
    "    print(f\"  A: '{result['answer']}' ({result['score']:.0%}){warning}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a Model Trained on SQuAD 2.0\n",
    "\n",
    "SQuAD 2.0 models are trained to recognize unanswerable questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a SQuAD 2.0 model (better at detecting unanswerable questions)\n",
    "print(\"Loading SQuAD 2.0 model (better with unanswerable questions)...\")\n",
    "qa_squad2 = pipeline(\"question-answering\", model=\"deepset/roberta-base-squad2\")\n",
    "print(\"Model loaded!\\n\")\n",
    "\n",
    "# Test the same questions\n",
    "print(\"Testing with SQuAD 2.0 Model:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for q, is_answerable in questions:\n",
    "    result = qa_squad2(question=q, context=context)\n",
    "    status = \"✓ Answerable\" if is_answerable else \"✗ Not in context\"\n",
    "    \n",
    "    print(f\"[{status}]\")\n",
    "    print(f\"  Q: {q}\")\n",
    "    print(f\"  A: '{result['answer']}' ({result['score']:.0%})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategy for Handling Low Confidence\n",
    "\n",
    "A practical approach: set a confidence threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_with_threshold(qa_pipeline, question, context, threshold=0.3):\n",
    "    \"\"\"\n",
    "    Answer a question with a confidence threshold.\n",
    "    \n",
    "    Returns:\n",
    "        dict with 'answer', 'confidence', and 'is_confident'\n",
    "    \"\"\"\n",
    "    result = qa_pipeline(question=question, context=context)\n",
    "    \n",
    "    is_confident = result['score'] >= threshold\n",
    "    \n",
    "    return {\n",
    "        'answer': result['answer'] if is_confident else \"I cannot confidently answer this question.\",\n",
    "        'raw_answer': result['answer'],\n",
    "        'confidence': result['score'],\n",
    "        'is_confident': is_confident,\n",
    "    }\n",
    "\n",
    "# Test the threshold approach\n",
    "test_questions = [\n",
    "    \"Who created Python?\",\n",
    "    \"What company does Guido work for?\",\n",
    "]\n",
    "\n",
    "print(\"Using Confidence Threshold (0.3):\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for q in test_questions:\n",
    "    result = answer_with_threshold(qa, q, context)\n",
    "    status = \"✓\" if result['is_confident'] else \"✗\"\n",
    "    print(f\"{status} Q: {q}\")\n",
    "    print(f\"  Answer: {result['answer']}\")\n",
    "    print(f\"  (Raw: '{result['raw_answer']}' at {result['confidence']:.0%})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing QA Models\n",
    "\n",
    "Different models have different strengths. Let's compare:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We already have two models loaded:\n",
    "# - qa: DistilBERT trained on SQuAD 1.1 (default, faster)\n",
    "# - qa_squad2: RoBERTa trained on SQuAD 2.0 (better with unanswerable)\n",
    "\n",
    "test_context = \"\"\"\n",
    "The Great Wall of China is a series of fortifications built along the northern \n",
    "borders of ancient Chinese states. Construction began in the 7th century BC and \n",
    "continued for centuries. The most well-known sections were built during the Ming \n",
    "Dynasty (1368-1644). The wall stretches over 21,000 kilometers.\n",
    "\"\"\"\n",
    "\n",
    "test_questions = [\n",
    "    \"How long is the Great Wall?\",\n",
    "    \"When did construction begin?\",\n",
    "    \"How many workers died building it?\",  # Not in context\n",
    "]\n",
    "\n",
    "print(\"Model Comparison:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for q in test_questions:\n",
    "    print(f\"\\nQ: {q}\")\n",
    "    \n",
    "    result1 = qa(question=q, context=test_context)\n",
    "    result2 = qa_squad2(question=q, context=test_context)\n",
    "    \n",
    "    print(f\"  DistilBERT: '{result1['answer']}' ({result1['score']:.0%})\")\n",
    "    print(f\"  RoBERTa:    '{result2['answer']}' ({result2['score']:.0%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 2: Unanswerable Question Detection (Semi-guided)\n",
    "\n",
    "**Difficulty**: Intermediate | **Time**: 15-20 minutes\n",
    "\n",
    "**Your task**: Write a function that classifies questions as answerable or unanswerable based on confidence scores and answer characteristics.\n",
    "\n",
    "**Hints**:\n",
    "1. Low confidence often indicates unanswerable questions\n",
    "2. Very short answers might be suspicious\n",
    "3. You might want to use multiple thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "def classify_answerability(qa_pipeline, question, context, \n",
    "                          confidence_threshold=0.3,\n",
    "                          min_answer_length=1):\n",
    "    \"\"\"\n",
    "    Classify whether a question is answerable from the given context.\n",
    "    \n",
    "    Args:\n",
    "        qa_pipeline: The QA pipeline to use\n",
    "        question: The question to ask\n",
    "        context: The context to search in\n",
    "        confidence_threshold: Minimum confidence to consider answerable\n",
    "        min_answer_length: Minimum answer length (characters)\n",
    "    \n",
    "    Returns:\n",
    "        dict with classification result and reasoning\n",
    "    \"\"\"\n",
    "    result = qa_pipeline(question=question, context=context)\n",
    "    \n",
    "    # Gather evidence\n",
    "    confidence = result['score']\n",
    "    answer_length = len(result['answer'].strip())\n",
    "    \n",
    "    # Make decision with reasoning\n",
    "    reasons = []\n",
    "    \n",
    "    # Check confidence\n",
    "    if confidence < confidence_threshold:\n",
    "        reasons.append(f\"Low confidence ({confidence:.0%} < {confidence_threshold:.0%})\")\n",
    "    \n",
    "    # Check answer length\n",
    "    if answer_length < min_answer_length:\n",
    "        reasons.append(f\"Answer too short ({answer_length} chars)\")\n",
    "    \n",
    "    # Determine answerability\n",
    "    is_answerable = len(reasons) == 0\n",
    "    \n",
    "    return {\n",
    "        'question': question,\n",
    "        'answer': result['answer'],\n",
    "        'confidence': confidence,\n",
    "        'is_answerable': is_answerable,\n",
    "        'reasons': reasons if reasons else [\"Confidence above threshold\"],\n",
    "    }\n",
    "\n",
    "\n",
    "# Test the classifier\n",
    "test_context = \"\"\"\n",
    "Amazon Web Services (AWS) was launched in 2006, offering cloud computing services.\n",
    "It provides services in computing, storage, and databases. AWS operates from data \n",
    "centers around the world. Jeff Bezos founded Amazon in 1994 in Seattle.\n",
    "\"\"\"\n",
    "\n",
    "test_questions = [\n",
    "    \"When was AWS launched?\",            # Answerable\n",
    "    \"Who founded Amazon?\",                # Answerable  \n",
    "    \"How much revenue does AWS generate?\", # Not in context\n",
    "    \"What is the AWS CEO's name?\",        # Not in context\n",
    "    \"Where is Amazon headquartered?\",     # Partially (Seattle mentioned)\n",
    "]\n",
    "\n",
    "print(\"Answerability Classification:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for q in test_questions:\n",
    "    result = classify_answerability(qa_squad2, q, test_context)\n",
    "    status = \"✓ ANSWERABLE\" if result['is_answerable'] else \"✗ UNANSWERABLE\"\n",
    "    \n",
    "    print(f\"\\n{status}\")\n",
    "    print(f\"  Q: {result['question']}\")\n",
    "    print(f\"  A: '{result['answer']}' ({result['confidence']:.0%})\")\n",
    "    print(f\"  Reasoning: {', '.join(result['reasons'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 4: Advanced Topics\n",
    "\n",
    "## Under the Hood: Start and End Logits\n",
    "\n",
    "Let's see what the QA model does internally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and tokenizer separately\n",
    "model_name = \"distilbert-base-cased-distilled-squad\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "\n",
    "print(f\"Model: {model_name}\")\n",
    "print(f\"Model type: {type(model).__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step-by-step QA\n",
    "context = \"Paris is the capital of France. It is known for the Eiffel Tower.\"\n",
    "question = \"What is the capital of France?\"\n",
    "\n",
    "# STEP 1: Tokenization\n",
    "# QA models take both question and context together\n",
    "inputs = tokenizer(\n",
    "    question, \n",
    "    context, \n",
    "    return_tensors=\"pt\",\n",
    "    return_offsets_mapping=True,\n",
    "    truncation=True,\n",
    "    max_length=512,\n",
    ")\n",
    "\n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "\n",
    "print(\"STEP 1 - Tokenization:\")\n",
    "print(f\"  Question: '{question}'\")\n",
    "print(f\"  Context: '{context}'\")\n",
    "print(f\"\\n  Tokens ({len(tokens)} total):\")\n",
    "for i, token in enumerate(tokens):\n",
    "    print(f\"    {i:2}: {token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2: Model inference\n",
    "# Remove offset_mapping before passing to model\n",
    "model_inputs = {k: v for k, v in inputs.items() if k != 'offset_mapping'}\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**model_inputs)\n",
    "\n",
    "print(\"STEP 2 - Model Inference:\")\n",
    "print(f\"  Start logits shape: {outputs.start_logits.shape}\")\n",
    "print(f\"  End logits shape: {outputs.end_logits.shape}\")\n",
    "print(\"  (Each has one score per token position)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 3: Convert logits to probabilities and find best positions\n",
    "start_probs = torch.softmax(outputs.start_logits, dim=1)\n",
    "end_probs = torch.softmax(outputs.end_logits, dim=1)\n",
    "\n",
    "# Get best positions\n",
    "start_idx = torch.argmax(start_probs).item()\n",
    "end_idx = torch.argmax(end_probs).item()\n",
    "\n",
    "print(\"STEP 3 - Position Prediction:\")\n",
    "print(f\"  Best start position: {start_idx} ('{tokens[start_idx]}')\")\n",
    "print(f\"  Best end position: {end_idx} ('{tokens[end_idx]}')\")\n",
    "print(f\"\\n  Top 3 start positions:\")\n",
    "for idx in torch.topk(start_probs[0], 3).indices:\n",
    "    print(f\"    Position {idx.item():2}: '{tokens[idx.item()]}' ({start_probs[0][idx]:.2%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 4: Extract the answer\n",
    "answer_tokens = tokens[start_idx:end_idx+1]\n",
    "answer = tokenizer.convert_tokens_to_string(answer_tokens)\n",
    "\n",
    "# Calculate confidence (product of start and end probabilities)\n",
    "confidence = start_probs[0][start_idx] * end_probs[0][end_idx]\n",
    "\n",
    "print(\"STEP 4 - Answer Extraction:\")\n",
    "print(f\"  Answer tokens: {answer_tokens}\")\n",
    "print(f\"  Answer text: '{answer}'\")\n",
    "print(f\"  Start prob: {start_probs[0][start_idx]:.2%}\")\n",
    "print(f\"  End prob: {end_probs[0][end_idx]:.2%}\")\n",
    "print(f\"  Combined confidence: {confidence:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize start and end probabilities\n",
    "print(\"Position Probability Visualization:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Pos':>3} {'Token':>15} {'Start':>10} {'End':>10}\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "for i, token in enumerate(tokens):\n",
    "    start_bar = \"█\" * int(start_probs[0][i].item() * 30)\n",
    "    end_bar = \"█\" * int(end_probs[0][i].item() * 30)\n",
    "    \n",
    "    # Highlight the answer positions\n",
    "    marker = \"\"\n",
    "    if i == start_idx:\n",
    "        marker = \" ← START\"\n",
    "    elif i == end_idx:\n",
    "        marker = \" ← END\"\n",
    "    \n",
    "    if start_probs[0][i] > 0.05 or end_probs[0][i] > 0.05:\n",
    "        print(f\"{i:3} {token:>15} {start_probs[0][i]:>8.1%} {end_probs[0][i]:>8.1%}{marker}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Long Contexts\n",
    "\n",
    "QA models have maximum token limits (usually 512). For longer documents, you need to:\n",
    "\n",
    "1. **Split into chunks**: Divide the document into overlapping chunks\n",
    "2. **Process each chunk**: Run QA on each chunk\n",
    "3. **Merge results**: Take the answer with highest confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qa_on_long_context(qa_pipeline, question, context, chunk_size=400, overlap=100):\n",
    "    \"\"\"\n",
    "    Answer questions on long contexts by chunking.\n",
    "    \n",
    "    Args:\n",
    "        qa_pipeline: The QA pipeline to use\n",
    "        question: The question to ask\n",
    "        context: The (possibly long) context\n",
    "        chunk_size: Maximum characters per chunk\n",
    "        overlap: Overlap between chunks\n",
    "    \n",
    "    Returns:\n",
    "        Best answer across all chunks\n",
    "    \"\"\"\n",
    "    # Simple sentence-aware chunking\n",
    "    sentences = context.replace('\\n', ' ').split('. ')\n",
    "    \n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        if len(current_chunk) + len(sentence) < chunk_size:\n",
    "            current_chunk += sentence + \". \"\n",
    "        else:\n",
    "            if current_chunk:\n",
    "                chunks.append(current_chunk.strip())\n",
    "            current_chunk = sentence + \". \"\n",
    "    \n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "    \n",
    "    # Process each chunk\n",
    "    best_result = None\n",
    "    best_score = -1\n",
    "    \n",
    "    for i, chunk in enumerate(chunks):\n",
    "        result = qa_pipeline(question=question, context=chunk)\n",
    "        result['chunk_index'] = i\n",
    "        result['chunk'] = chunk[:50] + \"...\"\n",
    "        \n",
    "        if result['score'] > best_score:\n",
    "            best_score = result['score']\n",
    "            best_result = result\n",
    "    \n",
    "    return best_result, chunks\n",
    "\n",
    "# Test with a longer context\n",
    "long_context = \"\"\"\n",
    "The history of artificial intelligence began in antiquity, with myths and stories \n",
    "of artificial beings endowed with intelligence. The modern field of AI research \n",
    "was founded at a workshop at Dartmouth College in 1956. The term \"artificial \n",
    "intelligence\" was coined by John McCarthy for this workshop.\n",
    "\n",
    "Early AI research focused on symbolic methods and problem solving. In the 1960s \n",
    "and 1970s, researchers developed expert systems that could reason about specialized \n",
    "domains. However, progress was slower than expected, leading to periods of reduced \n",
    "funding known as \"AI winters.\"\n",
    "\n",
    "Machine learning emerged as a major approach in the 1990s. Deep learning, using \n",
    "neural networks with many layers, achieved breakthroughs starting in 2012 with \n",
    "AlexNet's victory in the ImageNet competition. This was developed by Alex Krizhevsky, \n",
    "Ilya Sutskever, and Geoffrey Hinton at the University of Toronto.\n",
    "\n",
    "Today, AI is used in many applications including virtual assistants, autonomous \n",
    "vehicles, medical diagnosis, and language translation. Major AI research labs \n",
    "include OpenAI, Google DeepMind, and Anthropic. The development of large language \n",
    "models like GPT and Claude has enabled new capabilities in natural language processing.\n",
    "\"\"\"\n",
    "\n",
    "test_questions = [\n",
    "    \"When was the term artificial intelligence coined?\",\n",
    "    \"Who developed AlexNet?\",\n",
    "    \"What are AI winters?\",\n",
    "]\n",
    "\n",
    "print(\"Long Context Q&A:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for q in test_questions:\n",
    "    result, chunks = qa_on_long_context(qa, q, long_context)\n",
    "    print(f\"Q: {q}\")\n",
    "    print(f\"A: '{result['answer']}' ({result['score']:.0%})\")\n",
    "    print(f\"   Found in chunk {result['chunk_index']+1}/{len(chunks)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Considerations\n",
    "\n",
    "| Consideration | Recommendation |\n",
    "|---------------|----------------|\n",
    "| **Model size** | DistilBERT for speed, RoBERTa-large for accuracy |\n",
    "| **Long documents** | Chunk with overlap, merge by confidence |\n",
    "| **Unanswerable questions** | Use SQuAD 2.0 models, set confidence threshold |\n",
    "| **Multiple questions** | Batch process when possible |\n",
    "| **Domain-specific** | Fine-tune on domain data if available |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitations of Extractive QA\n",
    "\n",
    "1. **Answer must be in context**: Cannot synthesize or infer answers\n",
    "2. **Single span limitation**: Cannot combine information from multiple places\n",
    "3. **No reasoning**: Cannot perform math, logic, or multi-hop reasoning\n",
    "4. **Context length limit**: 512 tokens for most models\n",
    "5. **Exact match requirement**: Paraphrased answers won't be found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate limitations\n",
    "limitation_examples = [\n",
    "    # Requires synthesis from multiple sentences\n",
    "    {\n",
    "        'context': \"John was born in 1990. Mary was born in 1995.\",\n",
    "        'question': \"Who is older, John or Mary?\",\n",
    "        'issue': \"Requires comparison across sentences\"\n",
    "    },\n",
    "    # Requires calculation\n",
    "    {\n",
    "        'context': \"The company earned $10 million in Q1 and $15 million in Q2.\",\n",
    "        'question': \"What was the total earnings for both quarters?\",\n",
    "        'issue': \"Requires mathematical calculation\"\n",
    "    },\n",
    "    # Multi-hop reasoning\n",
    "    {\n",
    "        'context': \"Alice manages Bob. Bob manages Carol.\",\n",
    "        'question': \"Who is Carol's manager's manager?\",\n",
    "        'issue': \"Requires multi-hop reasoning\"\n",
    "    },\n",
    "]\n",
    "\n",
    "print(\"QA Limitations:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for ex in limitation_examples:\n",
    "    result = qa(question=ex['question'], context=ex['context'])\n",
    "    print(f\"\\nIssue: {ex['issue']}\")\n",
    "    print(f\"Context: '{ex['context']}'\")\n",
    "    print(f\"Q: {ex['question']}\")\n",
    "    print(f\"A: '{result['answer']}' ({result['score']:.0%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 3: Multi-Question Ranking (Independent)\n",
    "\n",
    "**Difficulty**: Advanced | **Time**: 15-20 minutes\n",
    "\n",
    "**Your task**: Build a system that takes multiple questions and ranks them by how well they can be answered from a given context.\n",
    "\n",
    "**Requirements**:\n",
    "1. Accept a context and list of questions\n",
    "2. Rank questions by answerability (confidence)\n",
    "3. Categorize as \"Easily Answered\", \"Partially Answered\", \"Cannot Answer\"\n",
    "4. Provide a summary report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "class QuestionRanker:\n",
    "    \"\"\"\n",
    "    Ranks questions by how well they can be answered from a given context.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, qa_pipeline=None):\n",
    "        \"\"\"Initialize with a QA pipeline.\"\"\"\n",
    "        self.qa = qa_pipeline or pipeline(\"question-answering\")\n",
    "        \n",
    "        # Thresholds for categorization\n",
    "        self.high_threshold = 0.7\n",
    "        self.low_threshold = 0.3\n",
    "    \n",
    "    def rank_questions(self, context, questions):\n",
    "        \"\"\"\n",
    "        Rank questions by answerability.\n",
    "        \n",
    "        Args:\n",
    "            context: The text to answer from\n",
    "            questions: List of questions\n",
    "            \n",
    "        Returns:\n",
    "            List of dicts with question, answer, score, category\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for q in questions:\n",
    "            answer = self.qa(question=q, context=context)\n",
    "            \n",
    "            # Categorize\n",
    "            if answer['score'] >= self.high_threshold:\n",
    "                category = \"Easily Answered\"\n",
    "            elif answer['score'] >= self.low_threshold:\n",
    "                category = \"Partially Answered\"\n",
    "            else:\n",
    "                category = \"Cannot Answer\"\n",
    "            \n",
    "            results.append({\n",
    "                'question': q,\n",
    "                'answer': answer['answer'],\n",
    "                'score': answer['score'],\n",
    "                'category': category,\n",
    "            })\n",
    "        \n",
    "        # Sort by score (highest first)\n",
    "        results.sort(key=lambda x: x['score'], reverse=True)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def get_summary(self, context, questions):\n",
    "        \"\"\"\n",
    "        Generate a summary report of question answerability.\n",
    "        \"\"\"\n",
    "        results = self.rank_questions(context, questions)\n",
    "        \n",
    "        lines = []\n",
    "        lines.append(\"Question Answerability Report\")\n",
    "        lines.append(\"=\" * 50)\n",
    "        lines.append(f\"Total questions: {len(questions)}\")\n",
    "        lines.append(\"\")\n",
    "        \n",
    "        # Group by category\n",
    "        categories = {\"Easily Answered\": [], \"Partially Answered\": [], \"Cannot Answer\": []}\n",
    "        for r in results:\n",
    "            categories[r['category']].append(r)\n",
    "        \n",
    "        # Print summary\n",
    "        icons = {\"Easily Answered\": \"✓\", \"Partially Answered\": \"~\", \"Cannot Answer\": \"✗\"}\n",
    "        \n",
    "        for category in [\"Easily Answered\", \"Partially Answered\", \"Cannot Answer\"]:\n",
    "            count = len(categories[category])\n",
    "            icon = icons[category]\n",
    "            lines.append(f\"{icon} {category}: {count} questions\")\n",
    "            \n",
    "            for r in categories[category]:\n",
    "                lines.append(f\"    [{r['score']:.0%}] {r['question']}\")\n",
    "                lines.append(f\"          → {r['answer']}\")\n",
    "            lines.append(\"\")\n",
    "        \n",
    "        return '\\n'.join(lines)\n",
    "\n",
    "\n",
    "# Test the ranker\n",
    "ranker = QuestionRanker(qa_squad2)\n",
    "\n",
    "context = \"\"\"\n",
    "SpaceX is an American aerospace manufacturer founded in 2002 by Elon Musk. \n",
    "The company's headquarters is located in Hawthorne, California. SpaceX has \n",
    "developed the Falcon 9 rocket and the Dragon spacecraft. The Falcon 9 has \n",
    "become the most frequently launched rocket in the world. SpaceX's goal is \n",
    "to reduce space transportation costs and enable Mars colonization.\n",
    "\"\"\"\n",
    "\n",
    "questions = [\n",
    "    \"Who founded SpaceX?\",\n",
    "    \"When was SpaceX founded?\",\n",
    "    \"Where is SpaceX headquarters?\",\n",
    "    \"What rockets has SpaceX developed?\",\n",
    "    \"What is SpaceX's revenue?\",\n",
    "    \"How many employees does SpaceX have?\",\n",
    "    \"What is SpaceX's main goal?\",\n",
    "    \"Who is the current CEO of SpaceX?\",\n",
    "]\n",
    "\n",
    "print(ranker.get_summary(context, questions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 5: Mini-Project\n",
    "\n",
    "## Project: Document Q&A System\n",
    "\n",
    "**Scenario**: You're building a customer support tool that answers questions about product documentation.\n",
    "\n",
    "**Your goal**: Build a `DocumentQA` class that:\n",
    "1. Takes a document (like a product manual) as input\n",
    "2. Answers user questions from the document\n",
    "3. Handles cases where the answer isn't in the document\n",
    "4. Provides confidence levels and source snippets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MINI-PROJECT: Document Q&A System\n",
    "# ==================================\n",
    "\n",
    "class DocumentQA:\n",
    "    \"\"\"\n",
    "    A document-based question answering system.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, use_squad2=True):\n",
    "        \"\"\"\n",
    "        Initialize the Document QA system.\n",
    "        \n",
    "        Args:\n",
    "            use_squad2: Whether to use SQuAD 2.0 model (better for unanswerable questions)\n",
    "        \"\"\"\n",
    "        if use_squad2:\n",
    "            self.qa = pipeline(\"question-answering\", model=\"deepset/roberta-base-squad2\")\n",
    "        else:\n",
    "            self.qa = pipeline(\"question-answering\")\n",
    "        \n",
    "        self.document = None\n",
    "        self.sections = []\n",
    "        self.confidence_threshold = 0.3\n",
    "    \n",
    "    def load_document(self, document_text, section_separator=\"\\n\\n\"):\n",
    "        \"\"\"\n",
    "        Load a document for Q&A.\n",
    "        \n",
    "        Args:\n",
    "            document_text: The full document text\n",
    "            section_separator: How to split into sections\n",
    "        \"\"\"\n",
    "        self.document = document_text\n",
    "        self.sections = [s.strip() for s in document_text.split(section_separator) if s.strip()]\n",
    "        return len(self.sections)\n",
    "    \n",
    "    def ask(self, question):\n",
    "        \"\"\"\n",
    "        Ask a question about the loaded document.\n",
    "        \n",
    "        Returns:\n",
    "            dict with answer, confidence, source, and status\n",
    "        \"\"\"\n",
    "        if not self.document:\n",
    "            return {'error': 'No document loaded'}\n",
    "        \n",
    "        # Try each section, keep best answer\n",
    "        best_result = None\n",
    "        best_score = -1\n",
    "        best_section_idx = -1\n",
    "        \n",
    "        for i, section in enumerate(self.sections):\n",
    "            if len(section) < 10:  # Skip very short sections\n",
    "                continue\n",
    "                \n",
    "            result = self.qa(question=question, context=section)\n",
    "            \n",
    "            if result['score'] > best_score:\n",
    "                best_score = result['score']\n",
    "                best_result = result\n",
    "                best_section_idx = i\n",
    "        \n",
    "        # Determine status\n",
    "        if best_score >= 0.7:\n",
    "            status = \"confident\"\n",
    "            status_message = \"I found a clear answer.\"\n",
    "        elif best_score >= self.confidence_threshold:\n",
    "            status = \"uncertain\"\n",
    "            status_message = \"I found a possible answer, but I'm not fully confident.\"\n",
    "        else:\n",
    "            status = \"not_found\"\n",
    "            status_message = \"I couldn't find a confident answer in the document.\"\n",
    "        \n",
    "        # Get context around the answer\n",
    "        section_text = self.sections[best_section_idx] if best_section_idx >= 0 else \"\"\n",
    "        source_snippet = section_text[:200] + \"...\" if len(section_text) > 200 else section_text\n",
    "        \n",
    "        return {\n",
    "            'question': question,\n",
    "            'answer': best_result['answer'] if status != 'not_found' else None,\n",
    "            'confidence': best_score,\n",
    "            'status': status,\n",
    "            'status_message': status_message,\n",
    "            'source_section': best_section_idx + 1,\n",
    "            'source_snippet': source_snippet,\n",
    "        }\n",
    "    \n",
    "    def ask_multiple(self, questions):\n",
    "        \"\"\"\n",
    "        Ask multiple questions about the document.\n",
    "        \"\"\"\n",
    "        return [self.ask(q) for q in questions]\n",
    "    \n",
    "    def format_response(self, result):\n",
    "        \"\"\"\n",
    "        Format the response for display.\n",
    "        \"\"\"\n",
    "        lines = []\n",
    "        \n",
    "        # Status indicator\n",
    "        status_icons = {'confident': '✓', 'uncertain': '?', 'not_found': '✗'}\n",
    "        icon = status_icons.get(result['status'], '•')\n",
    "        \n",
    "        lines.append(f\"Question: {result['question']}\")\n",
    "        lines.append(f\"{icon} Status: {result['status_message']}\")\n",
    "        \n",
    "        if result['answer']:\n",
    "            lines.append(f\"Answer: {result['answer']}\")\n",
    "            lines.append(f\"Confidence: {result['confidence']:.0%}\")\n",
    "            lines.append(f\"Source: Section {result['source_section']}\")\n",
    "        \n",
    "        return '\\n'.join(lines)\n",
    "\n",
    "\n",
    "# Create the Document QA system\n",
    "doc_qa = DocumentQA(use_squad2=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a sample product manual\n",
    "product_manual = \"\"\"\n",
    "SmartWidget Pro User Manual - Version 2.0\n",
    "\n",
    "GETTING STARTED\n",
    "Thank you for purchasing the SmartWidget Pro. This device features a 5-inch \n",
    "touchscreen display, 8GB of storage, and WiFi connectivity. The battery lasts \n",
    "up to 12 hours on a single charge. To turn on the device, press and hold the \n",
    "power button for 3 seconds.\n",
    "\n",
    "INITIAL SETUP\n",
    "When you first turn on your SmartWidget Pro, you will be prompted to select \n",
    "your language and connect to a WiFi network. The setup wizard will guide you \n",
    "through creating an account and personalizing your settings. Setup typically \n",
    "takes about 5 minutes.\n",
    "\n",
    "CHARGING\n",
    "Use only the included USB-C charger to charge your SmartWidget Pro. A full \n",
    "charge takes approximately 2 hours. The LED indicator turns green when fully \n",
    "charged and amber when charging. Never use third-party chargers as they may \n",
    "damage the battery.\n",
    "\n",
    "TROUBLESHOOTING\n",
    "If your device freezes, hold the power button for 10 seconds to force restart. \n",
    "If the screen is unresponsive, ensure the battery is charged. For WiFi issues, \n",
    "try toggling WiFi off and on in the Settings menu. For warranty claims, contact \n",
    "support@smartwidget.example.com within 1 year of purchase.\n",
    "\n",
    "WARRANTY INFORMATION\n",
    "Your SmartWidget Pro comes with a 1-year limited warranty covering manufacturing \n",
    "defects. Water damage and physical damage are not covered. To register your \n",
    "warranty, visit www.smartwidget.example.com/register with your serial number.\n",
    "\"\"\"\n",
    "\n",
    "num_sections = doc_qa.load_document(product_manual)\n",
    "print(f\"Document loaded with {num_sections} sections.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with customer questions\n",
    "customer_questions = [\n",
    "    \"How long does the battery last?\",\n",
    "    \"How do I turn on the device?\",\n",
    "    \"What should I do if the screen freezes?\",\n",
    "    \"How long is the warranty?\",\n",
    "    \"What charger should I use?\",\n",
    "]\n",
    "\n",
    "print(\"Customer Support Q&A\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for q in customer_questions:\n",
    "    result = doc_qa.ask(q)\n",
    "    print(doc_qa.format_response(result))\n",
    "    print(\"-\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with questions NOT in the manual\n",
    "unanswerable_questions = [\n",
    "    \"What is the price of the SmartWidget Pro?\",\n",
    "    \"Can I use this device underwater?\",\n",
    "    \"Does it support Bluetooth?\",\n",
    "]\n",
    "\n",
    "print(\"Testing Unanswerable Questions\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for q in unanswerable_questions:\n",
    "    result = doc_qa.ask(q)\n",
    "    print(doc_qa.format_response(result))\n",
    "    print(\"-\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive mode - try your own questions\n",
    "# Uncomment to use:\n",
    "\n",
    "# your_question = \"Your question here\"\n",
    "# result = doc_qa.ask(your_question)\n",
    "# print(doc_qa.format_response(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extension Ideas\n",
    "\n",
    "If you want to extend this project further:\n",
    "\n",
    "1. **Semantic search**: Use embeddings to find the most relevant section first\n",
    "2. **Follow-up questions**: Track conversation context for follow-up questions\n",
    "3. **Citation generation**: Return the exact quote from the document\n",
    "4. **Multi-document QA**: Search across multiple documents\n",
    "5. **Answer reformatting**: Use an LLM to make answers more conversational"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 6: Wrap-Up\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Extractive QA** finds answers by predicting start and end positions within a context - the answer is always a substring of the input\n",
    "\n",
    "2. **Start/End logits** are the model's predictions for where the answer begins and ends in the tokenized text\n",
    "\n",
    "3. **Confidence scores** indicate reliability - low scores often mean the question is unanswerable from the context\n",
    "\n",
    "4. **SQuAD 2.0 models** are specifically trained to handle unanswerable questions\n",
    "\n",
    "5. **Long documents** require chunking with overlap and merging results by confidence\n",
    "\n",
    "## Common Mistakes to Avoid\n",
    "\n",
    "| Mistake | Why It's a Problem |\n",
    "|---------|-------------------|\n",
    "| Ignoring confidence scores | The model will always return *something*, even for unanswerable questions |\n",
    "| Using SQuAD 1.1 models for production | They don't handle unanswerable questions well |\n",
    "| Not handling long contexts | Answers get truncated or missed |\n",
    "| Expecting reasoning | Extractive QA can't calculate, compare, or infer |\n",
    "\n",
    "## What's Next?\n",
    "\n",
    "In **Notebook 4: Text Summarization**, you'll learn:\n",
    "- How to compress long text while preserving key information\n",
    "- The difference between extractive and abstractive summarization\n",
    "- Encoder-decoder architectures for text generation\n",
    "\n",
    "This builds on QA - both tasks require understanding context, but summarization generates new text instead of extracting spans!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Solutions\n",
    "\n",
    "### Check Your Understanding (Quiz Answers)\n",
    "\n",
    "1. **B) Extractive QA** - Extracts answers directly from the context\n",
    "2. **B) Start and end positions within the context** - The model predicts where the answer begins and ends\n",
    "3. **B) Both are span extraction tasks using similar architectures** - Same BERT encoder, different output heads\n",
    "4. **C) The model may return low confidence or empty/incorrect span** - SQuAD 2.0 models are better at this\n",
    "\n",
    "### Exercise 2: Answerability Classifier (Sample Solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample solution for Exercise 2 is provided in the exercise itself\n",
    "# Key insights:\n",
    "# 1. Use multiple signals: confidence score, answer length, etc.\n",
    "# 2. SQuAD 2.0 models give more reliable confidence for unanswerable questions\n",
    "# 3. You can tune thresholds based on your use case (precision vs recall)\n",
    "\n",
    "# Additional test:\n",
    "test_context = \"\"\"\n",
    "The Eiffel Tower was completed in 1889 for the World's Fair. \n",
    "It stands 330 meters tall and is located in Paris, France.\n",
    "\"\"\"\n",
    "\n",
    "test_cases = [\n",
    "    \"When was the Eiffel Tower completed?\",  # Answerable\n",
    "    \"How much did it cost to build?\",        # Not answerable\n",
    "]\n",
    "\n",
    "for q in test_cases:\n",
    "    result = classify_answerability(qa_squad2, q, test_context)\n",
    "    print(f\"Q: {result['question']}\")\n",
    "    print(f\"   Answerable: {result['is_answerable']} ({result['confidence']:.0%})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "- [Hugging Face QA Pipeline Docs](https://huggingface.co/docs/transformers/main_classes/pipelines#transformers.QuestionAnsweringPipeline)\n",
    "- [SQuAD Dataset](https://rajpurkar.github.io/SQuAD-explorer/) - Stanford Question Answering Dataset\n",
    "- [SQuAD 2.0 Paper](https://arxiv.org/abs/1806.03822) - Know What You Don't Know\n",
    "- [BERT for QA](https://arxiv.org/abs/1810.04805) - Original BERT paper, Section 4.2\n",
    "- [RoBERTa Paper](https://arxiv.org/abs/1907.11692) - Improved pretraining for better QA"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
