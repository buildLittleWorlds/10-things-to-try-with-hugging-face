{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 10. Pipeline Internals (Capstone)\n",
        "\n",
        "**Estimated Time**: ~2 hours\n",
        "\n",
        "**Prerequisites**: Notebooks 1-9 (this capstone ties together everything you've learned)\n",
        "\n",
        "---\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "By the end of this notebook, you will be able to:\n",
        "\n",
        "1. **Understand** the three stages of a Hugging Face pipeline (tokenization, inference, post-processing)\n",
        "2. **Perform** manual tokenization and understand token IDs, attention masks, and special tokens\n",
        "3. **Run** model inference manually using PyTorch and interpret raw logits\n",
        "4. **Apply** softmax to convert logits to probabilities\n",
        "5. **Build** a custom classification pipeline from scratch with detailed logging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n",
        "\n",
        "Run this cell first. This notebook requires PyTorch and Transformers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Core imports\n",
        "from transformers import pipeline, AutoModelForSequenceClassification, AutoTokenizer\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "# Suppress warnings for cleaner output\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"Setup complete!\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"Device: {'cuda' if torch.cuda.is_available() else 'cpu'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Part 1: Conceptual Foundation\n",
        "\n",
        "## What Happens Inside a Pipeline?\n",
        "\n",
        "Throughout this course, you've used `pipeline()` to perform various NLP tasks. But what actually happens when you call it?\n",
        "\n",
        "```\n",
        "THE THREE STAGES OF A PIPELINE:\n",
        "\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚                                                                         â”‚\n",
        "â”‚   Input: \"I love machine learning!\"                                    â”‚\n",
        "â”‚                                                                         â”‚\n",
        "â”‚                         â–¼                                               â”‚\n",
        "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\n",
        "â”‚   â”‚  STAGE 1: TOKENIZATION                                          â”‚  â”‚\n",
        "â”‚   â”‚                                                                  â”‚  â”‚\n",
        "â”‚   â”‚  â€¢ Split text into tokens                                       â”‚  â”‚\n",
        "â”‚   â”‚  â€¢ Convert tokens to numerical IDs                              â”‚  â”‚\n",
        "â”‚   â”‚  â€¢ Add special tokens ([CLS], [SEP])                            â”‚  â”‚\n",
        "â”‚   â”‚  â€¢ Create attention mask                                        â”‚  â”‚\n",
        "â”‚   â”‚                                                                  â”‚  â”‚\n",
        "â”‚   â”‚  Output: {input_ids: [101, 1045, 2293, ...], attention_mask: ...}â”‚  â”‚\n",
        "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\n",
        "â”‚                         â–¼                                               â”‚\n",
        "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\n",
        "â”‚   â”‚  STAGE 2: MODEL INFERENCE                                       â”‚  â”‚\n",
        "â”‚   â”‚                                                                  â”‚  â”‚\n",
        "â”‚   â”‚  â€¢ Feed tokenized input to transformer model                    â”‚  â”‚\n",
        "â”‚   â”‚  â€¢ Model processes through all layers                           â”‚  â”‚\n",
        "â”‚   â”‚  â€¢ Output: raw logits (unnormalized scores)                     â”‚  â”‚\n",
        "â”‚   â”‚                                                                  â”‚  â”‚\n",
        "â”‚   â”‚  Output: tensor([[-2.45, 3.21]])  # Raw logits                  â”‚  â”‚\n",
        "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\n",
        "â”‚                         â–¼                                               â”‚\n",
        "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\n",
        "â”‚   â”‚  STAGE 3: POST-PROCESSING                                       â”‚  â”‚\n",
        "â”‚   â”‚                                                                  â”‚  â”‚\n",
        "â”‚   â”‚  â€¢ Apply softmax to get probabilities                           â”‚  â”‚\n",
        "â”‚   â”‚  â€¢ Map indices to labels                                        â”‚  â”‚\n",
        "â”‚   â”‚  â€¢ Format output                                                â”‚  â”‚\n",
        "â”‚   â”‚                                                                  â”‚  â”‚\n",
        "â”‚   â”‚  Output: {'label': 'POSITIVE', 'score': 0.96}                   â”‚  â”‚\n",
        "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\n",
        "â”‚                                                                         â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Why Understand Pipeline Internals?\n",
        "\n",
        "| Reason | Benefit |\n",
        "|--------|--------|\n",
        "| **Debugging** | Understand where errors come from |\n",
        "| **Optimization** | Batch processing, caching tokenization |\n",
        "| **Customization** | Modify any stage for specific needs |\n",
        "| **Understanding** | Know what the model actually \"sees\" |\n",
        "| **Research** | Access intermediate representations |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Connection to Previous Notebooks\n",
        "\n",
        "This capstone brings together concepts from every previous notebook:\n",
        "\n",
        "| Notebook | Concept | How It Connects |\n",
        "|----------|---------|----------------|\n",
        "| 1 (Fill-Mask) | [MASK] token | Special tokens in tokenization |\n",
        "| 2 (NER) | Token classification | Per-token logits |\n",
        "| 3 (QA) | Start/end positions | Position logits |\n",
        "| 4 (Summarization) | Generation params | Decoding from logits |\n",
        "| 5 (Text Gen) | Temperature, top-k | Probability manipulation |\n",
        "| 6 (Zero-Shot) | NLI hypothesis | How classification works |\n",
        "| 7 (Translation) | Encoder-decoder | Model architecture |\n",
        "| 8 (Embeddings) | Hidden states | What models produce internally |\n",
        "| 9 (Sentiment) | Multiple models | Same pipeline, different models |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Key Terminology\n",
        "\n",
        "| Term | Definition |\n",
        "|------|------------|\n",
        "| **Token** | A piece of text (word, subword, or character) |\n",
        "| **Token ID** | Numerical representation of a token |\n",
        "| **Special tokens** | Reserved tokens like [CLS], [SEP], [PAD], [MASK] |\n",
        "| **Attention mask** | Binary mask indicating real tokens (1) vs padding (0) |\n",
        "| **Logits** | Raw, unnormalized model outputs |\n",
        "| **Softmax** | Function that converts logits to probabilities |\n",
        "| **torch.no_grad()** | Context manager to disable gradient computation |\n",
        "| **Inference** | Running a model to get predictions (not training) |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Check Your Understanding\n",
        "\n",
        "Before moving on, try to answer these questions (answers at the end):\n",
        "\n",
        "1. What are the three stages of a Hugging Face pipeline?\n",
        "   - A) Loading, training, saving\n",
        "   - B) Tokenization, inference, post-processing\n",
        "   - C) Input, output, evaluation\n",
        "\n",
        "2. What are logits?\n",
        "   - A) Probabilities that sum to 1\n",
        "   - B) Raw, unnormalized model outputs\n",
        "   - C) Token IDs\n",
        "\n",
        "3. What does the attention mask do?\n",
        "   - A) Hides the model's attention patterns\n",
        "   - B) Indicates which tokens are real vs padding\n",
        "   - C) Masks random tokens for training\n",
        "\n",
        "4. Why use torch.no_grad() during inference?\n",
        "   - A) To make predictions random\n",
        "   - B) To save memory and speed up computation\n",
        "   - C) To train the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Part 2: Basic Implementation\n",
        "\n",
        "## Loading Model and Tokenizer Separately\n",
        "\n",
        "Instead of using `pipeline()`, let's load the components individually:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load model and tokenizer separately\n",
        "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "\n",
        "print(f\"Loading model: {model_name}\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Load tokenizer (handles Stage 1: Tokenization)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "print(\"Tokenizer loaded!\")\n",
        "print(f\"  Vocabulary size: {tokenizer.vocab_size:,}\")\n",
        "print(f\"  Model max length: {tokenizer.model_max_length}\")\n",
        "\n",
        "# Load model (handles Stage 2: Inference)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "print(\"\\nModel loaded!\")\n",
        "print(f\"  Number of labels: {model.config.num_labels}\")\n",
        "print(f\"  Label mapping: {model.config.id2label}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Stage 1: Tokenization Deep Dive\n",
        "\n",
        "Let's explore what happens during tokenization:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sample text\n",
        "text = \"I love machine learning!\"\n",
        "\n",
        "print(f\"Input text: \\\"{text}\\\"\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Step 1: Tokenize (convert to tokens)\n",
        "tokens = tokenizer.tokenize(text)\n",
        "print(f\"\\n1. Tokenization:\")\n",
        "print(f\"   Tokens: {tokens}\")\n",
        "print(f\"   Number of tokens: {len(tokens)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 2: Convert tokens to IDs\n",
        "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "print(f\"\\n2. Token to ID conversion:\")\n",
        "for token, id_ in zip(tokens, token_ids):\n",
        "    print(f\"   '{token}' â†’ {id_}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 3: Full tokenization with special tokens\n",
        "encoded = tokenizer(text, return_tensors=\"pt\")\n",
        "\n",
        "print(f\"\\n3. Full encoding (with special tokens):\")\n",
        "print(f\"   Keys: {list(encoded.keys())}\")\n",
        "print(f\"\\n   input_ids: {encoded['input_ids']}\")\n",
        "print(f\"   attention_mask: {encoded['attention_mask']}\")\n",
        "\n",
        "# Decode to see the full sequence\n",
        "decoded = tokenizer.decode(encoded['input_ids'][0])\n",
        "print(f\"\\n   Decoded: \\\"{decoded}\\\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize the tokenization process\n",
        "def visualize_tokenization(text, tokenizer):\n",
        "    \"\"\"Show detailed tokenization breakdown.\"\"\"\n",
        "    encoded = tokenizer(text, return_tensors=\"pt\")\n",
        "    input_ids = encoded['input_ids'][0].tolist()\n",
        "    attention_mask = encoded['attention_mask'][0].tolist()\n",
        "    \n",
        "    print(f\"Text: \\\"{text}\\\"\")\n",
        "    print(\"=\"*70)\n",
        "    print()\n",
        "    print(f\"{'Pos':<5} {'Token ID':<10} {'Token':<20} {'Attention':<10} {'Type'}\")\n",
        "    print(\"-\"*70)\n",
        "    \n",
        "    special_ids = {\n",
        "        tokenizer.cls_token_id: '[CLS] - Classification',\n",
        "        tokenizer.sep_token_id: '[SEP] - Separator',\n",
        "        tokenizer.pad_token_id: '[PAD] - Padding',\n",
        "        tokenizer.mask_token_id: '[MASK] - Mask',\n",
        "    }\n",
        "    \n",
        "    for i, (id_, mask) in enumerate(zip(input_ids, attention_mask)):\n",
        "        token = tokenizer.decode([id_])\n",
        "        token_type = special_ids.get(id_, 'Content')\n",
        "        print(f\"{i:<5} {id_:<10} {repr(token):<20} {mask:<10} {token_type}\")\n",
        "\n",
        "\n",
        "visualize_tokenization(text, tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Understanding Special Tokens\n",
        "\n",
        "```\n",
        "SPECIAL TOKENS IN BERT/DistilBERT:\n",
        "\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚  [CLS] I love machine learning ! [SEP]                          â”‚\n",
        "â”‚    â”‚                               â”‚                            â”‚\n",
        "â”‚    â”‚                               â””â”€â”€ End of sequence marker   â”‚\n",
        "â”‚    â”‚                                                            â”‚\n",
        "â”‚    â””â”€â”€ Start of sequence / Classification token                 â”‚\n",
        "â”‚        (Used for classification tasks)                          â”‚\n",
        "â”‚                                                                  â”‚\n",
        "â”‚  For classification, the model uses the [CLS] token's          â”‚\n",
        "â”‚  representation to make predictions.                            â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show special token IDs\n",
        "print(\"Special Token IDs:\")\n",
        "print(\"=\"*40)\n",
        "print(f\"[CLS] token: {tokenizer.cls_token} (ID: {tokenizer.cls_token_id})\")\n",
        "print(f\"[SEP] token: {tokenizer.sep_token} (ID: {tokenizer.sep_token_id})\")\n",
        "print(f\"[PAD] token: {tokenizer.pad_token} (ID: {tokenizer.pad_token_id})\")\n",
        "print(f\"[MASK] token: {tokenizer.mask_token} (ID: {tokenizer.mask_token_id})\")\n",
        "print(f\"[UNK] token: {tokenizer.unk_token} (ID: {tokenizer.unk_token_id})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Stage 2: Model Inference\n",
        "\n",
        "Now let's pass the tokenized input through the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Get tokenized input\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "\n",
        "print(\"Model Inference:\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Input text: \\\"{text}\\\"\")\n",
        "print(f\"\\nInputs to model:\")\n",
        "print(f\"  input_ids shape: {inputs['input_ids'].shape}\")\n",
        "print(f\"  attention_mask shape: {inputs['attention_mask'].shape}\")\n",
        "\n",
        "# Run inference (no gradient computation needed)\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "print(f\"\\nModel output type: {type(outputs)}\")\n",
        "print(f\"Output keys: {outputs.keys()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Examine the raw logits\n",
        "logits = outputs.logits\n",
        "\n",
        "print(\"Raw Logits:\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Logits tensor: {logits}\")\n",
        "print(f\"Shape: {logits.shape}  # (batch_size, num_labels)\")\n",
        "print()\n",
        "print(\"What do these numbers mean?\")\n",
        "print(\"-\"*60)\n",
        "print(\"  - Logits are RAW, UNNORMALIZED scores\")\n",
        "print(\"  - They can be any real number (positive or negative)\")\n",
        "print(\"  - Higher logit = model is more confident in that class\")\n",
        "print(f\"\\n  Logit for NEGATIVE (index 0): {logits[0][0].item():.4f}\")\n",
        "print(f\"  Logit for POSITIVE (index 1): {logits[0][1].item():.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Stage 3: Post-Processing\n",
        "\n",
        "Convert logits to probabilities and format output:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Apply softmax to convert logits to probabilities\n",
        "probabilities = F.softmax(logits, dim=-1)\n",
        "\n",
        "print(\"Softmax Conversion:\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nLogits:        {logits[0].tolist()}\")\n",
        "print(f\"Probabilities: {probabilities[0].tolist()}\")\n",
        "print(f\"Sum of probs:  {probabilities[0].sum().item():.4f}  (should be 1.0)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Map to labels\n",
        "predicted_class = torch.argmax(probabilities, dim=-1).item()\n",
        "predicted_label = model.config.id2label[predicted_class]\n",
        "confidence = probabilities[0][predicted_class].item()\n",
        "\n",
        "print(\"Final Prediction:\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nPredicted class index: {predicted_class}\")\n",
        "print(f\"Predicted label: {predicted_label}\")\n",
        "print(f\"Confidence: {confidence:.2%}\")\n",
        "print()\n",
        "print(\"All class probabilities:\")\n",
        "for i, prob in enumerate(probabilities[0]):\n",
        "    label = model.config.id2label[i]\n",
        "    bar = '*' * int(prob.item() * 40)\n",
        "    print(f\"  {label:10s}: {prob.item():.4f} {bar}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Comparing Manual vs Pipeline Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create pipeline for comparison\n",
        "classifier = pipeline(\"sentiment-analysis\", model=model_name)\n",
        "\n",
        "# Get pipeline result\n",
        "pipeline_result = classifier(text)[0]\n",
        "\n",
        "print(\"Comparison: Manual vs Pipeline\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nText: \\\"{text}\\\"\")\n",
        "print()\n",
        "print(f\"Manual approach:\")\n",
        "print(f\"  Label: {predicted_label}\")\n",
        "print(f\"  Score: {confidence:.4f}\")\n",
        "print()\n",
        "print(f\"Pipeline approach:\")\n",
        "print(f\"  Label: {pipeline_result['label']}\")\n",
        "print(f\"  Score: {pipeline_result['score']:.4f}\")\n",
        "print()\n",
        "print(\"Results match!\" if predicted_label == pipeline_result['label'] else \"Results differ!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Exercise 1: Manual Classification (Guided)\n",
        "\n",
        "**Difficulty**: Basic | **Time**: 10-15 minutes\n",
        "\n",
        "**Your task**: Implement a complete manual classification function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def manual_classify(text, tokenizer, model, verbose=True):\n",
        "    \"\"\"\n",
        "    Perform classification manually, showing each step.\n",
        "    \n",
        "    Args:\n",
        "        text: Input text to classify\n",
        "        tokenizer: The tokenizer\n",
        "        model: The classification model\n",
        "        verbose: Whether to print detailed output\n",
        "        \n",
        "    Returns:\n",
        "        dict with label and score\n",
        "    \"\"\"\n",
        "    if verbose:\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"MANUAL CLASSIFICATION\")\n",
        "        print(f\"{'='*60}\")\n",
        "        print(f\"Input: \\\"{text}\\\"\")\n",
        "    \n",
        "    # STAGE 1: Tokenization\n",
        "    if verbose:\n",
        "        print(f\"\\n--- Stage 1: Tokenization ---\")\n",
        "    \n",
        "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "    \n",
        "    if verbose:\n",
        "        tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
        "        print(f\"Tokens: {tokens}\")\n",
        "        print(f\"Input IDs: {inputs['input_ids'][0].tolist()}\")\n",
        "    \n",
        "    # STAGE 2: Model Inference\n",
        "    if verbose:\n",
        "        print(f\"\\n--- Stage 2: Model Inference ---\")\n",
        "    \n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    \n",
        "    logits = outputs.logits\n",
        "    \n",
        "    if verbose:\n",
        "        print(f\"Raw logits: {logits[0].tolist()}\")\n",
        "    \n",
        "    # STAGE 3: Post-processing\n",
        "    if verbose:\n",
        "        print(f\"\\n--- Stage 3: Post-processing ---\")\n",
        "    \n",
        "    probabilities = F.softmax(logits, dim=-1)\n",
        "    predicted_class = torch.argmax(probabilities, dim=-1).item()\n",
        "    predicted_label = model.config.id2label[predicted_class]\n",
        "    confidence = probabilities[0][predicted_class].item()\n",
        "    \n",
        "    if verbose:\n",
        "        print(f\"Probabilities: {probabilities[0].tolist()}\")\n",
        "        print(f\"Predicted: {predicted_label} ({confidence:.2%})\")\n",
        "    \n",
        "    return {'label': predicted_label, 'score': confidence}\n",
        "\n",
        "\n",
        "# Test the function\n",
        "test_texts = [\n",
        "    \"This movie was absolutely fantastic!\",\n",
        "    \"Terrible waste of time.\",\n",
        "    \"It was okay, nothing special.\",\n",
        "]\n",
        "\n",
        "for text in test_texts:\n",
        "    result = manual_classify(text, tokenizer, model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Part 3: Intermediate Exploration\n",
        "\n",
        "## Understanding Attention Masks\n",
        "\n",
        "When processing batches with different lengths, padding is needed:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Batch of texts with different lengths\n",
        "batch_texts = [\n",
        "    \"Short.\",\n",
        "    \"This is a medium length sentence.\",\n",
        "    \"This is a much longer sentence that has many more words and tokens.\",\n",
        "]\n",
        "\n",
        "# Tokenize with padding\n",
        "batch_inputs = tokenizer(\n",
        "    batch_texts, \n",
        "    padding=True,  # Pad to longest in batch\n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "\n",
        "print(\"Batch Tokenization with Padding:\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nInput shapes:\")\n",
        "print(f\"  input_ids: {batch_inputs['input_ids'].shape}\")\n",
        "print(f\"  attention_mask: {batch_inputs['attention_mask'].shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize padding and attention masks\n",
        "def visualize_batch_padding(texts, tokenizer):\n",
        "    \"\"\"Visualize how padding and attention masks work.\"\"\"\n",
        "    inputs = tokenizer(texts, padding=True, return_tensors=\"pt\")\n",
        "    \n",
        "    print(\"Batch Padding Visualization:\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    for i, text in enumerate(texts):\n",
        "        ids = inputs['input_ids'][i].tolist()\n",
        "        mask = inputs['attention_mask'][i].tolist()\n",
        "        \n",
        "        print(f\"\\nText {i+1}: \\\"{text}\\\"\")\n",
        "        print(\"-\"*80)\n",
        "        \n",
        "        # Show tokens with attention mask\n",
        "        visual = []\n",
        "        for id_, m in zip(ids, mask):\n",
        "            token = tokenizer.decode([id_])\n",
        "            if m == 1:\n",
        "                visual.append(f\"[{token}]\")\n",
        "            else:\n",
        "                visual.append(f\"({token})\")\n",
        "        \n",
        "        print(f\"  Tokens:    {' '.join(visual)}\")\n",
        "        print(f\"  Mask:      {mask}\")\n",
        "        print(f\"  Real tokens: {sum(mask)}, Padding: {len(mask) - sum(mask)}\")\n",
        "    \n",
        "    print(\"\\nLegend: [token] = real, (token) = padding\")\n",
        "\n",
        "\n",
        "visualize_batch_padding(batch_texts, tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Why Attention Masks Matter\n",
        "\n",
        "```\n",
        "WITHOUT ATTENTION MASK:\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚  Model would \"see\" padding tokens as real content!            â”‚\n",
        "â”‚                                                                â”‚\n",
        "â”‚  [CLS] Short . [SEP] [PAD] [PAD] [PAD] [PAD]                  â”‚\n",
        "â”‚     â†“                   â†“     â†“     â†“     â†“                   â”‚\n",
        "â”‚  Model attends to everything, including meaningless padding   â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "\n",
        "WITH ATTENTION MASK:\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚  Model only \"sees\" real tokens                                â”‚\n",
        "â”‚                                                                â”‚\n",
        "â”‚  [CLS] Short . [SEP] [PAD] [PAD] [PAD] [PAD]                  â”‚\n",
        "â”‚     1    1   1   1     0     0     0     0   â† Attention mask â”‚\n",
        "â”‚     â†“    â†“   â†“   â†“     âœ—     âœ—     âœ—     âœ—                    â”‚\n",
        "â”‚  Model ignores padding tokens completely                      â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Process the batch\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    outputs = model(**batch_inputs)\n",
        "\n",
        "# Get predictions for all items in batch\n",
        "probabilities = F.softmax(outputs.logits, dim=-1)\n",
        "\n",
        "print(\"\\nBatch Predictions:\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "for i, text in enumerate(batch_texts):\n",
        "    predicted_class = torch.argmax(probabilities[i]).item()\n",
        "    label = model.config.id2label[predicted_class]\n",
        "    score = probabilities[i][predicted_class].item()\n",
        "    \n",
        "    print(f\"\\n\\\"{text}\\\"\")\n",
        "    print(f\"  â†’ {label} ({score:.2%})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exploring Tokenizer Behavior\n",
        "\n",
        "Different inputs are tokenized differently:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test various tokenization scenarios\n",
        "test_cases = [\n",
        "    \"Hello world\",  # Simple\n",
        "    \"Hello World\",  # Capitalization\n",
        "    \"unbelievable\",  # Subword tokenization\n",
        "    \"antidisestablishmentarianism\",  # Long word\n",
        "    \"COVID-19\",  # Special characters\n",
        "    \"I'm don't won't\",  # Contractions\n",
        "    \"ğŸ”¥ This is fire! ğŸ”¥\",  # Emojis\n",
        "    \"supercalifragilisticexpialidocious\",  # Very long word\n",
        "]\n",
        "\n",
        "print(\"Tokenization Scenarios:\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "for text in test_cases:\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    print(f\"\\n\\\"{text}\\\"\")\n",
        "    print(f\"  â†’ {tokens}\")\n",
        "    print(f\"  â†’ {len(tokens)} tokens\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Understanding Subword Tokenization\n",
        "\n",
        "```\n",
        "WHY SUBWORD TOKENIZATION?\n",
        "\n",
        "Problem: Vocabulary can't contain every possible word\n",
        "Solution: Break unknown words into known subwords\n",
        "\n",
        "Example: \"unbelievable\" â†’ [\"un\", \"##believ\", \"##able\"]\n",
        "\n",
        "The ## prefix means \"continuation of previous token\"\n",
        "\n",
        "Benefits:\n",
        "â€¢ Handle any word, even misspellings\n",
        "â€¢ Share knowledge between related words\n",
        "â€¢ Manageable vocabulary size\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show subword tokenization in detail\n",
        "def analyze_subword_tokenization(word, tokenizer):\n",
        "    \"\"\"Analyze how a word is broken into subwords.\"\"\"\n",
        "    tokens = tokenizer.tokenize(word)\n",
        "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "    \n",
        "    print(f\"\\nWord: \\\"{word}\\\"\")\n",
        "    print(\"-\"*40)\n",
        "    print(f\"{'Token':<20} {'ID':<10} {'Type'}\")\n",
        "    print(\"-\"*40)\n",
        "    \n",
        "    for token, id_ in zip(tokens, token_ids):\n",
        "        if token.startswith('##'):\n",
        "            token_type = 'Continuation'\n",
        "        else:\n",
        "            token_type = 'Start of word'\n",
        "        print(f\"{token:<20} {id_:<10} {token_type}\")\n",
        "\n",
        "\n",
        "# Analyze some words\n",
        "words_to_analyze = [\n",
        "    \"unbelievable\",\n",
        "    \"preprocessing\",\n",
        "    \"transformers\",\n",
        "    \"tokenization\",\n",
        "]\n",
        "\n",
        "for word in words_to_analyze:\n",
        "    analyze_subword_tokenization(word, tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Exercise 2: Batch Processing (Semi-guided)\n",
        "\n",
        "**Difficulty**: Intermediate | **Time**: 15-20 minutes\n",
        "\n",
        "**Your task**: Implement efficient batch processing with proper padding and attention masks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def batch_classify(texts, tokenizer, model, batch_size=4):\n",
        "    \"\"\"\n",
        "    Classify multiple texts efficiently using batching.\n",
        "    \n",
        "    Args:\n",
        "        texts: List of texts to classify\n",
        "        tokenizer: The tokenizer\n",
        "        model: The classification model\n",
        "        batch_size: Number of texts per batch\n",
        "        \n",
        "    Returns:\n",
        "        List of dicts with label and score for each text\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    model.eval()\n",
        "    \n",
        "    # Process in batches\n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch_texts = texts[i:i + batch_size]\n",
        "        \n",
        "        # Tokenize batch with padding\n",
        "        inputs = tokenizer(\n",
        "            batch_texts,\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=512,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        \n",
        "        # Run inference\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "        \n",
        "        # Post-process\n",
        "        probabilities = F.softmax(outputs.logits, dim=-1)\n",
        "        \n",
        "        # Get predictions for each item in batch\n",
        "        for j in range(len(batch_texts)):\n",
        "            pred_class = torch.argmax(probabilities[j]).item()\n",
        "            results.append({\n",
        "                'text': batch_texts[j],\n",
        "                'label': model.config.id2label[pred_class],\n",
        "                'score': probabilities[j][pred_class].item(),\n",
        "                'all_scores': {model.config.id2label[k]: probabilities[j][k].item() \n",
        "                              for k in range(len(probabilities[j]))}\n",
        "            })\n",
        "    \n",
        "    return results\n",
        "\n",
        "\n",
        "# Test with multiple texts\n",
        "test_texts = [\n",
        "    \"I absolutely loved this movie!\",\n",
        "    \"Terrible experience, never again.\",\n",
        "    \"It was okay, nothing special.\",\n",
        "    \"The best purchase I've ever made!\",\n",
        "    \"Complete waste of money.\",\n",
        "    \"Pretty good, would recommend.\",\n",
        "    \"Not what I expected.\",\n",
        "    \"Exceeded all my expectations!\",\n",
        "]\n",
        "\n",
        "results = batch_classify(test_texts, tokenizer, model, batch_size=3)\n",
        "\n",
        "print(\"Batch Classification Results:\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "for r in results:\n",
        "    print(f\"\\n\\\"{r['text']}\\\"\")\n",
        "    print(f\"  â†’ {r['label']} ({r['score']:.2%})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Part 4: Advanced Topics\n",
        "\n",
        "## Accessing Hidden States\n",
        "\n",
        "Models produce more than just logits - we can access the internal representations:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load model with output_hidden_states enabled\n",
        "model_with_hidden = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_name,\n",
        "    output_hidden_states=True\n",
        ")\n",
        "\n",
        "text = \"I love machine learning!\"\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "\n",
        "model_with_hidden.eval()\n",
        "with torch.no_grad():\n",
        "    outputs = model_with_hidden(**inputs)\n",
        "\n",
        "print(\"Model Outputs with Hidden States:\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nOutput keys: {outputs.keys()}\")\n",
        "print(f\"\\nLogits shape: {outputs.logits.shape}\")\n",
        "print(f\"Number of hidden state layers: {len(outputs.hidden_states)}\")\n",
        "print(f\"Each hidden state shape: {outputs.hidden_states[0].shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze hidden states\n",
        "print(\"Hidden State Analysis:\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Get the final layer's hidden states\n",
        "final_hidden = outputs.hidden_states[-1]\n",
        "print(f\"\\nFinal hidden state shape: {final_hidden.shape}\")\n",
        "print(f\"  â†’ (batch_size, sequence_length, hidden_size)\")\n",
        "\n",
        "# The [CLS] token representation (used for classification)\n",
        "cls_representation = final_hidden[0, 0, :]\n",
        "print(f\"\\n[CLS] token representation shape: {cls_representation.shape}\")\n",
        "print(f\"First 10 values: {cls_representation[:10].tolist()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Understanding Layer-wise Representations\n",
        "\n",
        "```\n",
        "LAYER PROGRESSION:\n",
        "\n",
        "Layer 0 (Embedding): Raw token embeddings\n",
        "       â†“\n",
        "Layer 1: Basic patterns (syntax)\n",
        "       â†“\n",
        "Layer 2-3: More complex patterns\n",
        "       â†“\n",
        "Layer 4-5: Semantic understanding\n",
        "       â†“\n",
        "Final Layer: Task-specific representations\n",
        "       â†“\n",
        "Classification Head: Logits\n",
        "\n",
        "Each layer builds on the previous one,\n",
        "creating increasingly abstract representations.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Custom Post-Processing\n",
        "\n",
        "Customize how outputs are processed:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def custom_postprocess(logits, config, threshold=0.7, temperature=1.0):\n",
        "    \"\"\"\n",
        "    Custom post-processing with configurable options.\n",
        "    \n",
        "    Args:\n",
        "        logits: Raw model outputs\n",
        "        config: Model config with label mapping\n",
        "        threshold: Confidence threshold for predictions\n",
        "        temperature: Temperature for softmax (higher = more uniform)\n",
        "        \n",
        "    Returns:\n",
        "        dict with prediction details\n",
        "    \"\"\"\n",
        "    # Apply temperature scaling\n",
        "    scaled_logits = logits / temperature\n",
        "    \n",
        "    # Convert to probabilities\n",
        "    probabilities = F.softmax(scaled_logits, dim=-1)\n",
        "    \n",
        "    # Get prediction\n",
        "    max_prob, pred_class = torch.max(probabilities, dim=-1)\n",
        "    max_prob = max_prob.item()\n",
        "    pred_class = pred_class.item()\n",
        "    \n",
        "    # Apply threshold\n",
        "    if max_prob < threshold:\n",
        "        label = \"UNCERTAIN\"\n",
        "        confident = False\n",
        "    else:\n",
        "        label = config.id2label[pred_class]\n",
        "        confident = True\n",
        "    \n",
        "    return {\n",
        "        'label': label,\n",
        "        'score': max_prob,\n",
        "        'confident': confident,\n",
        "        'temperature': temperature,\n",
        "        'all_probs': {config.id2label[i]: probabilities[0][i].item() \n",
        "                      for i in range(len(config.id2label))}\n",
        "    }\n",
        "\n",
        "\n",
        "# Test with different temperatures\n",
        "text = \"It was okay, I guess.\"\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "print(f\"Text: \\\"{text}\\\"\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for temp in [0.5, 1.0, 2.0]:\n",
        "    result = custom_postprocess(outputs.logits, model.config, temperature=temp)\n",
        "    print(f\"\\nTemperature: {temp}\")\n",
        "    print(f\"  Label: {result['label']}\")\n",
        "    print(f\"  Score: {result['score']:.4f}\")\n",
        "    print(f\"  Confident: {result['confident']}\")\n",
        "    print(f\"  All probs: {result['all_probs']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Temperature Effect on Probabilities\n",
        "\n",
        "```\n",
        "TEMPERATURE SCALING:\n",
        "\n",
        "Low temperature (< 1.0):\n",
        "  - Makes distribution sharper\n",
        "  - Increases confidence in top prediction\n",
        "  - More \"decisive\"\n",
        "\n",
        "Temperature = 1.0:\n",
        "  - Standard softmax\n",
        "  - No scaling\n",
        "\n",
        "High temperature (> 1.0):\n",
        "  - Makes distribution flatter\n",
        "  - Decreases confidence\n",
        "  - More \"uncertain\"\n",
        "\n",
        "Example:\n",
        "Logits: [2.0, 1.0]\n",
        "\n",
        "T=0.5: [0.88, 0.12]  â† Very confident\n",
        "T=1.0: [0.73, 0.27]  â† Normal\n",
        "T=2.0: [0.62, 0.38]  â† Less confident\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Exercise 3: Build a Logging Pipeline (Independent)\n",
        "\n",
        "**Difficulty**: Advanced | **Time**: 15-20 minutes\n",
        "\n",
        "**Your task**: Build a pipeline wrapper that logs every step of the process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "\n",
        "import time\n",
        "\n",
        "class LoggingPipeline:\n",
        "    \"\"\"\n",
        "    A pipeline wrapper that logs every step.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, tokenizer, model, log_level='detailed'):\n",
        "        \"\"\"\n",
        "        Initialize the logging pipeline.\n",
        "        \n",
        "        Args:\n",
        "            tokenizer: The tokenizer\n",
        "            model: The classification model\n",
        "            log_level: 'minimal', 'standard', or 'detailed'\n",
        "        \"\"\"\n",
        "        self.tokenizer = tokenizer\n",
        "        self.model = model\n",
        "        self.log_level = log_level\n",
        "        self.logs = []\n",
        "    \n",
        "    def _log(self, stage, message, data=None):\n",
        "        \"\"\"Add a log entry.\"\"\"\n",
        "        entry = {\n",
        "            'timestamp': time.time(),\n",
        "            'stage': stage,\n",
        "            'message': message,\n",
        "            'data': data\n",
        "        }\n",
        "        self.logs.append(entry)\n",
        "    \n",
        "    def classify(self, text):\n",
        "        \"\"\"\n",
        "        Classify text with logging.\n",
        "        \n",
        "        Returns:\n",
        "            dict with prediction and logs\n",
        "        \"\"\"\n",
        "        self.logs = []  # Reset logs\n",
        "        start_time = time.time()\n",
        "        \n",
        "        # STAGE 1: Tokenization\n",
        "        self._log('tokenization', 'Starting tokenization', {'text': text})\n",
        "        \n",
        "        tok_start = time.time()\n",
        "        inputs = self.tokenizer(text, return_tensors=\"pt\")\n",
        "        tok_time = time.time() - tok_start\n",
        "        \n",
        "        tokens = self.tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
        "        self._log('tokenization', f'Tokenized into {len(tokens)} tokens', {\n",
        "            'tokens': tokens,\n",
        "            'input_ids': inputs['input_ids'][0].tolist(),\n",
        "            'time_ms': tok_time * 1000\n",
        "        })\n",
        "        \n",
        "        # STAGE 2: Inference\n",
        "        self._log('inference', 'Starting model inference')\n",
        "        \n",
        "        inf_start = time.time()\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(**inputs)\n",
        "        inf_time = time.time() - inf_start\n",
        "        \n",
        "        logits = outputs.logits\n",
        "        self._log('inference', 'Model inference complete', {\n",
        "            'logits': logits[0].tolist(),\n",
        "            'time_ms': inf_time * 1000\n",
        "        })\n",
        "        \n",
        "        # STAGE 3: Post-processing\n",
        "        self._log('postprocess', 'Starting post-processing')\n",
        "        \n",
        "        post_start = time.time()\n",
        "        probabilities = F.softmax(logits, dim=-1)\n",
        "        pred_class = torch.argmax(probabilities, dim=-1).item()\n",
        "        label = self.model.config.id2label[pred_class]\n",
        "        score = probabilities[0][pred_class].item()\n",
        "        post_time = time.time() - post_start\n",
        "        \n",
        "        self._log('postprocess', 'Post-processing complete', {\n",
        "            'probabilities': probabilities[0].tolist(),\n",
        "            'prediction': {'label': label, 'score': score},\n",
        "            'time_ms': post_time * 1000\n",
        "        })\n",
        "        \n",
        "        total_time = time.time() - start_time\n",
        "        self._log('complete', 'Classification complete', {\n",
        "            'total_time_ms': total_time * 1000\n",
        "        })\n",
        "        \n",
        "        return {\n",
        "            'label': label,\n",
        "            'score': score,\n",
        "            'logs': self.logs\n",
        "        }\n",
        "    \n",
        "    def print_logs(self, result=None):\n",
        "        \"\"\"Print formatted logs.\"\"\"\n",
        "        logs = result['logs'] if result else self.logs\n",
        "        \n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\" PIPELINE EXECUTION LOG\")\n",
        "        print(\"=\"*70)\n",
        "        \n",
        "        for log in logs:\n",
        "            stage = log['stage'].upper()\n",
        "            msg = log['message']\n",
        "            \n",
        "            print(f\"\\n[{stage}] {msg}\")\n",
        "            \n",
        "            if self.log_level == 'detailed' and log['data']:\n",
        "                for key, value in log['data'].items():\n",
        "                    if key == 'tokens' and len(value) > 10:\n",
        "                        print(f\"  {key}: {value[:5]} ... {value[-5:]}\")\n",
        "                    elif isinstance(value, list) and len(value) > 10:\n",
        "                        print(f\"  {key}: [{value[0]}, ..., {value[-1]}] (len={len(value)})\")\n",
        "                    elif key.endswith('_ms'):\n",
        "                        print(f\"  {key}: {value:.2f}ms\")\n",
        "                    else:\n",
        "                        print(f\"  {key}: {value}\")\n",
        "        \n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "\n",
        "\n",
        "# Test the logging pipeline\n",
        "logging_pipe = LoggingPipeline(tokenizer, model, log_level='detailed')\n",
        "\n",
        "result = logging_pipe.classify(\"This product is absolutely amazing!\")\n",
        "logging_pipe.print_logs(result)\n",
        "\n",
        "print(f\"\\nFinal result: {result['label']} ({result['score']:.2%})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Part 5: Mini-Project\n",
        "\n",
        "## Project: Custom Pipeline Builder\n",
        "\n",
        "**Scenario**: You're building a production classification system that needs:\n",
        "- Detailed logging for debugging\n",
        "- Configurable confidence thresholds\n",
        "- Batch processing support\n",
        "- Performance metrics\n",
        "\n",
        "**Your goal**: Build a complete `CustomClassificationPipeline` class that implements all three stages from scratch with full configurability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# MINI-PROJECT: Custom Pipeline Builder\n",
        "# =====================================\n",
        "\n",
        "import time\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Dict, Optional, Any\n",
        "\n",
        "@dataclass\n",
        "class PipelineConfig:\n",
        "    \"\"\"Configuration for the custom pipeline.\"\"\"\n",
        "    confidence_threshold: float = 0.5\n",
        "    temperature: float = 1.0\n",
        "    max_length: int = 512\n",
        "    batch_size: int = 8\n",
        "    return_all_scores: bool = False\n",
        "    log_level: str = 'standard'  # 'minimal', 'standard', 'detailed'\n",
        "\n",
        "\n",
        "class CustomClassificationPipeline:\n",
        "    \"\"\"\n",
        "    A fully customizable text classification pipeline.\n",
        "    \n",
        "    Features:\n",
        "    - Configurable confidence thresholds\n",
        "    - Temperature scaling\n",
        "    - Batch processing\n",
        "    - Detailed logging and metrics\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, model_name: str, config: Optional[PipelineConfig] = None):\n",
        "        \"\"\"\n",
        "        Initialize the pipeline.\n",
        "        \n",
        "        Args:\n",
        "            model_name: HuggingFace model name or path\n",
        "            config: Pipeline configuration\n",
        "        \"\"\"\n",
        "        self.config = config or PipelineConfig()\n",
        "        self.model_name = model_name\n",
        "        \n",
        "        # Load model and tokenizer\n",
        "        self._log(\"Loading tokenizer...\")\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        \n",
        "        self._log(\"Loading model...\")\n",
        "        self.model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "        self.model.eval()\n",
        "        \n",
        "        # Store label mapping\n",
        "        self.id2label = self.model.config.id2label\n",
        "        self.label2id = self.model.config.label2id\n",
        "        \n",
        "        # Metrics tracking\n",
        "        self.metrics = {\n",
        "            'total_predictions': 0,\n",
        "            'total_time_ms': 0,\n",
        "            'avg_time_ms': 0,\n",
        "            'confident_predictions': 0,\n",
        "            'uncertain_predictions': 0,\n",
        "        }\n",
        "        \n",
        "        self._log(f\"Pipeline initialized with {len(self.id2label)} labels: {list(self.id2label.values())}\")\n",
        "    \n",
        "    def _log(self, message: str, level: str = 'standard'):\n",
        "        \"\"\"Log a message based on log level.\"\"\"\n",
        "        levels = {'minimal': 0, 'standard': 1, 'detailed': 2}\n",
        "        if levels.get(level, 1) <= levels.get(self.config.log_level, 1):\n",
        "            print(f\"[Pipeline] {message}\")\n",
        "    \n",
        "    def tokenize(self, texts: List[str]) -> Dict[str, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Stage 1: Tokenization.\n",
        "        \n",
        "        Args:\n",
        "            texts: List of input texts\n",
        "            \n",
        "        Returns:\n",
        "            Dict with input_ids and attention_mask tensors\n",
        "        \"\"\"\n",
        "        self._log(f\"Tokenizing {len(texts)} text(s)...\", 'detailed')\n",
        "        \n",
        "        inputs = self.tokenizer(\n",
        "            texts,\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=self.config.max_length,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        \n",
        "        self._log(f\"Tokenized shape: {inputs['input_ids'].shape}\", 'detailed')\n",
        "        return inputs\n",
        "    \n",
        "    def infer(self, inputs: Dict[str, torch.Tensor]) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Stage 2: Model inference.\n",
        "        \n",
        "        Args:\n",
        "            inputs: Tokenized inputs\n",
        "            \n",
        "        Returns:\n",
        "            Logits tensor\n",
        "        \"\"\"\n",
        "        self._log(\"Running model inference...\", 'detailed')\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(**inputs)\n",
        "        \n",
        "        self._log(f\"Logits shape: {outputs.logits.shape}\", 'detailed')\n",
        "        return outputs.logits\n",
        "    \n",
        "    def postprocess(self, logits: torch.Tensor) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Stage 3: Post-processing.\n",
        "        \n",
        "        Args:\n",
        "            logits: Raw model outputs\n",
        "            \n",
        "        Returns:\n",
        "            List of prediction dictionaries\n",
        "        \"\"\"\n",
        "        self._log(\"Post-processing outputs...\", 'detailed')\n",
        "        \n",
        "        # Apply temperature scaling\n",
        "        scaled_logits = logits / self.config.temperature\n",
        "        \n",
        "        # Convert to probabilities\n",
        "        probabilities = F.softmax(scaled_logits, dim=-1)\n",
        "        \n",
        "        results = []\n",
        "        for i in range(len(probabilities)):\n",
        "            probs = probabilities[i]\n",
        "            max_prob, pred_class = torch.max(probs, dim=-1)\n",
        "            max_prob = max_prob.item()\n",
        "            pred_class = pred_class.item()\n",
        "            \n",
        "            # Apply threshold\n",
        "            confident = max_prob >= self.config.confidence_threshold\n",
        "            label = self.id2label[pred_class] if confident else \"UNCERTAIN\"\n",
        "            \n",
        "            result = {\n",
        "                'label': label,\n",
        "                'score': max_prob,\n",
        "                'confident': confident,\n",
        "                'predicted_class': pred_class,\n",
        "            }\n",
        "            \n",
        "            if self.config.return_all_scores:\n",
        "                result['all_scores'] = {\n",
        "                    self.id2label[j]: probs[j].item()\n",
        "                    for j in range(len(probs))\n",
        "                }\n",
        "            \n",
        "            results.append(result)\n",
        "            \n",
        "            # Update metrics\n",
        "            if confident:\n",
        "                self.metrics['confident_predictions'] += 1\n",
        "            else:\n",
        "                self.metrics['uncertain_predictions'] += 1\n",
        "        \n",
        "        return results\n",
        "    \n",
        "    def __call__(self, texts, **kwargs) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Run the full pipeline.\n",
        "        \n",
        "        Args:\n",
        "            texts: Single text or list of texts\n",
        "            **kwargs: Override config options\n",
        "            \n",
        "        Returns:\n",
        "            List of prediction dictionaries\n",
        "        \"\"\"\n",
        "        # Handle single text input\n",
        "        single_input = isinstance(texts, str)\n",
        "        if single_input:\n",
        "            texts = [texts]\n",
        "        \n",
        "        # Override config if needed\n",
        "        orig_config = {}\n",
        "        for key, value in kwargs.items():\n",
        "            if hasattr(self.config, key):\n",
        "                orig_config[key] = getattr(self.config, key)\n",
        "                setattr(self.config, key, value)\n",
        "        \n",
        "        start_time = time.time()\n",
        "        \n",
        "        all_results = []\n",
        "        \n",
        "        # Process in batches\n",
        "        for i in range(0, len(texts), self.config.batch_size):\n",
        "            batch = texts[i:i + self.config.batch_size]\n",
        "            self._log(f\"Processing batch {i//self.config.batch_size + 1} ({len(batch)} items)\", 'detailed')\n",
        "            \n",
        "            # Run pipeline stages\n",
        "            inputs = self.tokenize(batch)\n",
        "            logits = self.infer(inputs)\n",
        "            results = self.postprocess(logits)\n",
        "            \n",
        "            # Add original text to results\n",
        "            for j, result in enumerate(results):\n",
        "                result['text'] = batch[j]\n",
        "            \n",
        "            all_results.extend(results)\n",
        "        \n",
        "        # Update metrics\n",
        "        elapsed = (time.time() - start_time) * 1000\n",
        "        self.metrics['total_predictions'] += len(texts)\n",
        "        self.metrics['total_time_ms'] += elapsed\n",
        "        self.metrics['avg_time_ms'] = (\n",
        "            self.metrics['total_time_ms'] / self.metrics['total_predictions']\n",
        "        )\n",
        "        \n",
        "        self._log(f\"Processed {len(texts)} text(s) in {elapsed:.2f}ms\")\n",
        "        \n",
        "        # Restore config\n",
        "        for key, value in orig_config.items():\n",
        "            setattr(self.config, key, value)\n",
        "        \n",
        "        return all_results[0] if single_input else all_results\n",
        "    \n",
        "    def get_metrics(self) -> Dict[str, Any]:\n",
        "        \"\"\"Get pipeline metrics.\"\"\"\n",
        "        return self.metrics.copy()\n",
        "    \n",
        "    def reset_metrics(self):\n",
        "        \"\"\"Reset metrics.\"\"\"\n",
        "        self.metrics = {\n",
        "            'total_predictions': 0,\n",
        "            'total_time_ms': 0,\n",
        "            'avg_time_ms': 0,\n",
        "            'confident_predictions': 0,\n",
        "            'uncertain_predictions': 0,\n",
        "        }\n",
        "\n",
        "\n",
        "# Create custom pipeline\n",
        "config = PipelineConfig(\n",
        "    confidence_threshold=0.7,\n",
        "    temperature=1.0,\n",
        "    return_all_scores=True,\n",
        "    log_level='standard'\n",
        ")\n",
        "\n",
        "custom_pipe = CustomClassificationPipeline(\n",
        "    \"distilbert-base-uncased-finetuned-sst-2-english\",\n",
        "    config=config\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test single prediction\n",
        "result = custom_pipe(\"I absolutely love this product!\")\n",
        "\n",
        "print(\"\\nSingle Prediction Result:\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Text: \\\"{result['text']}\\\"\")\n",
        "print(f\"Label: {result['label']}\")\n",
        "print(f\"Score: {result['score']:.4f}\")\n",
        "print(f\"Confident: {result['confident']}\")\n",
        "if 'all_scores' in result:\n",
        "    print(f\"All scores: {result['all_scores']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test batch prediction\n",
        "test_texts = [\n",
        "    \"This is the best purchase I've ever made!\",\n",
        "    \"Terrible quality, completely disappointed.\",\n",
        "    \"It's okay, nothing special.\",\n",
        "    \"Exceeded all my expectations!\",\n",
        "    \"Would not recommend to anyone.\",\n",
        "    \"Pretty good for the price.\",\n",
        "    \"Meh.\",\n",
        "    \"Absolutely fantastic experience!\",\n",
        "]\n",
        "\n",
        "results = custom_pipe(test_texts)\n",
        "\n",
        "print(\"\\nBatch Prediction Results:\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "for r in results:\n",
        "    conf_marker = \"âœ“\" if r['confident'] else \"?\"\n",
        "    print(f\"\\n[{conf_marker}] \\\"{r['text']}\\\"\")\n",
        "    print(f\"    â†’ {r['label']} ({r['score']:.2%})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# View metrics\n",
        "metrics = custom_pipe.get_metrics()\n",
        "\n",
        "print(\"\\nPipeline Metrics:\")\n",
        "print(\"=\"*50)\n",
        "for key, value in metrics.items():\n",
        "    if 'time' in key:\n",
        "        print(f\"{key}: {value:.2f}ms\")\n",
        "    else:\n",
        "        print(f\"{key}: {value}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test with different configurations\n",
        "print(\"\\nTesting Different Configurations:\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "test_text = \"It was okay, I guess.\"\n",
        "\n",
        "# High threshold\n",
        "result_high = custom_pipe(test_text, confidence_threshold=0.9)\n",
        "print(f\"\\nHigh threshold (0.9):\")\n",
        "print(f\"  {result_high['label']} ({result_high['score']:.4f})\")\n",
        "\n",
        "# Low threshold\n",
        "result_low = custom_pipe(test_text, confidence_threshold=0.3)\n",
        "print(f\"\\nLow threshold (0.3):\")\n",
        "print(f\"  {result_low['label']} ({result_low['score']:.4f})\")\n",
        "\n",
        "# Different temperatures\n",
        "for temp in [0.5, 1.0, 2.0]:\n",
        "    result_temp = custom_pipe(test_text, temperature=temp, confidence_threshold=0.5)\n",
        "    print(f\"\\nTemperature {temp}:\")\n",
        "    print(f\"  {result_temp['label']} ({result_temp['score']:.4f})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Extension Ideas\n",
        "\n",
        "If you want to extend this project further:\n",
        "\n",
        "1. **GPU Support**: Move tensors to GPU for faster inference\n",
        "2. **Caching**: Cache tokenization for repeated texts\n",
        "3. **Async Processing**: Implement async batch processing\n",
        "4. **Model Ensemble**: Support multiple models with voting\n",
        "5. **Export**: Save predictions to CSV/JSON\n",
        "6. **Visualization**: Plot confidence distributions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Part 6: Wrap-Up\n",
        "\n",
        "## Key Takeaways\n",
        "\n",
        "1. **Pipelines have three stages**:\n",
        "   - Tokenization: Text â†’ Token IDs + Attention Mask\n",
        "   - Inference: Token IDs â†’ Raw Logits\n",
        "   - Post-processing: Logits â†’ Probabilities â†’ Labels\n",
        "\n",
        "2. **Tokenization involves**:\n",
        "   - Splitting text into tokens (subword tokenization)\n",
        "   - Adding special tokens ([CLS], [SEP])\n",
        "   - Creating attention masks for padding\n",
        "\n",
        "3. **Logits are raw, unnormalized outputs** - use softmax to convert to probabilities\n",
        "\n",
        "4. **torch.no_grad()** is essential during inference to save memory and speed up computation\n",
        "\n",
        "5. **Understanding internals enables**:\n",
        "   - Better debugging\n",
        "   - Custom post-processing\n",
        "   - Batch processing optimization\n",
        "   - Access to intermediate representations\n",
        "\n",
        "## How This Connects to Previous Notebooks\n",
        "\n",
        "| Notebook | Key Pipeline Aspect |\n",
        "|----------|--------------------|\n",
        "| 1 (Fill-Mask) | [MASK] token handling |\n",
        "| 2 (NER) | Per-token predictions |\n",
        "| 3 (QA) | Position-based logits |\n",
        "| 4-5 (Generation) | Autoregressive decoding |\n",
        "| 6-9 (Classification) | Sequence classification |\n",
        "| **10 (This)** | **All of the above!** |\n",
        "\n",
        "## Common Mistakes to Avoid\n",
        "\n",
        "| Mistake | Why It's a Problem |\n",
        "|---------|-------------------|\n",
        "| Forgetting attention_mask | Model attends to padding tokens |\n",
        "| Not using torch.no_grad() | Wastes memory, slower |\n",
        "| Confusing logits and probs | Logits aren't normalized |\n",
        "| Ignoring batch dimension | Shape mismatch errors |\n",
        "| Not setting model.eval() | Dropout still active |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Solutions\n",
        "\n",
        "### Check Your Understanding (Quiz Answers)\n",
        "\n",
        "1. **B) Tokenization, inference, post-processing** - The three stages of a Hugging Face pipeline\n",
        "2. **B) Raw, unnormalized model outputs** - Logits need softmax to become probabilities\n",
        "3. **B) Indicates which tokens are real vs padding** - Tells the model which tokens to attend to\n",
        "4. **B) To save memory and speed up computation** - No need to track gradients during inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Summary of key functions from this notebook:\n",
        "\n",
        "summary = {\n",
        "    'Tokenization': {\n",
        "        'tokenizer.tokenize(text)': 'Split text into tokens',\n",
        "        'tokenizer.convert_tokens_to_ids()': 'Convert tokens to IDs',\n",
        "        'tokenizer(text, return_tensors=\"pt\")': 'Full encoding with special tokens',\n",
        "        'tokenizer.decode(ids)': 'Convert IDs back to text',\n",
        "    },\n",
        "    'Inference': {\n",
        "        'model.eval()': 'Set model to evaluation mode',\n",
        "        'torch.no_grad()': 'Disable gradient computation',\n",
        "        'model(**inputs)': 'Run forward pass',\n",
        "    },\n",
        "    'Post-processing': {\n",
        "        'F.softmax(logits, dim=-1)': 'Convert logits to probabilities',\n",
        "        'torch.argmax()': 'Get predicted class',\n",
        "        'model.config.id2label': 'Map class index to label',\n",
        "    },\n",
        "}\n",
        "\n",
        "print(\"Key Functions Summary:\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for stage, functions in summary.items():\n",
        "    print(f\"\\n{stage}:\")\n",
        "    for func, desc in functions.items():\n",
        "        print(f\"  {func}\")\n",
        "        print(f\"    â†’ {desc}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Congratulations!\n",
        "\n",
        "You've completed the entire 10-notebook course on Hugging Face Transformers! You now have:\n",
        "\n",
        "- **Foundational knowledge** of NLP tasks and transformer architectures\n",
        "- **Practical experience** with pipelines, models, and tokenizers\n",
        "- **Deep understanding** of how everything works under the hood\n",
        "\n",
        "## Additional Resources\n",
        "\n",
        "- [Hugging Face Transformers Documentation](https://huggingface.co/docs/transformers)\n",
        "- [Hugging Face Course](https://huggingface.co/course)\n",
        "- [PyTorch Documentation](https://pytorch.org/docs/stable/index.html)\n",
        "- [Tokenizers Library](https://huggingface.co/docs/tokenizers)\n",
        "- [Model Hub](https://huggingface.co/models)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
