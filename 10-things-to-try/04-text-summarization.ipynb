{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Text Summarization\n",
    "\n",
    "**Estimated Time**: ~2 hours\n",
    "\n",
    "**Prerequisites**: Notebooks 1-3 (understanding of tokenization, pipelines, confidence scores, and context comprehension from QA)\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. **Distinguish** between extractive and abstractive summarization approaches\n",
    "2. **Understand** encoder-decoder (seq2seq) architecture for text generation\n",
    "3. **Control** summary length using min/max length parameters\n",
    "4. **Tune** generation parameters like beam search, sampling, and repetition penalty\n",
    "5. **Recognize** and mitigate hallucination in generated summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Run this cell first. If you completed Notebooks 1-3, you already have the core packages ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 1: Conceptual Foundation\n",
    "\n",
    "## What is Text Summarization?\n",
    "\n",
    "**In plain English**: Text summarization compresses a long document into a shorter version while preserving the most important information.\n",
    "\n",
    "**Technical definition**: Summarization is a text-to-text generation task where the model takes a long input sequence and produces a shorter output sequence that captures the key content.\n",
    "\n",
    "### Visual Example\n",
    "\n",
    "```\n",
    "INPUT (Long Article):\n",
    "\"Scientists at MIT have developed a new type of battery that can charge \n",
    "in under 5 minutes. The breakthrough uses a novel electrode material made \n",
    "from carbon nanotubes. Lead researcher Dr. Smith says the technology could \n",
    "revolutionize electric vehicles by eliminating range anxiety. The team \n",
    "published their findings in Nature Energy yesterday...\"\n",
    "\n",
    "OUTPUT (Summary):\n",
    "\"MIT researchers created a fast-charging battery using carbon nanotube \n",
    "electrodes that could transform electric vehicles.\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two Types of Summarization\n",
    "\n",
    "| Type | How It Works | Output | Pros/Cons |\n",
    "|------|--------------|--------|----------|\n",
    "| **Extractive** | Selects and combines existing sentences | Exact sentences from original | More faithful, but can be choppy |\n",
    "| **Abstractive** | Generates new text that paraphrases content | Newly written summary | More fluent, but may hallucinate |\n",
    "\n",
    "This notebook focuses on **Abstractive Summarization** - the model generates new text.\n",
    "\n",
    "```\n",
    "EXTRACTIVE SUMMARIZATION:\n",
    "┌─────────────────────────────────────────────┐\n",
    "│ Original: \"The cat sat on the mat. It was   │\n",
    "│ a sunny day. The cat was happy.\"            │\n",
    "└─────────────────┬───────────────────────────┘\n",
    "                  │ (select important sentences)\n",
    "                  ▼\n",
    "    \"The cat sat on the mat. The cat was happy.\"\n",
    "     └────────────────┬─────────────────────┘\n",
    "        Exact sentences from original\n",
    "\n",
    "ABSTRACTIVE SUMMARIZATION (This Notebook):\n",
    "┌─────────────────────────────────────────────┐\n",
    "│ Original: \"The cat sat on the mat. It was   │\n",
    "│ a sunny day. The cat was happy.\"            │\n",
    "└─────────────────┬───────────────────────────┘\n",
    "                  │ (understand and rewrite)\n",
    "                  ▼\n",
    "    \"A happy cat enjoyed sitting on a mat on a sunny day.\"\n",
    "     └────────────────────┬──────────────────────────────┘\n",
    "               Newly generated paraphrase\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How Abstractive Summarization Works: Encoder-Decoder\n",
    "\n",
    "Unlike the encoder-only models (BERT) from Notebooks 1-3, summarization uses an **encoder-decoder** architecture:\n",
    "\n",
    "```\n",
    "                    ENCODER                         DECODER\n",
    "              ┌─────────────────┐             ┌─────────────────┐\n",
    "INPUT TEXT    │                 │   CONTEXT   │                 │   OUTPUT\n",
    "\"Scientists   │  Processes and  │  ────────►  │  Generates new  │   \"MIT creates\n",
    " at MIT...\"   │  understands    │   VECTORS   │  text word by   │    fast battery\"\n",
    "              │  the input      │             │  word           │\n",
    "              └─────────────────┘             └─────────────────┘\n",
    "\n",
    "The encoder reads the entire input and creates a \"meaning\" representation.\n",
    "The decoder uses that meaning to generate the summary one token at a time.\n",
    "```\n",
    "\n",
    "Popular encoder-decoder models for summarization:\n",
    "- **BART** (Facebook): Bidirectional encoder + autoregressive decoder\n",
    "- **T5** (Google): \"Text-to-Text Transfer Transformer\"\n",
    "- **Pegasus** (Google): Pre-trained specifically for summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connection to Previous Notebooks\n",
    "\n",
    "| Notebook | Architecture | Task |\n",
    "|----------|--------------|------|\n",
    "| 1-3 (MLM, NER, QA) | Encoder-only (BERT) | Understanding/extraction |\n",
    "| **4 (Summarization)** | **Encoder-Decoder** | **Generation** |\n",
    "\n",
    "```\n",
    "ENCODER-ONLY (Previous Notebooks):     ENCODER-DECODER (This Notebook):\n",
    "┌──────────────────┐                   ┌──────────────────┐   ┌──────────────────┐\n",
    "│      ENCODER     │                   │      ENCODER     │───│     DECODER      │\n",
    "│                  │                   │                  │   │                  │\n",
    "│  Input → Output  │                   │  Input ─────────►│───│──────► Output    │\n",
    "│  (same length)   │                   │  (understanding) │   │  (generation)    │\n",
    "└──────────────────┘                   └──────────────────┘   └──────────────────┘\n",
    "   Tasks:                                 Tasks:\n",
    "   - Fill masked words                    - Summarization\n",
    "   - Classify tokens (NER)                - Translation\n",
    "   - Extract spans (QA)                   - Text generation\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Generation Concepts\n",
    "\n",
    "Unlike extraction (finding existing text), generation requires **decoding strategies**:\n",
    "\n",
    "| Concept | Description |\n",
    "|---------|-------------|\n",
    "| **Beam Search** | Explores multiple candidate sequences, keeps top-k best |\n",
    "| **Greedy Decoding** | Always picks the highest-probability next token |\n",
    "| **Sampling** | Randomly samples from probability distribution |\n",
    "| **Top-k Sampling** | Samples only from the k most likely tokens |\n",
    "| **Top-p (Nucleus)** | Samples from smallest set covering p probability mass |\n",
    "| **Temperature** | Controls randomness (lower = more focused, higher = more random) |\n",
    "| **Repetition Penalty** | Discourages repeating the same words/phrases |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real-World Applications\n",
    "\n",
    "Summarization powers many practical applications:\n",
    "\n",
    "- **News Digests**: Summarize articles for quick reading\n",
    "- **Meeting Notes**: Condense transcripts into action items\n",
    "- **Legal/Medical**: Summarize lengthy documents\n",
    "- **Email Triage**: Preview long emails with summaries\n",
    "- **Research**: Summarize academic papers\n",
    "- **Social Media**: Auto-generate post summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Terminology\n",
    "\n",
    "| Term | Definition |\n",
    "|------|------------|\n",
    "| **Abstractive** | Generating new text to summarize content |\n",
    "| **Extractive** | Selecting existing sentences to form summary |\n",
    "| **Encoder-Decoder** | Architecture with separate understanding and generation components |\n",
    "| **Seq2Seq** | Sequence-to-sequence: mapping one sequence to another |\n",
    "| **Beam Search** | Decoding strategy exploring multiple candidates |\n",
    "| **Hallucination** | Model generates plausible but factually incorrect content |\n",
    "| **Compression Ratio** | Original length / Summary length |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Your Understanding\n",
    "\n",
    "Before moving on, try to answer these questions (answers at the end):\n",
    "\n",
    "1. What type of summarization generates new text rather than selecting existing sentences?\n",
    "   - A) Extractive summarization\n",
    "   - B) Abstractive summarization\n",
    "   - C) Selective summarization\n",
    "\n",
    "2. What architecture is used for abstractive summarization?\n",
    "   - A) Encoder-only (like BERT)\n",
    "   - B) Decoder-only (like GPT)\n",
    "   - C) Encoder-Decoder (like BART, T5)\n",
    "\n",
    "3. What is \"hallucination\" in the context of summarization?\n",
    "   - A) When the model refuses to generate output\n",
    "   - B) When the model generates plausible but incorrect information\n",
    "   - C) When the summary is too long\n",
    "\n",
    "4. What does beam search do during text generation?\n",
    "   - A) Searches for keywords in the input\n",
    "   - B) Explores multiple candidate sequences and keeps the best ones\n",
    "   - C) Splits the input into beams of text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: Basic Implementation\n",
    "\n",
    "## Your First Summarization Pipeline\n",
    "\n",
    "Let's create a summarization pipeline and compress some text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a summarization pipeline\n",
    "# The default model is sshleifer/distilbart-cnn-12-6\n",
    "summarizer = pipeline(\"summarization\")\n",
    "\n",
    "# Sample article to summarize\n",
    "article = \"\"\"\n",
    "Artificial intelligence researchers at Google DeepMind have achieved a significant \n",
    "breakthrough in protein structure prediction. Their AI system, called AlphaFold, \n",
    "can accurately predict the 3D structure of proteins from their amino acid sequences. \n",
    "This has been one of biology's grand challenges for over 50 years.\n",
    "\n",
    "The implications are enormous for drug discovery and understanding diseases. \n",
    "Previously, determining protein structures required expensive and time-consuming \n",
    "experiments like X-ray crystallography. AlphaFold can now predict structures in \n",
    "hours rather than years, and with remarkable accuracy.\n",
    "\n",
    "The team has released predictions for nearly all known proteins, totaling over \n",
    "200 million structures. Scientists worldwide are already using this data to \n",
    "accelerate research in areas ranging from antibiotic resistance to plastic \n",
    "pollution cleanup.\n",
    "\"\"\"\n",
    "\n",
    "# Generate summary\n",
    "summary = summarizer(article, max_length=60, min_length=20, do_sample=False)\n",
    "\n",
    "print(\"Original article:\")\n",
    "print(article.strip())\n",
    "print(f\"\\n{'='*60}\\n\")\n",
    "print(\"Summary:\")\n",
    "print(summary[0]['summary_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Output\n",
    "\n",
    "The summarization pipeline returns a list of dictionaries, each containing:\n",
    "- `summary_text`: The generated summary\n",
    "\n",
    "Let's examine compression ratios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate compression statistics\n",
    "original_words = len(article.split())\n",
    "summary_words = len(summary[0]['summary_text'].split())\n",
    "compression_ratio = original_words / summary_words\n",
    "\n",
    "print(\"Compression Statistics:\")\n",
    "print(\"=\"*40)\n",
    "print(f\"  Original words:   {original_words}\")\n",
    "print(f\"  Summary words:    {summary_words}\")\n",
    "print(f\"  Compression ratio: {compression_ratio:.1f}x\")\n",
    "print(f\"  Reduction:        {(1 - summary_words/original_words)*100:.0f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Controlling Summary Length\n",
    "\n",
    "You can control the output length with `min_length` and `max_length` parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different summary lengths\n",
    "length_configs = [\n",
    "    {\"name\": \"Very Short\", \"min_length\": 10, \"max_length\": 30},\n",
    "    {\"name\": \"Short\", \"min_length\": 20, \"max_length\": 50},\n",
    "    {\"name\": \"Medium\", \"min_length\": 40, \"max_length\": 80},\n",
    "    {\"name\": \"Long\", \"min_length\": 60, \"max_length\": 120},\n",
    "]\n",
    "\n",
    "print(\"Summary Length Comparison:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for config in length_configs:\n",
    "    result = summarizer(\n",
    "        article, \n",
    "        min_length=config[\"min_length\"], \n",
    "        max_length=config[\"max_length\"],\n",
    "        do_sample=False\n",
    "    )\n",
    "    words = len(result[0]['summary_text'].split())\n",
    "    print(f\"\\n[{config['name']}] ({words} words):\")\n",
    "    print(f\"  {result[0]['summary_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing Different Types of Text\n",
    "\n",
    "Let's see how summarization performs on various text types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on different text types\n",
    "test_texts = {\n",
    "    \"News Article\": \"\"\"\n",
    "    The Federal Reserve announced today that it will raise interest rates by \n",
    "    0.25 percentage points, bringing the benchmark rate to a range of 5.25% \n",
    "    to 5.5%. This marks the eleventh rate increase since March 2022 as the \n",
    "    central bank continues its fight against inflation. Fed Chair Jerome Powell \n",
    "    stated that future rate decisions will depend on incoming economic data. \n",
    "    Markets reacted positively to the news, with stocks rising slightly.\n",
    "    \"\"\",\n",
    "    \n",
    "    \"Scientific Abstract\": \"\"\"\n",
    "    We present a novel approach to training large language models using \n",
    "    reinforcement learning from human feedback (RLHF). Our method involves \n",
    "    three stages: supervised fine-tuning on demonstration data, reward model \n",
    "    training, and policy optimization using proximal policy optimization (PPO). \n",
    "    Experiments on multiple benchmarks show that our approach significantly \n",
    "    improves alignment with human preferences while maintaining strong \n",
    "    performance on standard NLP tasks.\n",
    "    \"\"\",\n",
    "    \n",
    "    \"Product Description\": \"\"\"\n",
    "    Introducing the SmartHome Hub 3.0, the ultimate central control system for \n",
    "    your connected home. With support for over 500 smart devices, voice control \n",
    "    via Alexa and Google Assistant, and our new AI-powered automation engine, \n",
    "    you can create the perfect smart home experience. The hub features a \n",
    "    7-inch touchscreen display, built-in speaker, and Thread/Matter compatibility \n",
    "    for future-proof connectivity.\n",
    "    \"\"\",\n",
    "}\n",
    "\n",
    "print(\"Summarizing Different Text Types:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for text_type, text in test_texts.items():\n",
    "    result = summarizer(text, max_length=50, min_length=15, do_sample=False)\n",
    "    print(f\"\\n[{text_type}]\")\n",
    "    print(f\"  Summary: {result[0]['summary_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 1: Multi-Length Summaries (Guided)\n",
    "\n",
    "**Difficulty**: Basic | **Time**: 10-15 minutes\n",
    "\n",
    "**Your task**: Create summaries of varying lengths for the same article and compare them.\n",
    "\n",
    "### Step 1: Create a summary generator function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_multi_length_summaries(text, summarizer_pipeline):\n",
    "    \"\"\"\n",
    "    Generate summaries at different compression levels.\n",
    "    \n",
    "    Returns:\n",
    "        dict with 'tweet', 'paragraph', and 'abstract' summaries\n",
    "    \"\"\"\n",
    "    summaries = {}\n",
    "    \n",
    "    # Tweet-length (very short, ~280 characters / ~40 words)\n",
    "    result = summarizer_pipeline(text, min_length=10, max_length=40, do_sample=False)\n",
    "    summaries['tweet'] = result[0]['summary_text']\n",
    "    \n",
    "    # Paragraph (medium, ~100 words)\n",
    "    result = summarizer_pipeline(text, min_length=40, max_length=100, do_sample=False)\n",
    "    summaries['paragraph'] = result[0]['summary_text']\n",
    "    \n",
    "    # Abstract (longer, ~150 words)\n",
    "    result = summarizer_pipeline(text, min_length=80, max_length=150, do_sample=False)\n",
    "    summaries['abstract'] = result[0]['summary_text']\n",
    "    \n",
    "    return summaries\n",
    "\n",
    "# Test article\n",
    "climate_article = \"\"\"\n",
    "A new study published in Nature Climate Change reveals that global temperatures \n",
    "could rise by 2.7 degrees Celsius by 2100 under current policies. Researchers \n",
    "from the University of Oxford analyzed data from 195 countries and found that \n",
    "existing climate commitments fall short of the Paris Agreement goals.\n",
    "\n",
    "The study highlights several key findings: ice sheet melting is accelerating \n",
    "faster than predicted, sea levels could rise by up to one meter, and extreme \n",
    "weather events are becoming more frequent. Lead author Dr. Sarah Chen emphasized \n",
    "that immediate action is needed to prevent irreversible damage.\n",
    "\n",
    "However, the research also identified pathways to limit warming to 1.5 degrees. \n",
    "These include rapid decarbonization of the energy sector, widespread adoption of \n",
    "electric vehicles, reforestation efforts, and carbon capture technologies. The \n",
    "researchers estimate this would require $4 trillion in annual investment.\n",
    "\n",
    "Several governments have already responded to the findings. The European Union \n",
    "announced plans to accelerate its green transition, while China committed to \n",
    "reaching carbon neutrality before 2060. Climate activists are calling for even \n",
    "more ambitious targets.\n",
    "\"\"\"\n",
    "\n",
    "summaries = generate_multi_length_summaries(climate_article, summarizer)\n",
    "\n",
    "print(\"Multi-Length Summary Comparison:\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Display and compare summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display summaries with statistics\n",
    "original_words = len(climate_article.split())\n",
    "\n",
    "for level, summary_text in summaries.items():\n",
    "    words = len(summary_text.split())\n",
    "    chars = len(summary_text)\n",
    "    compression = original_words / words\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"[{level.upper()}] - {words} words, {chars} chars, {compression:.1f}x compression\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(summary_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Try your own article\n",
    "\n",
    "Paste your own article and generate multi-length summaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Paste your own article\n",
    "my_article = \"\"\"\n",
    "Paste your article here. It should be at least a few paragraphs long\n",
    "for meaningful summarization. News articles, blog posts, or research\n",
    "abstracts work well.\n",
    "\"\"\"\n",
    "\n",
    "# Uncomment to run:\n",
    "# my_summaries = generate_multi_length_summaries(my_article, summarizer)\n",
    "# for level, summary in my_summaries.items():\n",
    "#     print(f\"\\n[{level.upper()}]\")\n",
    "#     print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 3: Intermediate Exploration\n",
    "\n",
    "## Generation Parameters Deep Dive\n",
    "\n",
    "Summarization quality depends heavily on generation parameters. Let's explore them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample text for experiments\n",
    "sample_text = \"\"\"\n",
    "SpaceX successfully launched its Starship rocket on its third test flight, \n",
    "achieving several milestones. The massive rocket reached space for the first \n",
    "time, demonstrating its potential for future Mars missions. However, both the \n",
    "booster and the spacecraft were lost during the descent phase. Despite this, \n",
    "SpaceX called the mission a success, noting the valuable data collected. The \n",
    "company plans to continue rapid iteration on the design.\n",
    "\"\"\"\n",
    "\n",
    "# Beam search vs Greedy decoding\n",
    "print(\"Beam Search vs Greedy Decoding:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Greedy (num_beams=1)\n",
    "greedy_result = summarizer(\n",
    "    sample_text, \n",
    "    max_length=50, \n",
    "    num_beams=1,  # Greedy\n",
    "    do_sample=False\n",
    ")\n",
    "print(f\"\\n[Greedy (num_beams=1)]:\")\n",
    "print(f\"  {greedy_result[0]['summary_text']}\")\n",
    "\n",
    "# Beam search (num_beams=4)\n",
    "beam_result = summarizer(\n",
    "    sample_text, \n",
    "    max_length=50, \n",
    "    num_beams=4,  # Beam search\n",
    "    do_sample=False\n",
    ")\n",
    "print(f\"\\n[Beam Search (num_beams=4)]:\")\n",
    "print(f\"  {beam_result[0]['summary_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling with different temperatures\n",
    "print(\"\\nTemperature Effects (with sampling):\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "temperatures = [0.3, 0.7, 1.0, 1.5]\n",
    "\n",
    "for temp in temperatures:\n",
    "    result = summarizer(\n",
    "        sample_text, \n",
    "        max_length=50,\n",
    "        do_sample=True,\n",
    "        temperature=temp,\n",
    "        top_k=50,\n",
    "    )\n",
    "    print(f\"\\n[Temperature = {temp}]:\")\n",
    "    print(f\"  {result[0]['summary_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Generation Parameters\n",
    "\n",
    "| Parameter | Effect | Typical Values |\n",
    "|-----------|--------|----------------|\n",
    "| `num_beams` | Higher = explores more candidates | 1 (greedy) to 8 |\n",
    "| `do_sample` | True = random sampling, False = deterministic | True/False |\n",
    "| `temperature` | Higher = more random, Lower = more focused | 0.1 to 2.0 |\n",
    "| `top_k` | Limits to k most likely tokens | 10 to 100 |\n",
    "| `top_p` | Nucleus sampling - uses smallest set with p probability | 0.9 to 0.95 |\n",
    "| `repetition_penalty` | Discourages repeating tokens | 1.0 to 2.0 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repetition penalty demonstration\n",
    "# Some texts can cause repetition issues\n",
    "repetitive_text = \"\"\"\n",
    "The new product is great. The product has many features. The product is \n",
    "available in stores. The product costs $99. The product is very popular. \n",
    "The product has received positive reviews. The product is manufactured \n",
    "in the USA. The product comes with a warranty.\n",
    "\"\"\"\n",
    "\n",
    "print(\"Repetition Penalty Effect:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Without repetition penalty\n",
    "result_no_penalty = summarizer(\n",
    "    repetitive_text,\n",
    "    max_length=50,\n",
    "    repetition_penalty=1.0,  # No penalty\n",
    "    do_sample=False\n",
    ")\n",
    "print(f\"\\n[No Penalty (1.0)]:\")\n",
    "print(f\"  {result_no_penalty[0]['summary_text']}\")\n",
    "\n",
    "# With repetition penalty\n",
    "result_with_penalty = summarizer(\n",
    "    repetitive_text,\n",
    "    max_length=50,\n",
    "    repetition_penalty=1.5,  # Penalty applied\n",
    "    do_sample=False\n",
    ")\n",
    "print(f\"\\n[With Penalty (1.5)]:\")\n",
    "print(f\"  {result_with_penalty[0]['summary_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Summarization Models\n",
    "\n",
    "Different models have different strengths. Let's compare a few:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a different model - T5\n",
    "print(\"Loading T5 summarization model...\")\n",
    "summarizer_t5 = pipeline(\"summarization\", model=\"t5-small\")\n",
    "print(\"Model loaded!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare models on the same text\n",
    "comparison_text = \"\"\"\n",
    "Apple unveiled its new Vision Pro headset at WWDC, marking the company's entry \n",
    "into the mixed reality market. The device features a high-resolution display \n",
    "system, advanced eye and hand tracking, and runs on a new operating system \n",
    "called visionOS. Priced at $3,499, it targets professional and creative users \n",
    "rather than the mass market. Analysts are divided on whether it will succeed \n",
    "in a market dominated by Meta's more affordable Quest devices.\n",
    "\"\"\"\n",
    "\n",
    "print(\"Model Comparison:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# DistilBART (default)\n",
    "result_bart = summarizer(comparison_text, max_length=50, min_length=20, do_sample=False)\n",
    "print(f\"\\n[DistilBART (default)]:\")\n",
    "print(f\"  {result_bart[0]['summary_text']}\")\n",
    "\n",
    "# T5\n",
    "result_t5 = summarizer_t5(comparison_text, max_length=50, min_length=20, do_sample=False)\n",
    "print(f\"\\n[T5-small]:\")\n",
    "print(f\"  {result_t5[0]['summary_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 2: Parameter Tuning (Semi-guided)\n",
    "\n",
    "**Difficulty**: Intermediate | **Time**: 15-20 minutes\n",
    "\n",
    "**Your task**: Write a function that finds the best generation parameters for a given text.\n",
    "\n",
    "**Hints**:\n",
    "1. Try different combinations of parameters\n",
    "2. Evaluate based on length, readability, and information preservation\n",
    "3. Consider using metrics like word overlap with original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "def evaluate_summary(original, summary):\n",
    "    \"\"\"\n",
    "    Simple evaluation metrics for a summary.\n",
    "    \"\"\"\n",
    "    original_words = set(original.lower().split())\n",
    "    summary_words = set(summary.lower().split())\n",
    "    \n",
    "    # Word overlap (crude measure of information retention)\n",
    "    overlap = len(original_words & summary_words) / len(original_words)\n",
    "    \n",
    "    # Compression ratio\n",
    "    compression = len(original.split()) / max(len(summary.split()), 1)\n",
    "    \n",
    "    # Length check\n",
    "    word_count = len(summary.split())\n",
    "    \n",
    "    return {\n",
    "        'word_overlap': overlap,\n",
    "        'compression_ratio': compression,\n",
    "        'word_count': word_count,\n",
    "    }\n",
    "\n",
    "\n",
    "def find_best_parameters(summarizer_pipeline, text, target_length=50):\n",
    "    \"\"\"\n",
    "    Try different parameter combinations and return the best summary.\n",
    "    \"\"\"\n",
    "    parameter_sets = [\n",
    "        {\"num_beams\": 1, \"do_sample\": False, \"name\": \"Greedy\"},\n",
    "        {\"num_beams\": 4, \"do_sample\": False, \"name\": \"Beam-4\"},\n",
    "        {\"num_beams\": 4, \"do_sample\": False, \"length_penalty\": 1.5, \"name\": \"Beam-4+Length\"},\n",
    "        {\"num_beams\": 1, \"do_sample\": True, \"temperature\": 0.7, \"top_k\": 50, \"name\": \"Sampling\"},\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for params in parameter_sets:\n",
    "        name = params.pop(\"name\")\n",
    "        \n",
    "        summary = summarizer_pipeline(\n",
    "            text,\n",
    "            max_length=target_length,\n",
    "            min_length=target_length // 2,\n",
    "            **params\n",
    "        )[0]['summary_text']\n",
    "        \n",
    "        metrics = evaluate_summary(text, summary)\n",
    "        metrics['name'] = name\n",
    "        metrics['summary'] = summary\n",
    "        \n",
    "        results.append(metrics)\n",
    "        \n",
    "        # Restore name for next iteration\n",
    "        params[\"name\"] = name\n",
    "    \n",
    "    # Sort by word overlap (higher is better)\n",
    "    results.sort(key=lambda x: x['word_overlap'], reverse=True)\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# Test the parameter finder\n",
    "test_article = \"\"\"\n",
    "Researchers at Stanford University have developed a new AI system that can \n",
    "detect early signs of Alzheimer's disease from brain scans with 94% accuracy. \n",
    "The deep learning model was trained on thousands of MRI images and can identify \n",
    "subtle patterns that are invisible to the human eye. Early detection could \n",
    "enable earlier treatment and better patient outcomes. The team is now working \n",
    "with hospitals to conduct clinical trials.\n",
    "\"\"\"\n",
    "\n",
    "results = find_best_parameters(summarizer, test_article)\n",
    "\n",
    "print(\"Parameter Comparison Results:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for r in results:\n",
    "    print(f\"\\n[{r['name']}]\")\n",
    "    print(f\"  Word overlap: {r['word_overlap']:.1%}\")\n",
    "    print(f\"  Compression:  {r['compression_ratio']:.1f}x\")\n",
    "    print(f\"  Words:        {r['word_count']}\")\n",
    "    print(f\"  Summary:      {r['summary']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 4: Advanced Topics\n",
    "\n",
    "## Understanding Hallucination\n",
    "\n",
    "A critical issue with abstractive summarization is **hallucination** - when the model generates plausible but factually incorrect information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example where hallucination might occur\n",
    "# (The model might add details not present in the original)\n",
    "hallucination_test = \"\"\"\n",
    "The company announced record profits for the quarter. Revenue grew significantly \n",
    "compared to last year. The CEO expressed optimism about future growth.\n",
    "\"\"\"\n",
    "\n",
    "# Generate multiple summaries to see variation\n",
    "print(\"Potential Hallucination Detection:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Original text: {hallucination_test.strip()}\\n\")\n",
    "\n",
    "# Generate with sampling (more likely to hallucinate)\n",
    "for i in range(3):\n",
    "    result = summarizer(\n",
    "        hallucination_test,\n",
    "        max_length=40,\n",
    "        do_sample=True,\n",
    "        temperature=0.9,\n",
    "    )\n",
    "    print(f\"Summary {i+1}: {result[0]['summary_text']}\")\n",
    "\n",
    "print(\"\\n⚠️ Watch for: specific numbers, percentages, or names not in the original!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategies to reduce hallucination\n",
    "\n",
    "def check_for_hallucinations(original, summary):\n",
    "    \"\"\"\n",
    "    Simple check for potential hallucinations.\n",
    "    Looks for specific patterns that might indicate made-up content.\n",
    "    \"\"\"\n",
    "    import re\n",
    "    \n",
    "    warnings = []\n",
    "    \n",
    "    # Check for numbers in summary not in original\n",
    "    summary_numbers = set(re.findall(r'\\b\\d+(?:\\.\\d+)?%?\\b', summary))\n",
    "    original_numbers = set(re.findall(r'\\b\\d+(?:\\.\\d+)?%?\\b', original))\n",
    "    new_numbers = summary_numbers - original_numbers\n",
    "    \n",
    "    if new_numbers:\n",
    "        warnings.append(f\"Numbers in summary not in original: {new_numbers}\")\n",
    "    \n",
    "    # Check for proper nouns (simplified - words starting with capitals)\n",
    "    # This is a rough heuristic\n",
    "    summary_words = summary.split()\n",
    "    original_lower = original.lower()\n",
    "    \n",
    "    for word in summary_words:\n",
    "        if word[0].isupper() and len(word) > 2:\n",
    "            if word.lower() not in original_lower:\n",
    "                warnings.append(f\"Capitalized word not in original: '{word}'\")\n",
    "    \n",
    "    return warnings\n",
    "\n",
    "\n",
    "# Test hallucination detection\n",
    "original = \"\"\"\n",
    "The startup raised funding in its Series A round. Investors included several \n",
    "venture capital firms. The company plans to use the funds for expansion.\n",
    "\"\"\"\n",
    "\n",
    "# Potentially hallucinated summary\n",
    "fake_summary = \"The startup raised $50 million in Series A funding from Sequoia Capital.\"\n",
    "\n",
    "print(\"Hallucination Detection:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Original: {original.strip()}\")\n",
    "print(f\"\\nSummary: {fake_summary}\")\n",
    "\n",
    "warnings = check_for_hallucinations(original, fake_summary)\n",
    "if warnings:\n",
    "    print(f\"\\n⚠️ Potential hallucinations detected:\")\n",
    "    for w in warnings:\n",
    "        print(f\"  - {w}\")\n",
    "else:\n",
    "    print(\"\\n✓ No obvious hallucinations detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Under the Hood: Encoder-Decoder Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and tokenizer separately to see internals\n",
    "model_name = \"sshleifer/distilbart-cnn-12-6\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "print(f\"Model: {model_name}\")\n",
    "print(f\"Model type: {type(model).__name__}\")\n",
    "print(f\"\\nModel structure:\")\n",
    "print(f\"  Encoder layers: {len(model.model.encoder.layers)}\")\n",
    "print(f\"  Decoder layers: {len(model.model.decoder.layers)}\")\n",
    "print(f\"  Hidden size: {model.config.d_model}\")\n",
    "print(f\"  Vocab size: {model.config.vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step-by-step generation\n",
    "text = \"Scientists discovered a new species of deep-sea fish. The fish lives at depths exceeding 8,000 meters.\"\n",
    "\n",
    "# STEP 1: Tokenize input\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "\n",
    "print(\"STEP 1 - Tokenization:\")\n",
    "print(f\"  Input text: '{text}'\")\n",
    "print(f\"  Input tokens: {inputs['input_ids'].shape[1]}\")\n",
    "print(f\"  Token IDs: {inputs['input_ids'][0][:10].tolist()}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2: Encode\n",
    "with torch.no_grad():\n",
    "    encoder_outputs = model.model.encoder(**inputs)\n",
    "\n",
    "print(\"STEP 2 - Encoding:\")\n",
    "print(f\"  Encoder output shape: {encoder_outputs.last_hidden_state.shape}\")\n",
    "print(f\"  (batch_size, sequence_length, hidden_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 3: Generate (decode)\n",
    "with torch.no_grad():\n",
    "    generated_ids = model.generate(\n",
    "        inputs['input_ids'],\n",
    "        max_length=50,\n",
    "        num_beams=4,\n",
    "        early_stopping=True,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True,\n",
    "    )\n",
    "\n",
    "# Decode the generated tokens\n",
    "summary = tokenizer.decode(generated_ids.sequences[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"STEP 3 - Generation:\")\n",
    "print(f\"  Generated token count: {len(generated_ids.sequences[0])}\")\n",
    "print(f\"  Generated summary: '{summary}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize generation step by step\n",
    "print(\"\\nToken-by-token generation:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "tokens = tokenizer.convert_ids_to_tokens(generated_ids.sequences[0])\n",
    "for i, token in enumerate(tokens[:15]):  # First 15 tokens\n",
    "    if token not in ['<s>', '</s>', '<pad>']:\n",
    "        print(f\"  Step {i:2}: '{token}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Considerations\n",
    "\n",
    "| Consideration | Recommendation |\n",
    "|---------------|----------------|\n",
    "| **Model size** | DistilBART for speed, BART-large for quality |\n",
    "| **Long inputs** | Use models trained for long documents (LED, Longformer) |\n",
    "| **Hallucination** | Use beam search (not sampling), verify facts |\n",
    "| **Batch processing** | Process multiple texts together |\n",
    "| **Length control** | Tune min/max_length for your use case |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitations of Abstractive Summarization\n",
    "\n",
    "1. **Hallucination**: May generate plausible but incorrect facts\n",
    "2. **Context limits**: Most models limited to ~1024 tokens\n",
    "3. **Domain sensitivity**: Models trained on news may struggle with technical text\n",
    "4. **Evaluation difficulty**: Hard to automatically measure summary quality\n",
    "5. **Abstractiveness**: Sometimes just paraphrases rather than truly summarizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate domain sensitivity\n",
    "domain_examples = {\n",
    "    \"News (good)\": \"\"\"\n",
    "    The president announced new economic policies today at a press conference. \n",
    "    The measures include tax cuts for middle-income families and increased \n",
    "    infrastructure spending. Market analysts responded positively to the news.\n",
    "    \"\"\",\n",
    "    \n",
    "    \"Technical (may struggle)\": \"\"\"\n",
    "    The implementation uses a red-black tree with amortized O(log n) insertion \n",
    "    complexity. Memory allocation is handled through a custom slab allocator to \n",
    "    minimize fragmentation. The garbage collector uses a generational approach \n",
    "    with mark-and-sweep for the old generation.\n",
    "    \"\"\",\n",
    "    \n",
    "    \"Legal (may struggle)\": \"\"\"\n",
    "    Pursuant to Section 14(a) of the Securities Exchange Act of 1934, the \n",
    "    undersigned hereby certifies that to the best of their knowledge, the \n",
    "    proxy statement does not contain any untrue statement of material fact \n",
    "    or omit to state a material fact necessary to make the statements therein \n",
    "    not misleading.\n",
    "    \"\"\",\n",
    "}\n",
    "\n",
    "print(\"Domain Comparison:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for domain, text in domain_examples.items():\n",
    "    result = summarizer(text, max_length=40, min_length=15, do_sample=False)\n",
    "    print(f\"\\n[{domain}]\")\n",
    "    print(f\"  {result[0]['summary_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 3: Model Comparison (Independent)\n",
    "\n",
    "**Difficulty**: Advanced | **Time**: 15-20 minutes\n",
    "\n",
    "**Your task**: Build a class that compares multiple summarization models on the same text.\n",
    "\n",
    "**Requirements**:\n",
    "1. Load and compare at least 2 different models\n",
    "2. Generate summaries with consistent parameters\n",
    "3. Calculate comparison metrics\n",
    "4. Identify which model works best for different text types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "class SummarizationComparator:\n",
    "    \"\"\"\n",
    "    Compares multiple summarization models on the same text.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize with multiple models.\"\"\"\n",
    "        self.models = {}\n",
    "        self.load_models()\n",
    "    \n",
    "    def load_models(self):\n",
    "        \"\"\"Load summarization models.\"\"\"\n",
    "        print(\"Loading models...\")\n",
    "        \n",
    "        # Default DistilBART\n",
    "        self.models['DistilBART'] = pipeline(\"summarization\")\n",
    "        print(\"  Loaded DistilBART\")\n",
    "        \n",
    "        # T5-small\n",
    "        self.models['T5-small'] = pipeline(\"summarization\", model=\"t5-small\")\n",
    "        print(\"  Loaded T5-small\")\n",
    "        \n",
    "        print(\"All models loaded!\")\n",
    "    \n",
    "    def compare(self, text, max_length=60, min_length=20):\n",
    "        \"\"\"\n",
    "        Compare all models on the same text.\n",
    "        \n",
    "        Returns:\n",
    "            dict with model names as keys and results as values\n",
    "        \"\"\"\n",
    "        results = {}\n",
    "        original_words = len(text.split())\n",
    "        \n",
    "        for name, model in self.models.items():\n",
    "            summary = model(\n",
    "                text,\n",
    "                max_length=max_length,\n",
    "                min_length=min_length,\n",
    "                do_sample=False,\n",
    "            )[0]['summary_text']\n",
    "            \n",
    "            summary_words = len(summary.split())\n",
    "            \n",
    "            results[name] = {\n",
    "                'summary': summary,\n",
    "                'word_count': summary_words,\n",
    "                'compression': original_words / summary_words,\n",
    "                'char_count': len(summary),\n",
    "            }\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def format_comparison(self, text, results):\n",
    "        \"\"\"\n",
    "        Format comparison results for display.\n",
    "        \"\"\"\n",
    "        lines = []\n",
    "        lines.append(\"Summarization Model Comparison\")\n",
    "        lines.append(\"=\" * 70)\n",
    "        lines.append(f\"Original text ({len(text.split())} words):\")\n",
    "        lines.append(f\"  {text[:150]}...\")\n",
    "        lines.append(\"\")\n",
    "        \n",
    "        for name, result in results.items():\n",
    "            lines.append(f\"[{name}]\")\n",
    "            lines.append(f\"  Words: {result['word_count']} | Compression: {result['compression']:.1f}x\")\n",
    "            lines.append(f\"  Summary: {result['summary']}\")\n",
    "            lines.append(\"\")\n",
    "        \n",
    "        return '\\n'.join(lines)\n",
    "\n",
    "\n",
    "# Create the comparator\n",
    "comparator = SummarizationComparator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on different text types\n",
    "test_texts = {\n",
    "    \"Tech News\": \"\"\"\n",
    "    OpenAI released GPT-4, their most advanced AI model yet. The new model \n",
    "    demonstrates improved reasoning capabilities and can process both text \n",
    "    and images. Early tests show it outperforms previous versions on various \n",
    "    benchmarks. The company is gradually rolling out access to developers \n",
    "    and businesses through their API.\n",
    "    \"\"\",\n",
    "    \n",
    "    \"Science\": \"\"\"\n",
    "    A team of astronomers has detected water vapor in the atmosphere of a \n",
    "    planet orbiting in the habitable zone of a distant star. The exoplanet, \n",
    "    named K2-18b, is located about 110 light-years from Earth. This discovery \n",
    "    marks the first time water has been found on a potentially habitable \n",
    "    world outside our solar system. The findings were published in Nature.\n",
    "    \"\"\",\n",
    "}\n",
    "\n",
    "for text_type, text in test_texts.items():\n",
    "    print(f\"\\n{'#'*70}\")\n",
    "    print(f\"TEXT TYPE: {text_type}\")\n",
    "    print(f\"{'#'*70}\")\n",
    "    \n",
    "    results = comparator.compare(text)\n",
    "    print(comparator.format_comparison(text, results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 5: Mini-Project\n",
    "\n",
    "## Project: Article Digest Generator\n",
    "\n",
    "**Scenario**: You're building a content curation tool that needs to generate summaries at different lengths for different platforms.\n",
    "\n",
    "**Your goal**: Build an `ArticleDigestGenerator` class that:\n",
    "1. Takes an article as input\n",
    "2. Generates summaries for tweet, paragraph, and abstract lengths\n",
    "3. Includes hallucination warnings\n",
    "4. Provides formatting for different platforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MINI-PROJECT: Article Digest Generator\n",
    "# ======================================\n",
    "\n",
    "import re\n",
    "\n",
    "class ArticleDigestGenerator:\n",
    "    \"\"\"\n",
    "    Generates multi-format digests from articles.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the digest generator.\"\"\"\n",
    "        self.summarizer = pipeline(\"summarization\")\n",
    "        \n",
    "        # Length configurations for different formats\n",
    "        self.formats = {\n",
    "            'tweet': {'min': 10, 'max': 35, 'char_limit': 280},\n",
    "            'paragraph': {'min': 30, 'max': 80, 'char_limit': 500},\n",
    "            'abstract': {'min': 60, 'max': 150, 'char_limit': 1000},\n",
    "        }\n",
    "    \n",
    "    def generate_digest(self, article):\n",
    "        \"\"\"\n",
    "        Generate digests in all formats.\n",
    "        \n",
    "        Args:\n",
    "            article: The full article text\n",
    "            \n",
    "        Returns:\n",
    "            dict with digests for each format\n",
    "        \"\"\"\n",
    "        digests = {}\n",
    "        original_word_count = len(article.split())\n",
    "        \n",
    "        for format_name, config in self.formats.items():\n",
    "            # Generate summary\n",
    "            result = self.summarizer(\n",
    "                article,\n",
    "                min_length=config['min'],\n",
    "                max_length=config['max'],\n",
    "                do_sample=False,\n",
    "                num_beams=4,\n",
    "            )\n",
    "            \n",
    "            summary = result[0]['summary_text']\n",
    "            \n",
    "            # Trim to character limit if needed\n",
    "            if len(summary) > config['char_limit']:\n",
    "                summary = summary[:config['char_limit']-3] + \"...\"\n",
    "            \n",
    "            # Check for potential hallucinations\n",
    "            warnings = self._check_hallucinations(article, summary)\n",
    "            \n",
    "            digests[format_name] = {\n",
    "                'text': summary,\n",
    "                'word_count': len(summary.split()),\n",
    "                'char_count': len(summary),\n",
    "                'compression': original_word_count / len(summary.split()),\n",
    "                'warnings': warnings,\n",
    "            }\n",
    "        \n",
    "        return digests\n",
    "    \n",
    "    def _check_hallucinations(self, original, summary):\n",
    "        \"\"\"\n",
    "        Check for potential hallucinations in the summary.\n",
    "        \"\"\"\n",
    "        warnings = []\n",
    "        \n",
    "        # Check for new numbers\n",
    "        summary_nums = set(re.findall(r'\\b\\d+(?:\\.\\d+)?%?\\b', summary))\n",
    "        original_nums = set(re.findall(r'\\b\\d+(?:\\.\\d+)?%?\\b', original))\n",
    "        new_nums = summary_nums - original_nums\n",
    "        \n",
    "        if new_nums:\n",
    "            warnings.append(f\"New numbers not in original: {new_nums}\")\n",
    "        \n",
    "        return warnings\n",
    "    \n",
    "    def format_for_twitter(self, digest):\n",
    "        \"\"\"\n",
    "        Format digest for Twitter posting.\n",
    "        \"\"\"\n",
    "        text = digest['tweet']['text']\n",
    "        chars = digest['tweet']['char_count']\n",
    "        \n",
    "        lines = []\n",
    "        lines.append(\"Twitter Post Preview\")\n",
    "        lines.append(\"-\" * 40)\n",
    "        lines.append(text)\n",
    "        lines.append(\"-\" * 40)\n",
    "        lines.append(f\"Characters: {chars}/280\")\n",
    "        \n",
    "        if digest['tweet']['warnings']:\n",
    "            lines.append(f\"⚠️ Warnings: {digest['tweet']['warnings']}\")\n",
    "        \n",
    "        return '\\n'.join(lines)\n",
    "    \n",
    "    def format_for_newsletter(self, digest):\n",
    "        \"\"\"\n",
    "        Format digest for email newsletter.\n",
    "        \"\"\"\n",
    "        lines = []\n",
    "        lines.append(\"📰 NEWSLETTER DIGEST\")\n",
    "        lines.append(\"=\" * 50)\n",
    "        lines.append(\"\")\n",
    "        lines.append(\"📝 TLDR:\")\n",
    "        lines.append(digest['tweet']['text'])\n",
    "        lines.append(\"\")\n",
    "        lines.append(\"📖 SUMMARY:\")\n",
    "        lines.append(digest['paragraph']['text'])\n",
    "        lines.append(\"\")\n",
    "        lines.append(\"📚 DETAILED:\")\n",
    "        lines.append(digest['abstract']['text'])\n",
    "        \n",
    "        return '\\n'.join(lines)\n",
    "    \n",
    "    def get_stats(self, digest):\n",
    "        \"\"\"\n",
    "        Get statistics about the generated digests.\n",
    "        \"\"\"\n",
    "        lines = []\n",
    "        lines.append(\"Digest Statistics\")\n",
    "        lines.append(\"=\" * 50)\n",
    "        \n",
    "        for format_name, data in digest.items():\n",
    "            lines.append(f\"\\n[{format_name.upper()}]\")\n",
    "            lines.append(f\"  Words: {data['word_count']}\")\n",
    "            lines.append(f\"  Characters: {data['char_count']}\")\n",
    "            lines.append(f\"  Compression: {data['compression']:.1f}x\")\n",
    "            \n",
    "            if data['warnings']:\n",
    "                lines.append(f\"  ⚠️ Warnings: {len(data['warnings'])}\")\n",
    "        \n",
    "        return '\\n'.join(lines)\n",
    "\n",
    "\n",
    "# Create the generator\n",
    "generator = ArticleDigestGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a sample article\n",
    "sample_article = \"\"\"\n",
    "Microsoft announced a major investment in artificial intelligence company OpenAI, \n",
    "committing billions of dollars to accelerate AI research and development. The \n",
    "partnership will integrate OpenAI's technology into Microsoft's cloud computing \n",
    "platform Azure, making advanced AI capabilities available to enterprise customers.\n",
    "\n",
    "CEO Satya Nadella emphasized that this investment aligns with Microsoft's long-term \n",
    "vision of democratizing AI technology. The company plans to use OpenAI's models \n",
    "to enhance products across its portfolio, including Office, Bing, and GitHub Copilot.\n",
    "\n",
    "Industry analysts view this as a strategic move to compete with Google and Amazon \n",
    "in the rapidly evolving AI market. The deal gives Microsoft exclusive access to \n",
    "OpenAI's most advanced models while providing OpenAI with the computing resources \n",
    "needed to train increasingly powerful AI systems.\n",
    "\n",
    "OpenAI, known for creating ChatGPT and DALL-E, has emerged as a leader in \n",
    "generative AI technology. The investment comes at a time when demand for AI \n",
    "services is surging across industries, from healthcare to finance to entertainment.\n",
    "\"\"\"\n",
    "\n",
    "# Generate digests\n",
    "digest = generator.generate_digest(sample_article)\n",
    "\n",
    "# Display all formats\n",
    "print(generator.get_stats(digest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show Twitter format\n",
    "print(\"\\n\")\n",
    "print(generator.format_for_twitter(digest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show newsletter format\n",
    "print(\"\\n\")\n",
    "print(generator.format_for_newsletter(digest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try with your own article\n",
    "# Uncomment to use:\n",
    "\n",
    "# your_article = \"\"\"\n",
    "# Paste your article here...\n",
    "# \"\"\"\n",
    "# your_digest = generator.generate_digest(your_article)\n",
    "# print(generator.format_for_newsletter(your_digest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extension Ideas\n",
    "\n",
    "If you want to extend this project further:\n",
    "\n",
    "1. **Keyword extraction**: Add keywords/hashtags to the tweet format\n",
    "2. **Sentiment analysis**: Detect and report the article's sentiment\n",
    "3. **Topic classification**: Categorize the article by topic\n",
    "4. **Multi-article summaries**: Summarize multiple related articles together\n",
    "5. **Custom formats**: Add formats for LinkedIn, email subjects, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 6: Wrap-Up\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Abstractive summarization** generates new text to summarize content, unlike extractive which selects existing sentences\n",
    "\n",
    "2. **Encoder-decoder architecture** uses separate components for understanding input and generating output\n",
    "\n",
    "3. **Generation parameters** like beam search, temperature, and repetition penalty significantly affect output quality\n",
    "\n",
    "4. **Hallucination** is a real risk - models can generate plausible but factually incorrect information\n",
    "\n",
    "5. **Length control** via min_length and max_length allows generating summaries for different use cases\n",
    "\n",
    "## Common Mistakes to Avoid\n",
    "\n",
    "| Mistake | Why It's a Problem |\n",
    "|---------|-------------------|\n",
    "| Using sampling without temperature control | Can produce random or incoherent output |\n",
    "| Not checking for hallucinations | May publish factually incorrect summaries |\n",
    "| Setting max_length too short | Cuts off important information |\n",
    "| Using news-trained models on technical text | Poor quality summaries for specialized content |\n",
    "\n",
    "## What's Next?\n",
    "\n",
    "In **Notebook 5: Text Generation**, you'll learn:\n",
    "- How to generate open-ended text continuations\n",
    "- Different decoding strategies for creative control\n",
    "- The creativity-coherence tradeoff in language models\n",
    "\n",
    "This builds on summarization - both use encoder-decoder or decoder-only architectures, but generation focuses on creativity rather than compression!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Solutions\n",
    "\n",
    "### Check Your Understanding (Quiz Answers)\n",
    "\n",
    "1. **B) Abstractive summarization** - Generates new text rather than selecting existing sentences\n",
    "2. **C) Encoder-Decoder (like BART, T5)** - Encoder understands input, decoder generates output\n",
    "3. **B) When the model generates plausible but incorrect information** - A significant risk in abstractive summarization\n",
    "4. **B) Explores multiple candidate sequences and keeps the best ones** - More thorough than greedy decoding\n",
    "\n",
    "### Exercise 2: Parameter Tuning (Key Insights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key insights from parameter tuning:\n",
    "\n",
    "# 1. Beam search (num_beams > 1) generally produces more coherent summaries\n",
    "#    but takes longer to generate\n",
    "\n",
    "# 2. Higher length_penalty values encourage longer summaries\n",
    "\n",
    "# 3. Sampling with low temperature (0.3-0.7) can add variety while\n",
    "#    maintaining quality\n",
    "\n",
    "# 4. For factual content, avoid high temperature (>1.0) to reduce\n",
    "#    hallucination risk\n",
    "\n",
    "# Best practices:\n",
    "best_params_factual = {\n",
    "    \"num_beams\": 4,\n",
    "    \"do_sample\": False,\n",
    "    \"length_penalty\": 1.0,\n",
    "    \"repetition_penalty\": 1.2,\n",
    "}\n",
    "\n",
    "best_params_creative = {\n",
    "    \"num_beams\": 1,\n",
    "    \"do_sample\": True,\n",
    "    \"temperature\": 0.7,\n",
    "    \"top_k\": 50,\n",
    "    \"top_p\": 0.9,\n",
    "}\n",
    "\n",
    "print(\"Recommended parameters for factual summaries:\")\n",
    "for k, v in best_params_factual.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "print(\"\\nRecommended parameters for creative summaries:\")\n",
    "for k, v in best_params_creative.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "- [Hugging Face Summarization Docs](https://huggingface.co/docs/transformers/main_classes/pipelines#transformers.SummarizationPipeline)\n",
    "- [BART Paper](https://arxiv.org/abs/1910.13461) - Denoising Sequence-to-Sequence Pre-training\n",
    "- [T5 Paper](https://arxiv.org/abs/1910.10683) - Text-to-Text Transfer Transformer\n",
    "- [Pegasus Paper](https://arxiv.org/abs/1912.08777) - Pre-training with Extracted Gap-sentences for Summarization\n",
    "- [Hallucination in Summarization](https://arxiv.org/abs/2005.00661) - Understanding and mitigating hallucinations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
