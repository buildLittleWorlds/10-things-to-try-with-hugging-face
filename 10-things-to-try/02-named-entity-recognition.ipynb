{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Named Entity Recognition (NER)\n",
    "\n",
    "**Estimated Time**: ~2 hours\n",
    "\n",
    "**Prerequisites**: Notebook 1 (Fill-Mask) - understanding of tokenization, pipelines, and confidence scores\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. **Define** named entity recognition and list common entity types (PER, ORG, LOC, MISC)\n",
    "2. **Use** the `ner` pipeline with `grouped_entities=True` to extract entities from text\n",
    "3. **Understand** the BIO tagging scheme (Beginning, Inside, Outside)\n",
    "4. **Handle** entity aggregation and overlapping entities\n",
    "5. **Evaluate** NER output quality on different text types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Run this cell first. If you completed Notebook 1, you already have the models cached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForTokenClassification\n",
    "import torch\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 1: Conceptual Foundation\n",
    "\n",
    "## What is Named Entity Recognition?\n",
    "\n",
    "**In plain English**: NER is the task of finding and categorizing \"important things\" in text - like people's names, company names, places, and dates.\n",
    "\n",
    "**Technical definition**: NER is a sequence labeling task where each token in a text is assigned a label indicating whether it's part of a named entity and what type of entity it is.\n",
    "\n",
    "### Visual Example\n",
    "\n",
    "```\n",
    "Input:  \"Elon Musk founded SpaceX in California.\"\n",
    "         ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ  ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ  ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ  ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "            PER       ORG      LOC      LOC\n",
    "         (Person)  (Org)   (Location) (Location)\n",
    "\n",
    "Output: [\n",
    "    {\"entity\": \"PER\", \"word\": \"Elon Musk\"},\n",
    "    {\"entity\": \"ORG\", \"word\": \"SpaceX\"},\n",
    "    {\"entity\": \"LOC\", \"word\": \"California\"}\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Entity Types\n",
    "\n",
    "Most NER models recognize these standard entity types:\n",
    "\n",
    "| Type | Full Name | Examples |\n",
    "|------|-----------|----------|\n",
    "| **PER** | Person | Elon Musk, Marie Curie, John Smith |\n",
    "| **ORG** | Organization | Google, United Nations, Harvard University |\n",
    "| **LOC** | Location | Paris, Mount Everest, Pacific Ocean |\n",
    "| **MISC** | Miscellaneous | English (language), Nobel Prize, COVID-19 |\n",
    "\n",
    "Some specialized models include additional types:\n",
    "- **DATE**: January 2024, next Monday\n",
    "- **TIME**: 3:00 PM, noon\n",
    "- **MONEY**: $500, 50 euros\n",
    "- **PERCENT**: 25%, half"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How NER Models Work: Token Classification\n",
    "\n",
    "Remember from Notebook 1 how we tokenized text? NER builds on that:\n",
    "\n",
    "```\n",
    "Text:    \"Elon Musk works at SpaceX\"\n",
    "Tokens:  [\"Elon\", \"Musk\", \"works\", \"at\", \"SpaceX\"]\n",
    "Labels:  [B-PER,  I-PER,   O,      O,    B-ORG  ]\n",
    "```\n",
    "\n",
    "Instead of predicting masked words (fill-mask), the model predicts a **label for each token**.\n",
    "\n",
    "### The BIO Tagging Scheme\n",
    "\n",
    "NER uses a special tagging scheme to handle multi-word entities:\n",
    "\n",
    "| Tag | Meaning | Example |\n",
    "|-----|---------|--------|\n",
    "| **B-XXX** | Beginning of entity type XXX | \"Elon\" ‚Üí B-PER |\n",
    "| **I-XXX** | Inside (continuation) of entity | \"Musk\" ‚Üí I-PER |\n",
    "| **O** | Outside any entity | \"works\", \"at\" ‚Üí O |\n",
    "\n",
    "This allows the model to handle:\n",
    "- Multi-word entities: \"New York City\" ‚Üí B-LOC, I-LOC, I-LOC\n",
    "- Adjacent entities: \"Google Microsoft\" ‚Üí B-ORG, B-ORG (two separate orgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connection to Notebook 1: Same Architecture, Different Task\n",
    "\n",
    "NER models use the **same BERT architecture** from Notebook 1, but with a different \"head\":\n",
    "\n",
    "```\n",
    "Fill-Mask (Notebook 1):         NER (This Notebook):\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ   BERT Encoder   ‚îÇ            ‚îÇ   BERT Encoder   ‚îÇ\n",
    "‚îÇ  (same weights)  ‚îÇ            ‚îÇ  (same weights)  ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "         ‚îÇ                               ‚îÇ\n",
    "         ‚ñº                               ‚ñº\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  MLM Head        ‚îÇ            ‚îÇ  Token Class Head‚îÇ\n",
    "‚îÇ  (predict word)  ‚îÇ            ‚îÇ  (predict label) ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "The pre-trained BERT knowledge transfers to help NER!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real-World Applications\n",
    "\n",
    "NER powers many practical applications:\n",
    "\n",
    "- **Information Extraction**: Pull structured data from unstructured text\n",
    "- **Search Engines**: Understand queries like \"restaurants near Eiffel Tower\"\n",
    "- **Customer Support**: Identify products, order numbers, and customer names\n",
    "- **News Analysis**: Track mentions of companies, politicians, locations\n",
    "- **Privacy/Redaction**: Find and mask PII (personally identifiable information)\n",
    "- **Knowledge Graphs**: Build connections between entities\n",
    "\n",
    "### Key Terminology\n",
    "\n",
    "| Term | Definition |\n",
    "|------|------------|\n",
    "| **Named Entity** | A real-world object with a proper name (person, place, organization) |\n",
    "| **Token Classification** | Assigning a label to each token in a sequence |\n",
    "| **BIO Tagging** | Begin-Inside-Outside scheme for marking entity boundaries |\n",
    "| **Entity Span** | The character positions where an entity starts and ends |\n",
    "| **Entity Aggregation** | Combining B-/I- tags into complete entity strings |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Your Understanding\n",
    "\n",
    "Before moving on, try to answer these questions (answers at the end):\n",
    "\n",
    "1. What does NER stand for?\n",
    "   - A) Neural Entity Recognition\n",
    "   - B) Named Entity Recognition\n",
    "   - C) Natural Entity Resolution\n",
    "\n",
    "2. In BIO tagging, what does the \"I\" prefix mean?\n",
    "   - A) Initial token of an entity\n",
    "   - B) Inside (continuation) of an entity\n",
    "   - C) Identifier for the entity\n",
    "\n",
    "3. Which entity type would \"Harvard University\" be?\n",
    "   - A) PER (Person)\n",
    "   - B) LOC (Location)\n",
    "   - C) ORG (Organization)\n",
    "\n",
    "4. How does NER relate to the BERT model from Notebook 1?\n",
    "   - A) They're completely different architectures\n",
    "   - B) NER uses BERT with a different output head\n",
    "   - C) NER doesn't use neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: Basic Implementation\n",
    "\n",
    "## Your First NER Pipeline\n",
    "\n",
    "Let's create an NER pipeline and extract entities from text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an NER pipeline\n",
    "# The default model is dbmdz/bert-large-cased-finetuned-conll03-english\n",
    "ner = pipeline(\"ner\", grouped_entities=True)\n",
    "\n",
    "# Extract entities from a sentence\n",
    "text = \"Elon Musk founded SpaceX in California and later acquired Twitter.\"\n",
    "entities = ner(text)\n",
    "\n",
    "print(f\"Text: '{text}'\\n\")\n",
    "print(\"Extracted entities:\")\n",
    "for entity in entities:\n",
    "    print(f\"  {entity['word']:20} ‚Üí {entity['entity_group']:5} ({entity['score']:.2%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Output\n",
    "\n",
    "Each entity in the result contains:\n",
    "\n",
    "- `word`: The extracted entity text\n",
    "- `entity_group`: The entity type (PER, ORG, LOC, MISC)\n",
    "- `score`: Confidence score (like in Notebook 1!)\n",
    "- `start`: Character position where entity begins\n",
    "- `end`: Character position where entity ends\n",
    "\n",
    "Let's examine an entity in detail:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the first entity in detail\n",
    "first_entity = entities[0]\n",
    "\n",
    "print(\"Detailed view of first entity:\")\n",
    "for key, value in first_entity.items():\n",
    "    if key == 'score':\n",
    "        print(f\"  {key:12}: {value:.4f} ({value:.2%})\")\n",
    "    else:\n",
    "        print(f\"  {key:12}: {value}\")\n",
    "\n",
    "# Verify the span is correct\n",
    "print(f\"\\nVerification: text[{first_entity['start']}:{first_entity['end']}] = '{text[first_entity['start']:first_entity['end']]}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Importance of `grouped_entities=True`\n",
    "\n",
    "Without this parameter, you get raw BIO tags - one label per token. Let's compare:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without grouped_entities\n",
    "ner_raw = pipeline(\"ner\", grouped_entities=False)\n",
    "\n",
    "text = \"Elon Musk works at SpaceX.\"\n",
    "raw_entities = ner_raw(text)\n",
    "\n",
    "print(\"WITHOUT grouped_entities (raw BIO tags):\")\n",
    "print(f\"Text: '{text}'\\n\")\n",
    "for ent in raw_entities:\n",
    "    print(f\"  '{ent['word']:12}' ‚Üí {ent['entity']:10} ({ent['score']:.2%})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# With grouped_entities\n",
    "grouped_entities = ner(text)\n",
    "\n",
    "print(\"WITH grouped_entities (aggregated):\")\n",
    "for ent in grouped_entities:\n",
    "    print(f\"  '{ent['word']:12}' ‚Üí {ent['entity_group']:5} ({ent['score']:.2%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice:\n",
    "- Raw output: \"Elon\" (B-PER) and \"Musk\" (I-PER) are separate\n",
    "- Grouped output: \"Elon Musk\" is combined into one PER entity\n",
    "\n",
    "For most applications, `grouped_entities=True` is what you want!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing Different Types of Text\n",
    "\n",
    "Let's see how NER performs on various text types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on different text types\n",
    "test_texts = [\n",
    "    # News headline\n",
    "    \"Apple CEO Tim Cook announced new iPhone features in Cupertino.\",\n",
    "    \n",
    "    # Historical text\n",
    "    \"In 1969, Neil Armstrong became the first person to walk on the Moon.\",\n",
    "    \n",
    "    # Sports news\n",
    "    \"The Los Angeles Lakers defeated the Boston Celtics at Madison Square Garden.\",\n",
    "    \n",
    "    # Scientific text\n",
    "    \"Dr. Jane Goodall studied chimpanzees in Gombe Stream National Park in Tanzania.\",\n",
    "]\n",
    "\n",
    "for text in test_texts:\n",
    "    entities = ner(text)\n",
    "    print(f\"TEXT: '{text}'\")\n",
    "    if entities:\n",
    "        for ent in entities:\n",
    "            print(f\"  ‚Üí {ent['word']:25} [{ent['entity_group']:4}] ({ent['score']:.0%})\")\n",
    "    else:\n",
    "        print(\"  ‚Üí No entities found\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 1: News Headline Entity Extraction (Guided)\n",
    "\n",
    "**Difficulty**: Basic | **Time**: 10-15 minutes\n",
    "\n",
    "**Your task**: Extract entities from news headlines and categorize them by type.\n",
    "\n",
    "### Step 1: Run NER on these headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample news headlines\n",
    "headlines = [\n",
    "    \"Microsoft acquires Activision Blizzard for $69 billion\",\n",
    "    \"President Biden visits Tokyo for G7 summit\",\n",
    "    \"Tesla opens new Gigafactory in Berlin, Germany\",\n",
    "    \"NASA's Perseverance rover discovers water on Mars\",\n",
    "    \"Amazon founder Jeff Bezos announces Blue Origin mission\"\n",
    "]\n",
    "\n",
    "# Process each headline\n",
    "all_entities = []\n",
    "for headline in headlines:\n",
    "    entities = ner(headline)\n",
    "    print(f\"Headline: '{headline}'\")\n",
    "    for ent in entities:\n",
    "        print(f\"  {ent['entity_group']:4}: {ent['word']}\")\n",
    "        all_entities.append(ent)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Count entities by type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count entities by type\n",
    "entity_counts = Counter(ent['entity_group'] for ent in all_entities)\n",
    "\n",
    "print(\"Entity type distribution:\")\n",
    "for entity_type, count in entity_counts.most_common():\n",
    "    bar = '‚ñà' * count\n",
    "    print(f\"  {entity_type:5}: {count:2} {bar}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Try your own headlines\n",
    "\n",
    "Add 3 of your own news headlines and run NER on them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Add your own headlines\n",
    "my_headlines = [\n",
    "    # Replace with your own headlines\n",
    "    \"Your headline 1 here\",\n",
    "    \"Your headline 2 here\",\n",
    "    \"Your headline 3 here\",\n",
    "]\n",
    "\n",
    "for headline in my_headlines:\n",
    "    entities = ner(headline)\n",
    "    print(f\"Headline: '{headline}'\")\n",
    "    for ent in entities:\n",
    "        print(f\"  {ent['entity_group']:4}: {ent['word']} ({ent['score']:.0%})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 3: Intermediate Exploration\n",
    "\n",
    "## Handling Edge Cases and Errors\n",
    "\n",
    "NER isn't perfect. Let's explore common issues:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edge case 1: Ambiguous names (same name, different entity types)\n",
    "ambiguous_cases = [\n",
    "    \"I love Apple products.\",                    # Apple = ORG (company)\n",
    "    \"I ate an apple for breakfast.\",             # apple = not an entity\n",
    "    \"Jordan is a great basketball player.\",      # Jordan = PER (Michael Jordan)\n",
    "    \"Jordan is a country in the Middle East.\",   # Jordan = LOC (country)\n",
    "]\n",
    "\n",
    "print(\"AMBIGUOUS NAMES:\")\n",
    "print(\"=\"*50)\n",
    "for text in ambiguous_cases:\n",
    "    entities = ner(text)\n",
    "    print(f\"'{text}'\")\n",
    "    if entities:\n",
    "        for ent in entities:\n",
    "            print(f\"  ‚Üí {ent['word']}: {ent['entity_group']} ({ent['score']:.0%})\")\n",
    "    else:\n",
    "        print(\"  ‚Üí No entities detected\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edge case 2: Multi-word entities that get split incorrectly\n",
    "complex_entities = [\n",
    "    \"The United States of America signed the treaty.\",\n",
    "    \"The University of California, Berkeley is renowned.\",\n",
    "    \"Dr. Martin Luther King Jr. gave a famous speech.\",\n",
    "    \"The New York Stock Exchange opened higher today.\",\n",
    "]\n",
    "\n",
    "print(\"COMPLEX MULTI-WORD ENTITIES:\")\n",
    "print(\"=\"*50)\n",
    "for text in complex_entities:\n",
    "    entities = ner(text)\n",
    "    print(f\"'{text}'\")\n",
    "    for ent in entities:\n",
    "        # Check if entity looks complete\n",
    "        print(f\"  ‚Üí {ent['word']:35} [{ent['entity_group']:4}] ({ent['score']:.0%})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confidence Score Analysis\n",
    "\n",
    "Like in Notebook 1, confidence scores tell us how certain the model is. Low confidence often indicates potential errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze confidence scores across a longer text\n",
    "long_text = \"\"\"\n",
    "Mark Zuckerberg, CEO of Meta, announced new AI features at their headquarters in Menlo Park.\n",
    "The company, formerly known as Facebook, is competing with Google, Microsoft, and OpenAI.\n",
    "Zuckerberg mentioned that Meta's Chief AI Scientist, Yann LeCun, has been instrumental in their research.\n",
    "\"\"\"\n",
    "\n",
    "entities = ner(long_text)\n",
    "\n",
    "print(\"Entity confidence analysis:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Sort by confidence\n",
    "for ent in sorted(entities, key=lambda x: x['score'], reverse=True):\n",
    "    confidence_bar = '‚ñà' * int(ent['score'] * 20)\n",
    "    confidence_indicator = '‚úì' if ent['score'] > 0.9 else ('?' if ent['score'] > 0.7 else '‚ö†')\n",
    "    print(f\"{confidence_indicator} {ent['word']:20} [{ent['entity_group']:4}] {ent['score']:5.1%} {confidence_bar}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Different NER Models\n",
    "\n",
    "Different models have different strengths. Let's compare a few:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a different model - DistilBERT-based NER (faster, slightly less accurate)\n",
    "print(\"Loading DistilBERT NER model...\")\n",
    "ner_distil = pipeline(\"ner\", \n",
    "                       model=\"elastic/distilbert-base-cased-finetuned-conll03-english\",\n",
    "                       grouped_entities=True)\n",
    "print(\"Model loaded!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare models on the same text\n",
    "comparison_text = \"Steve Jobs founded Apple in Cupertino with Steve Wozniak.\"\n",
    "\n",
    "print(f\"Text: '{comparison_text}'\\n\")\n",
    "\n",
    "print(\"Default Model (BERT-large):\")\n",
    "for ent in ner(comparison_text):\n",
    "    print(f\"  {ent['word']:20} ‚Üí {ent['entity_group']:4} ({ent['score']:.0%})\")\n",
    "\n",
    "print(\"\\nDistilBERT Model:\")\n",
    "for ent in ner_distil(comparison_text):\n",
    "    print(f\"  {ent['word']:20} ‚Üí {ent['entity_group']:4} ({ent['score']:.0%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 2: Handling Split Entities (Semi-guided)\n",
    "\n",
    "**Difficulty**: Intermediate | **Time**: 15-20 minutes\n",
    "\n",
    "**Your task**: Write a function that detects when entities might be incorrectly split and attempts to merge adjacent entities of the same type.\n",
    "\n",
    "**Hints**:\n",
    "1. Look at the `start` and `end` positions of entities\n",
    "2. Adjacent entities have `end` of one close to `start` of the next\n",
    "3. Consider only merging if they're the same entity type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "def merge_adjacent_entities(entities, text, max_gap=2):\n",
    "    \"\"\"\n",
    "    Merge adjacent entities of the same type.\n",
    "    \n",
    "    Args:\n",
    "        entities: List of entity dicts from NER pipeline\n",
    "        text: Original text\n",
    "        max_gap: Maximum character gap to consider entities adjacent\n",
    "    \n",
    "    Returns:\n",
    "        List of merged entities\n",
    "    \"\"\"\n",
    "    if not entities:\n",
    "        return entities\n",
    "    \n",
    "    # Sort entities by start position\n",
    "    sorted_entities = sorted(entities, key=lambda x: x['start'])\n",
    "    \n",
    "    merged = []\n",
    "    current = sorted_entities[0].copy()\n",
    "    \n",
    "    for next_ent in sorted_entities[1:]:\n",
    "        # Check if entities should be merged\n",
    "        gap = next_ent['start'] - current['end']\n",
    "        same_type = next_ent['entity_group'] == current['entity_group']\n",
    "        \n",
    "        if same_type and gap <= max_gap:\n",
    "            # Merge: extend current entity\n",
    "            current['end'] = next_ent['end']\n",
    "            current['word'] = text[current['start']:current['end']]\n",
    "            # Average the scores\n",
    "            current['score'] = (current['score'] + next_ent['score']) / 2\n",
    "        else:\n",
    "            # Don't merge: save current and start new\n",
    "            merged.append(current)\n",
    "            current = next_ent.copy()\n",
    "    \n",
    "    merged.append(current)\n",
    "    return merged\n",
    "\n",
    "\n",
    "# Test the function\n",
    "test_text = \"The New York Stock Exchange opened at 9:30 AM.\"\n",
    "original_entities = ner(test_text)\n",
    "\n",
    "print(f\"Text: '{test_text}'\\n\")\n",
    "\n",
    "print(\"Original entities:\")\n",
    "for ent in original_entities:\n",
    "    print(f\"  {ent['word']:25} [{ent['entity_group']}] (start: {ent['start']}, end: {ent['end']})\")\n",
    "\n",
    "print(\"\\nMerged entities:\")\n",
    "merged_entities = merge_adjacent_entities(original_entities, test_text)\n",
    "for ent in merged_entities:\n",
    "    print(f\"  {ent['word']:25} [{ent['entity_group']}] (start: {ent['start']}, end: {ent['end']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 4: Advanced Topics\n",
    "\n",
    "## Under the Hood: Token Classification\n",
    "\n",
    "Let's see what the NER pipeline does internally, building on what we learned in Notebook 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and tokenizer separately\n",
    "model_name = \"dbmdz/bert-large-cased-finetuned-conll03-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "\n",
    "print(f\"Model: {model_name}\")\n",
    "print(f\"Number of labels: {model.config.num_labels}\")\n",
    "print(f\"Labels: {model.config.id2label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step-by-step NER\n",
    "text = \"Elon Musk founded SpaceX.\"\n",
    "\n",
    "# STEP 1: Tokenization (same as Notebook 1!)\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", return_offsets_mapping=True)\n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "offsets = inputs['offset_mapping'][0].tolist()\n",
    "\n",
    "print(\"STEP 1 - Tokenization:\")\n",
    "print(f\"  Text: '{text}'\")\n",
    "print(f\"  Tokens: {tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2: Model inference\n",
    "# Remove offset_mapping before passing to model\n",
    "model_inputs = {k: v for k, v in inputs.items() if k != 'offset_mapping'}\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**model_inputs)\n",
    "\n",
    "# Get predictions\n",
    "predictions = torch.argmax(outputs.logits, dim=2)\n",
    "\n",
    "print(\"STEP 2 - Model Inference:\")\n",
    "print(f\"  Logits shape: {outputs.logits.shape}\")\n",
    "print(f\"  (batch_size, sequence_length, num_labels)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 3: Convert predictions to labels\n",
    "print(\"STEP 3 - Token Labels:\")\n",
    "print(f\"{'Token':<12} {'Label':<10} {'Offset'}\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "for token, pred_id, offset in zip(tokens, predictions[0], offsets):\n",
    "    label = model.config.id2label[pred_id.item()]\n",
    "    print(f\"{token:<12} {label:<10} {offset}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 4: Convert to entities with confidence scores\n",
    "print(\"\\nSTEP 4 - Convert to Entities with Confidence:\")\n",
    "\n",
    "# Get probabilities\n",
    "probs = torch.softmax(outputs.logits, dim=2)\n",
    "\n",
    "for token, pred_id, prob, offset in zip(tokens, predictions[0], probs[0], offsets):\n",
    "    label = model.config.id2label[pred_id.item()]\n",
    "    confidence = prob[pred_id].item()\n",
    "    \n",
    "    # Skip special tokens and O labels for clarity\n",
    "    if label != 'O' and offset != (0, 0):\n",
    "        print(f\"{token:<12} ‚Üí {label:<10} ({confidence:.2%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Considerations\n",
    "\n",
    "| Consideration | Recommendation |\n",
    "|---------------|----------------|\n",
    "| **Model size** | Use DistilBERT for speed (2x faster, slightly less accurate) |\n",
    "| **Batch processing** | Process multiple texts at once |\n",
    "| **Long texts** | Split into sentences or chunks (models have max token limits) |\n",
    "| **Post-processing** | Use `grouped_entities=True` and validate outputs |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch processing example\n",
    "texts = [\n",
    "    \"Google announced new AI features.\",\n",
    "    \"Microsoft CEO Satya Nadella spoke in Seattle.\",\n",
    "    \"The European Union passed new regulations.\"\n",
    "]\n",
    "\n",
    "# Process all at once (more efficient)\n",
    "batch_results = ner(texts)\n",
    "\n",
    "for text, entities in zip(texts, batch_results):\n",
    "    print(f\"'{text}'\")\n",
    "    for ent in entities:\n",
    "        print(f\"  ‚Üí {ent['word']}: {ent['entity_group']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitations of NER\n",
    "\n",
    "1. **Domain dependency**: Models trained on news may struggle with medical or legal text\n",
    "2. **New entities**: Models can't recognize entities that didn't exist during training\n",
    "3. **Context sensitivity**: Same name can be different entity types\n",
    "4. **Language support**: Most models are English-only; multilingual models exist but have lower accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limitation example: Domain-specific entities\n",
    "domain_texts = [\n",
    "    # Medical (might struggle)\n",
    "    \"The patient was prescribed Lisinopril for hypertension.\",\n",
    "    \n",
    "    # Legal (might struggle)\n",
    "    \"The defendant violated Section 230 of the Communications Decency Act.\",\n",
    "    \n",
    "    # News (should work well - training domain)\n",
    "    \"President Biden met with Chancellor Scholz in Berlin.\",\n",
    "]\n",
    "\n",
    "print(\"DOMAIN COMPARISON:\")\n",
    "print(\"=\"*50)\n",
    "for text in domain_texts:\n",
    "    entities = ner(text)\n",
    "    print(f\"'{text}'\")\n",
    "    if entities:\n",
    "        for ent in entities:\n",
    "            print(f\"  {ent['word']}: {ent['entity_group']} ({ent['score']:.0%})\")\n",
    "    else:\n",
    "        print(\"  No entities found\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 3: Entity Frequency Counter (Independent)\n",
    "\n",
    "**Difficulty**: Advanced | **Time**: 15-20 minutes\n",
    "\n",
    "**Your task**: Build a class that tracks entity frequencies across multiple texts.\n",
    "\n",
    "**Requirements**:\n",
    "1. Process multiple texts and track all entities\n",
    "2. Count how many times each unique entity appears\n",
    "3. Group by entity type\n",
    "4. Provide a summary report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "class EntityFrequencyTracker:\n",
    "    \"\"\"\n",
    "    Tracks entity frequencies across multiple texts.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the tracker.\"\"\"\n",
    "        # TODO: Initialize data structures\n",
    "        self.ner = pipeline(\"ner\", grouped_entities=True)\n",
    "        self.entity_counts = defaultdict(Counter)  # {entity_type: Counter({entity: count})}\n",
    "        self.total_texts = 0\n",
    "    \n",
    "    def process_text(self, text):\n",
    "        \"\"\"Process a single text and update counts.\"\"\"\n",
    "        # TODO: Extract entities and update counts\n",
    "        entities = self.ner(text)\n",
    "        self.total_texts += 1\n",
    "        \n",
    "        for ent in entities:\n",
    "            entity_type = ent['entity_group']\n",
    "            entity_text = ent['word'].strip()\n",
    "            self.entity_counts[entity_type][entity_text] += 1\n",
    "        \n",
    "        return entities\n",
    "    \n",
    "    def process_batch(self, texts):\n",
    "        \"\"\"Process multiple texts.\"\"\"\n",
    "        for text in texts:\n",
    "            self.process_text(text)\n",
    "    \n",
    "    def get_top_entities(self, entity_type=None, n=10):\n",
    "        \"\"\"\n",
    "        Get the most common entities.\n",
    "        \n",
    "        Args:\n",
    "            entity_type: Filter by type (PER, ORG, LOC, MISC) or None for all\n",
    "            n: Number of top entities to return\n",
    "        \"\"\"\n",
    "        # TODO: Return top entities\n",
    "        if entity_type:\n",
    "            return self.entity_counts[entity_type].most_common(n)\n",
    "        else:\n",
    "            # Combine all counts\n",
    "            all_counts = Counter()\n",
    "            for type_counts in self.entity_counts.values():\n",
    "                all_counts.update(type_counts)\n",
    "            return all_counts.most_common(n)\n",
    "    \n",
    "    def get_summary(self):\n",
    "        \"\"\"Return a summary report.\"\"\"\n",
    "        # TODO: Create summary\n",
    "        summary = []\n",
    "        summary.append(f\"Entity Frequency Report\")\n",
    "        summary.append(f\"=\" * 40)\n",
    "        summary.append(f\"Texts processed: {self.total_texts}\")\n",
    "        summary.append(f\"\")\n",
    "        \n",
    "        for entity_type in ['PER', 'ORG', 'LOC', 'MISC']:\n",
    "            if entity_type in self.entity_counts:\n",
    "                total = sum(self.entity_counts[entity_type].values())\n",
    "                unique = len(self.entity_counts[entity_type])\n",
    "                summary.append(f\"{entity_type}: {total} mentions ({unique} unique)\")\n",
    "                for entity, count in self.entity_counts[entity_type].most_common(3):\n",
    "                    summary.append(f\"  - {entity}: {count}\")\n",
    "                summary.append(\"\")\n",
    "        \n",
    "        return '\\n'.join(summary)\n",
    "\n",
    "\n",
    "# Test the tracker\n",
    "tracker = EntityFrequencyTracker()\n",
    "\n",
    "# Sample texts about tech companies\n",
    "sample_texts = [\n",
    "    \"Apple CEO Tim Cook spoke at the Apple Park headquarters in Cupertino.\",\n",
    "    \"Google and Microsoft are competing in the AI space.\",\n",
    "    \"Satya Nadella, CEO of Microsoft, announced new Azure features.\",\n",
    "    \"Tim Cook visited the new Apple Store in New York City.\",\n",
    "    \"Google's Sundar Pichai discussed AI ethics in San Francisco.\",\n",
    "    \"Microsoft acquired Activision Blizzard for gaming expansion.\",\n",
    "]\n",
    "\n",
    "tracker.process_batch(sample_texts)\n",
    "print(tracker.get_summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 5: Mini-Project\n",
    "\n",
    "## Project: News Article Entity Analyzer\n",
    "\n",
    "**Scenario**: You're building a news aggregation tool that automatically tags articles with key entities for better search and categorization.\n",
    "\n",
    "**Your goal**: Build a `NewsEntityAnalyzer` class that:\n",
    "1. Takes a news article text\n",
    "2. Extracts all entities and groups them by type\n",
    "3. Identifies the most mentioned entities (likely the main subjects)\n",
    "4. Creates a brief \"entity summary\" for the article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MINI-PROJECT: News Article Entity Analyzer\n",
    "# ==========================================\n",
    "\n",
    "class NewsEntityAnalyzer:\n",
    "    \"\"\"\n",
    "    Analyzes news articles to extract and summarize key entities.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the analyzer with NER pipeline.\"\"\"\n",
    "        self.ner = pipeline(\"ner\", grouped_entities=True)\n",
    "    \n",
    "    def analyze(self, article_text):\n",
    "        \"\"\"\n",
    "        Analyze an article and return structured entity information.\n",
    "        \n",
    "        Args:\n",
    "            article_text: The full article text\n",
    "            \n",
    "        Returns:\n",
    "            dict with entity analysis results\n",
    "        \"\"\"\n",
    "        # Extract entities\n",
    "        entities = self.ner(article_text)\n",
    "        \n",
    "        # Group by type\n",
    "        by_type = defaultdict(list)\n",
    "        for ent in entities:\n",
    "            by_type[ent['entity_group']].append({\n",
    "                'text': ent['word'],\n",
    "                'score': ent['score'],\n",
    "                'start': ent['start'],\n",
    "                'end': ent['end']\n",
    "            })\n",
    "        \n",
    "        # Count entity frequencies\n",
    "        entity_freq = Counter(ent['word'] for ent in entities)\n",
    "        \n",
    "        # Identify main subjects (entities mentioned more than once or with high confidence)\n",
    "        main_subjects = []\n",
    "        seen = set()\n",
    "        for ent in sorted(entities, key=lambda x: (-entity_freq[x['word']], -x['score'])):\n",
    "            if ent['word'] not in seen:\n",
    "                main_subjects.append({\n",
    "                    'entity': ent['word'],\n",
    "                    'type': ent['entity_group'],\n",
    "                    'mentions': entity_freq[ent['word']],\n",
    "                    'confidence': ent['score']\n",
    "                })\n",
    "                seen.add(ent['word'])\n",
    "        \n",
    "        return {\n",
    "            'total_entities': len(entities),\n",
    "            'unique_entities': len(set(e['word'] for e in entities)),\n",
    "            'by_type': dict(by_type),\n",
    "            'main_subjects': main_subjects[:5],  # Top 5\n",
    "            'entity_frequency': dict(entity_freq)\n",
    "        }\n",
    "    \n",
    "    def get_summary(self, article_text):\n",
    "        \"\"\"\n",
    "        Generate a human-readable entity summary for an article.\n",
    "        \"\"\"\n",
    "        analysis = self.analyze(article_text)\n",
    "        \n",
    "        lines = []\n",
    "        lines.append(\"üì∞ Article Entity Analysis\")\n",
    "        lines.append(\"=\" * 50)\n",
    "        lines.append(f\"Total entity mentions: {analysis['total_entities']}\")\n",
    "        lines.append(f\"Unique entities: {analysis['unique_entities']}\")\n",
    "        lines.append(\"\")\n",
    "        \n",
    "        # Main subjects\n",
    "        lines.append(\"üéØ Main Subjects:\")\n",
    "        for subj in analysis['main_subjects']:\n",
    "            mention_text = \"mention\" if subj['mentions'] == 1 else \"mentions\"\n",
    "            lines.append(f\"  ‚Ä¢ {subj['entity']} ({subj['type']}) - {subj['mentions']} {mention_text}\")\n",
    "        lines.append(\"\")\n",
    "        \n",
    "        # By type breakdown\n",
    "        lines.append(\"üìä Entities by Type:\")\n",
    "        type_icons = {'PER': 'üë§', 'ORG': 'üè¢', 'LOC': 'üìç', 'MISC': 'üè∑Ô∏è'}\n",
    "        for entity_type in ['PER', 'ORG', 'LOC', 'MISC']:\n",
    "            if entity_type in analysis['by_type']:\n",
    "                entities = analysis['by_type'][entity_type]\n",
    "                unique_names = list(set(e['text'] for e in entities))\n",
    "                icon = type_icons.get(entity_type, '‚Ä¢')\n",
    "                lines.append(f\"  {icon} {entity_type}: {', '.join(unique_names[:5])}\")\n",
    "                if len(unique_names) > 5:\n",
    "                    lines.append(f\"       ... and {len(unique_names) - 5} more\")\n",
    "        \n",
    "        return '\\n'.join(lines)\n",
    "    \n",
    "    def get_tags(self, article_text, max_tags=10):\n",
    "        \"\"\"\n",
    "        Generate tags for the article based on entities.\n",
    "        \n",
    "        Returns:\n",
    "            list of (tag, entity_type) tuples\n",
    "        \"\"\"\n",
    "        analysis = self.analyze(article_text)\n",
    "        \n",
    "        tags = []\n",
    "        for subj in analysis['main_subjects'][:max_tags]:\n",
    "            tags.append((subj['entity'], subj['type']))\n",
    "        \n",
    "        return tags\n",
    "\n",
    "\n",
    "# Create the analyzer\n",
    "analyzer = NewsEntityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a sample news article\n",
    "sample_article = \"\"\"\n",
    "Tech Giants Face New EU Regulations\n",
    "\n",
    "Brussels - The European Union announced sweeping new regulations targeting major \n",
    "technology companies on Tuesday. European Commission President Ursula von der Leyen \n",
    "unveiled the Digital Markets Act, which will impose strict rules on companies like \n",
    "Apple, Google, Amazon, and Meta.\n",
    "\n",
    "The regulations, developed in consultation with Margrethe Vestager, the EU's competition \n",
    "chief, aim to create a more level playing field in the digital economy. \"These companies \n",
    "have become gatekeepers,\" von der Leyen said at a press conference in Brussels.\n",
    "\n",
    "Tim Cook, Apple's CEO, expressed concerns about the new rules during a visit to Paris \n",
    "last week. Meanwhile, Sundar Pichai of Google and Mark Zuckerberg of Meta have indicated \n",
    "they are reviewing the regulations with their legal teams.\n",
    "\n",
    "The United States Trade Representative has called the regulations potentially \n",
    "discriminatory against American companies. However, EU officials maintain that the \n",
    "rules apply equally to all companies operating in Europe, including European firms \n",
    "like Spotify and SAP.\n",
    "\"\"\"\n",
    "\n",
    "print(analyzer.get_summary(sample_article))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate tags for the article\n",
    "tags = analyzer.get_tags(sample_article)\n",
    "\n",
    "print(\"\\nüè∑Ô∏è Suggested Article Tags:\")\n",
    "for tag, entity_type in tags:\n",
    "    print(f\"  #{tag.replace(' ', '')} ({entity_type})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try with your own article\n",
    "# Paste a news article here:\n",
    "your_article = \"\"\"\n",
    "Paste your own news article here for analysis.\n",
    "The analyzer will extract all entities and provide\n",
    "a structured summary.\n",
    "\"\"\"\n",
    "\n",
    "# Uncomment to analyze:\n",
    "# print(analyzer.get_summary(your_article))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extension Ideas\n",
    "\n",
    "If you want to extend this project further:\n",
    "\n",
    "1. **Entity linking**: Connect entities to Wikipedia or a knowledge base\n",
    "2. **Relationship extraction**: Find connections between entities (\"X works at Y\")\n",
    "3. **Sentiment per entity**: Determine if coverage of each entity is positive/negative\n",
    "4. **Timeline extraction**: Build a timeline from DATE entities\n",
    "5. **Cross-article tracking**: Track entities across multiple articles over time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 6: Wrap-Up\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Named Entity Recognition** extracts and categorizes real-world entities (people, organizations, locations) from text\n",
    "\n",
    "2. **BIO tagging** (Begin-Inside-Outside) is the scheme used to mark entity boundaries and handle multi-word entities\n",
    "\n",
    "3. **Same BERT architecture** from Notebook 1, but with a different output head for token classification\n",
    "\n",
    "4. **Use `grouped_entities=True`** to get aggregated entities instead of raw BIO tags\n",
    "\n",
    "5. **Confidence scores matter** - low confidence often indicates ambiguous or incorrect predictions\n",
    "\n",
    "## Common Mistakes to Avoid\n",
    "\n",
    "| Mistake | Why It's a Problem |\n",
    "|---------|-------------------|\n",
    "| Forgetting `grouped_entities=True` | You get raw B-/I- tags instead of complete entities |\n",
    "| Not handling split entities | Multi-word entities may be incorrectly separated |\n",
    "| Using news-trained models on other domains | Medical/legal text needs specialized models |\n",
    "| Ignoring confidence scores | Low-confidence predictions are often wrong |\n",
    "\n",
    "## What's Next?\n",
    "\n",
    "In **Notebook 3: Question Answering**, you'll learn:\n",
    "- How to find answers within a given context (extractive QA)\n",
    "- How models predict start and end positions of answer spans\n",
    "- This builds on NER - both tasks extract spans from text!\n",
    "\n",
    "The concepts of token classification and span extraction will directly apply!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Solutions\n",
    "\n",
    "### Check Your Understanding (Quiz Answers)\n",
    "\n",
    "1. **B) Named Entity Recognition**\n",
    "2. **B) Inside (continuation) of an entity**\n",
    "3. **C) ORG (Organization)** - Universities are organizations\n",
    "4. **B) NER uses BERT with a different output head**\n",
    "\n",
    "### Exercise 2: Merge Adjacent Entities (Sample Solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample solution for Exercise 2 is provided in the exercise itself\n",
    "# The merge_adjacent_entities function handles:\n",
    "# 1. Sorting entities by position\n",
    "# 2. Checking for small gaps between entities\n",
    "# 3. Only merging same entity types\n",
    "# 4. Averaging confidence scores\n",
    "\n",
    "# Test it with a challenging case:\n",
    "test_text = \"The University of California at Berkeley is in the Bay Area.\"\n",
    "entities = ner(test_text)\n",
    "\n",
    "print(f\"Text: '{test_text}'\")\n",
    "print(\"\\nOriginal entities:\")\n",
    "for ent in entities:\n",
    "    print(f\"  '{ent['word']}' [{ent['entity_group']}]\")\n",
    "\n",
    "print(\"\\nMerged entities:\")\n",
    "merged = merge_adjacent_entities(entities, test_text, max_gap=3)\n",
    "for ent in merged:\n",
    "    print(f\"  '{ent['word']}' [{ent['entity_group']}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "- [Hugging Face NER Pipeline Docs](https://huggingface.co/docs/transformers/main_classes/pipelines#transformers.TokenClassificationPipeline)\n",
    "- [CoNLL-2003 Dataset](https://huggingface.co/datasets/conll2003) - The dataset many NER models are trained on\n",
    "- [BERT for NER Paper](https://arxiv.org/abs/1810.04805) - Section on token classification\n",
    "- [OntoNotes NER](https://catalog.ldc.upenn.edu/LDC2013T19) - Dataset with 18 entity types"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
