{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# 8. Text Similarity with Embeddings\n",
    "\n",
    "**Estimated Time**: ~2 hours\n",
    "\n",
    "**Prerequisites**: Notebooks 1-7 (especially understanding of text representations from Translation)\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. **Understand** how models represent text as numerical vectors (embeddings)\n",
    "2. **Use** the feature-extraction pipeline to generate embeddings\n",
    "3. **Calculate** similarity between texts using cosine similarity\n",
    "4. **Compare** different pooling strategies (mean, CLS, max)\n",
    "5. **Build** a semantic FAQ matcher for real-world applications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Run this cell first. If you completed previous notebooks, you already have the core packages ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "from transformers import pipeline, AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Setup complete!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 1: Conceptual Foundation\n",
    "\n",
    "## What are Text Embeddings?\n",
    "\n",
    "**In plain English**: Embeddings are a way to represent text as numbers that capture meaning. Similar texts have similar numbers, letting us measure how alike two pieces of text are.\n",
    "\n",
    "**Technical definition**: Text embeddings are dense vector representations where semantically similar texts are mapped to nearby points in a high-dimensional space, enabling mathematical operations like similarity computation.\n",
    "\n",
    "### Why Do We Need Embeddings?\n",
    "\n",
    "```\n",
    "THE PROBLEM:\n",
    "┌────────────────────────────────────────────────────────────────┐\n",
    "│  Computers can't understand text directly.                    │\n",
    "│                                                                │\n",
    "│  \"The movie was great!\" → ???                                 │\n",
    "│  \"I loved the film!\"    → ???                                 │\n",
    "│                                                                │\n",
    "│  Are these similar? Computer has no idea!                     │\n",
    "└────────────────────────────────────────────────────────────────┘\n",
    "\n",
    "THE SOLUTION - EMBEDDINGS:\n",
    "┌────────────────────────────────────────────────────────────────┐\n",
    "│  Convert text to numbers that capture meaning:                │\n",
    "│                                                                │\n",
    "│  \"The movie was great!\" → [0.82, -0.15, 0.43, 0.91, ...]     │\n",
    "│  \"I loved the film!\"    → [0.79, -0.18, 0.45, 0.88, ...]     │\n",
    "│                                                                │\n",
    "│  Similar meanings → Similar numbers → We can compare!         │\n",
    "└────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "### How Embeddings Work\n",
    "\n",
    "```\n",
    "TEXT TO EMBEDDING PROCESS:\n",
    "\n",
    "Input: \"I love machine learning\"\n",
    "                │\n",
    "                ▼\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│                      TOKENIZER                              │\n",
    "│  Split into tokens: [\"I\", \"love\", \"machine\", \"learning\"]   │\n",
    "│  Convert to IDs:    [101, 1045, 2293, 3698, 4083, 102]     │\n",
    "│                     ([CLS]  I   love machine learning [SEP])│\n",
    "└─────────────────────────────────────────────────────────────┘\n",
    "                │\n",
    "                ▼\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│                    TRANSFORMER MODEL                        │\n",
    "│  Each token → Hidden state vector (768 dimensions)         │\n",
    "│                                                             │\n",
    "│  [CLS]    → [h₀₁, h₀₂, h₀₃, ..., h₀₇₆₈]                    │\n",
    "│  I        → [h₁₁, h₁₂, h₁₃, ..., h₁₇₆₈]                    │\n",
    "│  love     → [h₂₁, h₂₂, h₂₃, ..., h₂₇₆₈]                    │\n",
    "│  machine  → [h₃₁, h₃₂, h₃₃, ..., h₃₇₆₈]                    │\n",
    "│  learning → [h₄₁, h₄₂, h₄₃, ..., h₄₇₆₈]                    │\n",
    "│  [SEP]    → [h₅₁, h₅₂, h₅₃, ..., h₅₇₆₈]                    │\n",
    "└─────────────────────────────────────────────────────────────┘\n",
    "                │\n",
    "                ▼\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│                      POOLING                                │\n",
    "│  Combine token vectors into one sentence vector             │\n",
    "│                                                             │\n",
    "│  Methods:                                                   │\n",
    "│  • CLS pooling:  Use [CLS] token vector only               │\n",
    "│  • Mean pooling: Average all token vectors                  │\n",
    "│  • Max pooling:  Take maximum value per dimension           │\n",
    "└─────────────────────────────────────────────────────────────┘\n",
    "                │\n",
    "                ▼\n",
    "Output: [0.23, -0.45, 0.67, ..., 0.12]  (768-dim vector)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "### Visualizing the Embedding Space\n",
    "\n",
    "```\n",
    "2D SIMPLIFICATION OF EMBEDDING SPACE:\n",
    "(Real embeddings have 768+ dimensions!)\n",
    "\n",
    "        Positive Sentiment ↑\n",
    "                          │\n",
    "                    ★ \"Amazing movie!\"\n",
    "            ★ \"Great film!\"     ★ \"I loved it!\"\n",
    "                          │\n",
    "    ─────────────────────┼─────────────────────→ Entertainment\n",
    "                          │\n",
    "        ○ \"Okay, I guess\"│\n",
    "                          │\n",
    "            ◆ \"Terrible!\" │  ◆ \"Waste of time\"\n",
    "                    ◆ \"Hated it\"\n",
    "                          │\n",
    "        Negative Sentiment ↓\n",
    "\n",
    "SIMILAR TEXTS CLUSTER TOGETHER!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "### Connection to Previous Notebooks\n",
    "\n",
    "| Notebook | What It Does | Relation to Embeddings |\n",
    "|----------|--------------|------------------------|\n",
    "| 1 (Fill-Mask) | Predicts missing words | Uses internal embeddings |\n",
    "| 2 (NER) | Labels entities | Token-level embeddings |\n",
    "| 6 (Zero-Shot) | Classifies text | Compares text embeddings |\n",
    "| 7 (Translation) | Converts languages | Encoder creates meaning vectors |\n",
    "| **8 (This notebook)** | **Extracts embeddings directly** | **Full control over representations** |\n",
    "\n",
    "In previous notebooks, embeddings were used internally. Now we'll extract and use them directly!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "### Measuring Similarity: Cosine Similarity\n",
    "\n",
    "Once we have embeddings, we need a way to compare them:\n",
    "\n",
    "```\n",
    "COSINE SIMILARITY:\n",
    "\n",
    "Measures the angle between two vectors (ignores magnitude)\n",
    "\n",
    "                    A · B\n",
    "cos(θ) = ─────────────────────\n",
    "          ||A|| × ||B||\n",
    "\n",
    "Score interpretation:\n",
    "├── 1.0:   Identical direction (very similar)\n",
    "├── 0.7+:  High similarity\n",
    "├── 0.5:   Moderate similarity\n",
    "├── 0.3-:  Low similarity\n",
    "└── 0.0:   Orthogonal (unrelated)\n",
    "\n",
    "Example:\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│  \"I love dogs\"  ←─── cos = 0.92 ───→  \"Dogs are great\"    │\n",
    "│                                                             │\n",
    "│  \"I love dogs\"  ←─── cos = 0.45 ───→  \"The weather is nice\" │\n",
    "│                                                             │\n",
    "│  \"I love dogs\"  ←─── cos = 0.23 ───→  \"Python programming\" │\n",
    "└─────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "### Real-World Applications\n",
    "\n",
    "Embeddings power many applications:\n",
    "\n",
    "- **Semantic Search**: Find documents by meaning, not just keywords\n",
    "- **Duplicate Detection**: Find similar or duplicate content\n",
    "- **FAQ Matching**: Match user questions to known answers\n",
    "- **Recommendation Systems**: Suggest similar content\n",
    "- **Clustering**: Group similar documents automatically\n",
    "- **Retrieval-Augmented Generation (RAG)**: Find relevant context for LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "### Key Terminology\n",
    "\n",
    "| Term | Definition |\n",
    "|------|------------|\n",
    "| **Embedding** | Dense vector representation of text |\n",
    "| **Dimension** | Number of values in the vector (e.g., 768) |\n",
    "| **Pooling** | Combining token vectors into one sentence vector |\n",
    "| **Cosine similarity** | Measure of angle between vectors (0 to 1) |\n",
    "| **Semantic similarity** | How close in meaning two texts are |\n",
    "| **Vector space** | Mathematical space where embeddings live |\n",
    "| **Dense vector** | Vector with mostly non-zero values |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "### Check Your Understanding\n",
    "\n",
    "Before moving on, try to answer these questions (answers at the end):\n",
    "\n",
    "1. What is the purpose of text embeddings?\n",
    "   - A) To compress text files\n",
    "   - B) To represent text as numbers that capture meaning\n",
    "   - C) To encrypt text for security\n",
    "\n",
    "2. What does cosine similarity measure?\n",
    "   - A) The length of two vectors\n",
    "   - B) The angle between two vectors\n",
    "   - C) The sum of two vectors\n",
    "\n",
    "3. If two texts have cosine similarity of 0.95, what does that mean?\n",
    "   - A) They are very different\n",
    "   - B) They are very similar\n",
    "   - C) They are exactly the same\n",
    "\n",
    "4. What is pooling used for?\n",
    "   - A) Combining multiple token vectors into one sentence vector\n",
    "   - B) Removing water from the model\n",
    "   - C) Training the model faster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: Basic Implementation\n",
    "\n",
    "## Your First Embeddings\n",
    "\n",
    "Let's generate embeddings using the feature-extraction pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a feature-extraction pipeline\n",
    "# This extracts the raw embeddings from a model\n",
    "extractor = pipeline(\"feature-extraction\", model=\"distilbert-base-uncased\")\n",
    "\n",
    "# Generate embeddings for a sentence\n",
    "text = \"Machine learning is fascinating.\"\n",
    "embeddings = extractor(text)\n",
    "\n",
    "print(\"Feature Extraction Result:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Input: \\\"{text}\\\"\")\n",
    "print(f\"\\nOutput type: {type(embeddings)}\")\n",
    "print(f\"Output shape: {len(embeddings)} x {len(embeddings[0])} x {len(embeddings[0][0])}\")\n",
    "print(f\"\\nExplanation:\")\n",
    "print(f\"  - 1 batch (1 input text)\")\n",
    "print(f\"  - {len(embeddings[0])} tokens (including [CLS] and [SEP])\")\n",
    "print(f\"  - {len(embeddings[0][0])} dimensions per token\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "### Understanding the Output Shape\n",
    "\n",
    "The pipeline returns:\n",
    "- A nested list: `[batch][tokens][dimensions]`\n",
    "- Each token has its own 768-dimensional vector\n",
    "- We need to combine these into one sentence vector (pooling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to numpy for easier manipulation\n",
    "embeddings_array = np.array(embeddings)\n",
    "\n",
    "print(\"Embeddings as NumPy Array:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Shape: {embeddings_array.shape}\")\n",
    "print(f\"  → (batch_size, num_tokens, hidden_size)\")\n",
    "\n",
    "# Look at the first few values of the first token\n",
    "print(f\"\\nFirst 10 values of [CLS] token:\")\n",
    "print(embeddings_array[0, 0, :10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "### Implementing Pooling Strategies\n",
    "\n",
    "We need to combine token embeddings into a single sentence embedding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text, extractor, pooling='mean'):\n",
    "    \"\"\"\n",
    "    Generate a sentence embedding with specified pooling.\n",
    "    \n",
    "    Args:\n",
    "        text: Input text\n",
    "        extractor: Feature extraction pipeline\n",
    "        pooling: 'mean', 'cls', or 'max'\n",
    "        \n",
    "    Returns:\n",
    "        numpy array of shape (hidden_size,)\n",
    "    \"\"\"\n",
    "    # Get raw embeddings\n",
    "    embeddings = extractor(text)\n",
    "    embeddings = np.array(embeddings)[0]  # Remove batch dimension\n",
    "    \n",
    "    if pooling == 'cls':\n",
    "        # Use only the [CLS] token (first token)\n",
    "        return embeddings[0]\n",
    "    elif pooling == 'mean':\n",
    "        # Average all token embeddings\n",
    "        return np.mean(embeddings, axis=0)\n",
    "    elif pooling == 'max':\n",
    "        # Take maximum value across tokens for each dimension\n",
    "        return np.max(embeddings, axis=0)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown pooling: {pooling}\")\n",
    "\n",
    "\n",
    "# Test different pooling methods\n",
    "text = \"I love learning about artificial intelligence.\"\n",
    "\n",
    "cls_embedding = get_embedding(text, extractor, pooling='cls')\n",
    "mean_embedding = get_embedding(text, extractor, pooling='mean')\n",
    "max_embedding = get_embedding(text, extractor, pooling='max')\n",
    "\n",
    "print(\"Pooling Comparison:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Text: \\\"{text}\\\"\\n\")\n",
    "\n",
    "for name, emb in [('CLS', cls_embedding), ('Mean', mean_embedding), ('Max', max_embedding)]:\n",
    "    print(f\"{name} pooling:\")\n",
    "    print(f\"  Shape: {emb.shape}\")\n",
    "    print(f\"  First 5 values: {emb[:5].round(3)}\")\n",
    "    print(f\"  Min: {emb.min():.3f}, Max: {emb.max():.3f}, Mean: {emb.mean():.3f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "### Implementing Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    \"\"\"\n",
    "    Calculate cosine similarity between two vectors.\n",
    "    \n",
    "    Returns:\n",
    "        float between -1 and 1 (usually 0 to 1 for embeddings)\n",
    "    \"\"\"\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm1 = np.linalg.norm(vec1)\n",
    "    norm2 = np.linalg.norm(vec2)\n",
    "    \n",
    "    return dot_product / (norm1 * norm2)\n",
    "\n",
    "\n",
    "# Test with two similar sentences\n",
    "text1 = \"I love dogs.\"\n",
    "text2 = \"Dogs are wonderful pets.\"\n",
    "text3 = \"The weather is nice today.\"\n",
    "\n",
    "emb1 = get_embedding(text1, extractor, pooling='mean')\n",
    "emb2 = get_embedding(text2, extractor, pooling='mean')\n",
    "emb3 = get_embedding(text3, extractor, pooling='mean')\n",
    "\n",
    "print(\"Cosine Similarity Comparison:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Text 1: \\\"{text1}\\\"\")\n",
    "print(f\"Text 2: \\\"{text2}\\\"\")\n",
    "print(f\"Text 3: \\\"{text3}\\\"\")\n",
    "print()\n",
    "print(f\"Similarity (1 vs 2): {cosine_similarity(emb1, emb2):.4f}  ← Both about dogs!\")\n",
    "print(f\"Similarity (1 vs 3): {cosine_similarity(emb1, emb3):.4f}  ← Different topics\")\n",
    "print(f\"Similarity (2 vs 3): {cosine_similarity(emb2, emb3):.4f}  ← Different topics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "### Finding the Most Similar Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_most_similar(query, candidates, extractor, pooling='mean'):\n",
    "    \"\"\"\n",
    "    Find the most similar text from a list of candidates.\n",
    "    \n",
    "    Args:\n",
    "        query: The query text\n",
    "        candidates: List of candidate texts to compare against\n",
    "        extractor: Feature extraction pipeline\n",
    "        pooling: Pooling strategy\n",
    "        \n",
    "    Returns:\n",
    "        List of (candidate, similarity) tuples, sorted by similarity\n",
    "    \"\"\"\n",
    "    # Get query embedding\n",
    "    query_emb = get_embedding(query, extractor, pooling)\n",
    "    \n",
    "    # Calculate similarity with each candidate\n",
    "    results = []\n",
    "    for candidate in candidates:\n",
    "        cand_emb = get_embedding(candidate, extractor, pooling)\n",
    "        similarity = cosine_similarity(query_emb, cand_emb)\n",
    "        results.append((candidate, similarity))\n",
    "    \n",
    "    # Sort by similarity (highest first)\n",
    "    results.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# Test: Find the best match for a query\n",
    "query = \"How do I reset my password?\"\n",
    "\n",
    "candidates = [\n",
    "    \"Click 'Forgot Password' on the login page.\",\n",
    "    \"Our store is open 9 AM to 5 PM.\",\n",
    "    \"You can change your password in account settings.\",\n",
    "    \"Contact support for billing questions.\",\n",
    "    \"Password recovery is available via email.\",\n",
    "]\n",
    "\n",
    "results = find_most_similar(query, candidates, extractor)\n",
    "\n",
    "print(f\"Query: \\\"{query}\\\"\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nResults (ranked by similarity):\")\n",
    "for i, (candidate, score) in enumerate(results, 1):\n",
    "    bar = '*' * int(score * 30)\n",
    "    print(f\"\\n{i}. [{score:.3f}] {bar}\")\n",
    "    print(f\"   {candidate}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 1: Similarity Calculator (Guided)\n",
    "\n",
    "**Difficulty**: Basic | **Time**: 10-15 minutes\n",
    "\n",
    "**Your task**: Build a function that compares multiple text pairs and visualizes their similarity.\n",
    "\n",
    "### Step 1: Create a batch similarity calculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_pairwise_similarities(texts, extractor, pooling='mean'):\n",
    "    \"\"\"\n",
    "    Calculate similarity between all pairs of texts.\n",
    "    \n",
    "    Args:\n",
    "        texts: List of texts\n",
    "        extractor: Feature extraction pipeline\n",
    "        pooling: Pooling strategy\n",
    "        \n",
    "    Returns:\n",
    "        numpy array of shape (n_texts, n_texts) with similarities\n",
    "    \"\"\"\n",
    "    # Get embeddings for all texts\n",
    "    embeddings = [get_embedding(text, extractor, pooling) for text in texts]\n",
    "    \n",
    "    # Calculate pairwise similarities\n",
    "    n = len(texts)\n",
    "    similarity_matrix = np.zeros((n, n))\n",
    "    \n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            similarity_matrix[i, j] = cosine_similarity(embeddings[i], embeddings[j])\n",
    "    \n",
    "    return similarity_matrix\n",
    "\n",
    "\n",
    "# Test texts\n",
    "test_texts = [\n",
    "    \"I love programming in Python.\",\n",
    "    \"Python is my favorite programming language.\",\n",
    "    \"The weather is beautiful today.\",\n",
    "    \"It's a sunny and warm day outside.\",\n",
    "    \"Machine learning is transforming technology.\",\n",
    "]\n",
    "\n",
    "# Calculate similarities\n",
    "sim_matrix = calculate_pairwise_similarities(test_texts, extractor)\n",
    "\n",
    "print(\"Pairwise Similarity Matrix:\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "### Step 2: Create a visual display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_similarity_matrix(texts, sim_matrix):\n",
    "    \"\"\"\n",
    "    Display similarity matrix in a readable format.\n",
    "    \"\"\"\n",
    "    n = len(texts)\n",
    "    \n",
    "    # Print abbreviated texts\n",
    "    print(\"Texts:\")\n",
    "    for i, text in enumerate(texts):\n",
    "        abbrev = text[:40] + '...' if len(text) > 40 else text\n",
    "        print(f\"  [{i}] {abbrev}\")\n",
    "    \n",
    "    print(\"\\nSimilarity Matrix (rows vs columns):\")\n",
    "    print()\n",
    "    \n",
    "    # Header row\n",
    "    header = \"     \" + \"  \".join(f\"[{i}]\" for i in range(n))\n",
    "    print(header)\n",
    "    print(\"     \" + \"─\" * (len(header) - 5))\n",
    "    \n",
    "    # Data rows\n",
    "    for i in range(n):\n",
    "        row = f\"[{i}] │ \"\n",
    "        for j in range(n):\n",
    "            score = sim_matrix[i, j]\n",
    "            # Color coding using text\n",
    "            if i == j:\n",
    "                row += f\"1.00 \"\n",
    "            elif score > 0.8:\n",
    "                row += f\"{score:.2f}★\"\n",
    "            elif score > 0.6:\n",
    "                row += f\"{score:.2f}+\"\n",
    "            else:\n",
    "                row += f\"{score:.2f} \"\n",
    "        print(row)\n",
    "    \n",
    "    print(\"\\n★ = High similarity (>0.8)  + = Moderate (>0.6)\")\n",
    "\n",
    "\n",
    "display_similarity_matrix(test_texts, sim_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": [
    "### Step 3: Try your own text comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Write your own texts to compare\n",
    "\n",
    "my_texts = [\n",
    "    \"Your first text here\",\n",
    "    \"Your second text here\",\n",
    "    \"Your third text here\",\n",
    "]\n",
    "\n",
    "# Uncomment to run:\n",
    "# my_sim_matrix = calculate_pairwise_similarities(my_texts, extractor)\n",
    "# display_similarity_matrix(my_texts, my_sim_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-27",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 3: Intermediate Exploration\n",
    "\n",
    "## Comparing Pooling Strategies\n",
    "\n",
    "Different pooling strategies can affect similarity results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare pooling strategies\n",
    "text_pairs = [\n",
    "    (\"The cat sat on the mat.\", \"A cat is sitting on a rug.\"),  # Similar\n",
    "    (\"I love pizza.\", \"Pizza is my favorite food.\"),  # Similar\n",
    "    (\"The sky is blue.\", \"Programming in Python.\"),  # Different\n",
    "    (\"Machine learning is exciting.\", \"AI technology advances.\"),  # Related\n",
    "]\n",
    "\n",
    "print(\"Pooling Strategy Comparison:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for text1, text2 in text_pairs:\n",
    "    print(f\"\\nText 1: \\\"{text1}\\\"\")\n",
    "    print(f\"Text 2: \\\"{text2}\\\"\")\n",
    "    print()\n",
    "    \n",
    "    for pooling in ['cls', 'mean', 'max']:\n",
    "        emb1 = get_embedding(text1, extractor, pooling)\n",
    "        emb2 = get_embedding(text2, extractor, pooling)\n",
    "        sim = cosine_similarity(emb1, emb2)\n",
    "        bar = '*' * int(sim * 20)\n",
    "        print(f\"  {pooling.upper():5s}: {sim:.4f} {bar}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-29",
   "metadata": {},
   "source": [
    "### Pooling Strategy Guide\n",
    "\n",
    "| Strategy | How It Works | Best For |\n",
    "|----------|--------------|----------|\n",
    "| **CLS** | Uses [CLS] token only | Classification tasks |\n",
    "| **Mean** | Averages all tokens | General similarity |\n",
    "| **Max** | Maximum per dimension | Capturing key features |\n",
    "\n",
    "**Mean pooling** is typically the best default choice for semantic similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-30",
   "metadata": {},
   "source": [
    "## Using Sentence Transformers (Better for Similarity)\n",
    "\n",
    "For production-quality embeddings, sentence-transformers models are specifically trained for semantic similarity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a model trained specifically for similarity\n",
    "# all-MiniLM-L6-v2 is fast and effective for semantic similarity\n",
    "print(\"Loading sentence-transformers model...\")\n",
    "similarity_extractor = pipeline(\n",
    "    \"feature-extraction\", \n",
    "    model=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n",
    "print(\"Model loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare base model vs sentence-transformers\n",
    "test_pairs = [\n",
    "    (\"A man is eating food.\", \"A person is having a meal.\"),\n",
    "    (\"The dog runs fast.\", \"A canine is sprinting quickly.\"),\n",
    "    (\"I love programming.\", \"The sky is cloudy.\"),\n",
    "]\n",
    "\n",
    "print(\"Comparison: Base Model vs Sentence-Transformers\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for text1, text2 in test_pairs:\n",
    "    # Base model (DistilBERT)\n",
    "    emb1_base = get_embedding(text1, extractor, 'mean')\n",
    "    emb2_base = get_embedding(text2, extractor, 'mean')\n",
    "    sim_base = cosine_similarity(emb1_base, emb2_base)\n",
    "    \n",
    "    # Sentence-transformers\n",
    "    emb1_st = get_embedding(text1, similarity_extractor, 'mean')\n",
    "    emb2_st = get_embedding(text2, similarity_extractor, 'mean')\n",
    "    sim_st = cosine_similarity(emb1_st, emb2_st)\n",
    "    \n",
    "    print(f\"\\n\\\"{text1}\\\"\")\n",
    "    print(f\"\\\"{text2}\\\"\")\n",
    "    print(f\"  Base DistilBERT:      {sim_base:.4f}\")\n",
    "    print(f\"  Sentence-Transformers: {sim_st:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-33",
   "metadata": {},
   "source": [
    "### Understanding the Difference\n",
    "\n",
    "| Model Type | Training | Embedding Quality |\n",
    "|------------|----------|-------------------|\n",
    "| Base BERT/DistilBERT | General language modeling | Okay for similarity |\n",
    "| Sentence-Transformers | Trained on similarity pairs | Great for similarity |\n",
    "\n",
    "Sentence-transformer models are trained using contrastive learning to make similar texts closer and dissimilar texts farther apart."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-34",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 2: Duplicate Detector (Semi-guided)\n",
    "\n",
    "**Difficulty**: Intermediate | **Time**: 15-20 minutes\n",
    "\n",
    "**Your task**: Build a function that finds potential duplicate or near-duplicate content.\n",
    "\n",
    "**Hints**:\n",
    "- Compare each text with every other text\n",
    "- Flag pairs above a similarity threshold\n",
    "- Avoid comparing a text with itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "def find_duplicates(texts, extractor, threshold=0.85, pooling='mean'):\n",
    "    \"\"\"\n",
    "    Find potential duplicate or near-duplicate texts.\n",
    "    \n",
    "    Args:\n",
    "        texts: List of texts to check\n",
    "        extractor: Feature extraction pipeline\n",
    "        threshold: Similarity threshold to flag as duplicate\n",
    "        pooling: Pooling strategy\n",
    "        \n",
    "    Returns:\n",
    "        List of (idx1, idx2, similarity) tuples for potential duplicates\n",
    "    \"\"\"\n",
    "    # Get all embeddings\n",
    "    embeddings = [get_embedding(text, extractor, pooling) for text in texts]\n",
    "    \n",
    "    duplicates = []\n",
    "    \n",
    "    # Compare each pair (avoiding self-comparison and duplicates)\n",
    "    for i in range(len(texts)):\n",
    "        for j in range(i + 1, len(texts)):  # Start from i+1 to avoid duplicates\n",
    "            sim = cosine_similarity(embeddings[i], embeddings[j])\n",
    "            if sim >= threshold:\n",
    "                duplicates.append((i, j, sim))\n",
    "    \n",
    "    # Sort by similarity (highest first)\n",
    "    duplicates.sort(key=lambda x: x[2], reverse=True)\n",
    "    \n",
    "    return duplicates\n",
    "\n",
    "\n",
    "def format_duplicate_report(texts, duplicates):\n",
    "    \"\"\"\n",
    "    Format duplicate findings as a readable report.\n",
    "    \"\"\"\n",
    "    if not duplicates:\n",
    "        return \"No potential duplicates found above threshold.\"\n",
    "    \n",
    "    lines = []\n",
    "    lines.append(f\"Found {len(duplicates)} potential duplicate pair(s):\")\n",
    "    lines.append(\"=\"*60)\n",
    "    \n",
    "    for idx1, idx2, sim in duplicates:\n",
    "        lines.append(f\"\\nSimilarity: {sim:.1%}\")\n",
    "        lines.append(f\"  [{idx1}] {texts[idx1][:60]}...\" if len(texts[idx1]) > 60 else f\"  [{idx1}] {texts[idx1]}\")\n",
    "        lines.append(f\"  [{idx2}] {texts[idx2][:60]}...\" if len(texts[idx2]) > 60 else f\"  [{idx2}] {texts[idx2]}\")\n",
    "    \n",
    "    return '\\n'.join(lines)\n",
    "\n",
    "\n",
    "# Test with sample documents\n",
    "documents = [\n",
    "    \"How do I reset my password for my account?\",\n",
    "    \"What are your business hours?\",\n",
    "    \"I forgot my password, how can I reset it?\",\n",
    "    \"When is your store open?\",\n",
    "    \"Can you help me recover my account password?\",\n",
    "    \"What time do you close?\",\n",
    "    \"I need help with a refund.\",\n",
    "    \"How do I return a product for a refund?\",\n",
    "]\n",
    "\n",
    "print(\"Duplicate Detection Report:\")\n",
    "print(\"Threshold: 0.75\\n\")\n",
    "\n",
    "duplicates = find_duplicates(documents, similarity_extractor, threshold=0.75)\n",
    "print(format_duplicate_report(documents, duplicates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group potential duplicates together\n",
    "def group_duplicates(texts, duplicates):\n",
    "    \"\"\"\n",
    "    Group texts that are duplicates of each other.\n",
    "    \"\"\"\n",
    "    from collections import defaultdict\n",
    "    \n",
    "    # Union-find like grouping\n",
    "    groups = {i: i for i in range(len(texts))}\n",
    "    \n",
    "    def find(x):\n",
    "        if groups[x] != x:\n",
    "            groups[x] = find(groups[x])\n",
    "        return groups[x]\n",
    "    \n",
    "    def union(x, y):\n",
    "        px, py = find(x), find(y)\n",
    "        if px != py:\n",
    "            groups[px] = py\n",
    "    \n",
    "    for idx1, idx2, _ in duplicates:\n",
    "        union(idx1, idx2)\n",
    "    \n",
    "    # Collect groups\n",
    "    group_members = defaultdict(list)\n",
    "    for i in range(len(texts)):\n",
    "        group_members[find(i)].append(i)\n",
    "    \n",
    "    # Filter to groups with more than one member\n",
    "    duplicate_groups = [members for members in group_members.values() if len(members) > 1]\n",
    "    \n",
    "    return duplicate_groups\n",
    "\n",
    "\n",
    "# Show grouped duplicates\n",
    "groups = group_duplicates(documents, duplicates)\n",
    "\n",
    "print(\"\\nDuplicate Groups:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i, group in enumerate(groups, 1):\n",
    "    print(f\"\\nGroup {i}:\")\n",
    "    for idx in group:\n",
    "        print(f\"  [{idx}] {documents[idx]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-37",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 4: Advanced Topics\n",
    "\n",
    "## Batch Processing for Efficiency\n",
    "\n",
    "Processing many texts individually is slow. Let's batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings_batch(texts, extractor, pooling='mean'):\n",
    "    \"\"\"\n",
    "    Get embeddings for multiple texts efficiently.\n",
    "    \n",
    "    Note: The pipeline already handles batching internally,\n",
    "    but we can process multiple texts in one call.\n",
    "    \"\"\"\n",
    "    # Get all embeddings at once\n",
    "    all_embeddings = extractor(texts)\n",
    "    \n",
    "    # Apply pooling to each\n",
    "    result = []\n",
    "    for emb in all_embeddings:\n",
    "        emb_array = np.array(emb)\n",
    "        if pooling == 'cls':\n",
    "            result.append(emb_array[0])\n",
    "        elif pooling == 'mean':\n",
    "            result.append(np.mean(emb_array, axis=0))\n",
    "        elif pooling == 'max':\n",
    "            result.append(np.max(emb_array, axis=0))\n",
    "    \n",
    "    return np.array(result)\n",
    "\n",
    "\n",
    "# Time comparison (simplified)\n",
    "import time\n",
    "\n",
    "sample_texts = [\n",
    "    \"This is the first sample text.\",\n",
    "    \"Here is another sample for testing.\",\n",
    "    \"A third text to process.\",\n",
    "    \"Fourth sample text here.\",\n",
    "    \"And finally the fifth sample.\",\n",
    "]\n",
    "\n",
    "# Individual processing\n",
    "start = time.time()\n",
    "individual_results = [get_embedding(t, similarity_extractor, 'mean') for t in sample_texts]\n",
    "individual_time = time.time() - start\n",
    "\n",
    "# Batch processing\n",
    "start = time.time()\n",
    "batch_results = get_embeddings_batch(sample_texts, similarity_extractor, 'mean')\n",
    "batch_time = time.time() - start\n",
    "\n",
    "print(\"Processing Time Comparison:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Individual processing: {individual_time:.3f}s\")\n",
    "print(f\"Batch processing:      {batch_time:.3f}s\")\n",
    "print(f\"Speedup:               {individual_time/batch_time:.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-39",
   "metadata": {},
   "source": [
    "## Similarity Search at Scale\n",
    "\n",
    "For large collections, pre-compute embeddings and use efficient search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-40",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingIndex:\n",
    "    \"\"\"\n",
    "    A simple embedding index for similarity search.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, extractor, pooling='mean'):\n",
    "        self.extractor = extractor\n",
    "        self.pooling = pooling\n",
    "        self.documents = []\n",
    "        self.embeddings = None\n",
    "    \n",
    "    def add_documents(self, documents):\n",
    "        \"\"\"\n",
    "        Add documents to the index.\n",
    "        \"\"\"\n",
    "        self.documents.extend(documents)\n",
    "        \n",
    "        # Compute embeddings\n",
    "        new_embeddings = get_embeddings_batch(documents, self.extractor, self.pooling)\n",
    "        \n",
    "        if self.embeddings is None:\n",
    "            self.embeddings = new_embeddings\n",
    "        else:\n",
    "            self.embeddings = np.vstack([self.embeddings, new_embeddings])\n",
    "    \n",
    "    def search(self, query, top_k=5):\n",
    "        \"\"\"\n",
    "        Search for most similar documents.\n",
    "        \"\"\"\n",
    "        # Get query embedding\n",
    "        query_emb = get_embedding(query, self.extractor, self.pooling)\n",
    "        \n",
    "        # Calculate similarities with all documents\n",
    "        similarities = []\n",
    "        for i, doc_emb in enumerate(self.embeddings):\n",
    "            sim = cosine_similarity(query_emb, doc_emb)\n",
    "            similarities.append((i, self.documents[i], sim))\n",
    "        \n",
    "        # Sort by similarity\n",
    "        similarities.sort(key=lambda x: x[2], reverse=True)\n",
    "        \n",
    "        return similarities[:top_k]\n",
    "\n",
    "\n",
    "# Create index with sample FAQs\n",
    "faqs = [\n",
    "    \"How do I reset my password?\",\n",
    "    \"What payment methods do you accept?\",\n",
    "    \"How long does shipping take?\",\n",
    "    \"What is your return policy?\",\n",
    "    \"How do I track my order?\",\n",
    "    \"Do you offer international shipping?\",\n",
    "    \"How do I contact customer support?\",\n",
    "    \"What are your business hours?\",\n",
    "    \"How do I cancel my subscription?\",\n",
    "    \"Is there a mobile app available?\",\n",
    "]\n",
    "\n",
    "# Build index\n",
    "print(\"Building FAQ index...\")\n",
    "index = EmbeddingIndex(similarity_extractor)\n",
    "index.add_documents(faqs)\n",
    "print(f\"Index contains {len(index.documents)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test queries\n",
    "queries = [\n",
    "    \"I forgot my password\",\n",
    "    \"Where's my package?\",\n",
    "    \"Can I pay with credit card?\",\n",
    "    \"I want to return something\",\n",
    "]\n",
    "\n",
    "print(\"FAQ Search Results:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for query in queries:\n",
    "    print(f\"\\nQuery: \\\"{query}\\\"\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    results = index.search(query, top_k=3)\n",
    "    for rank, (idx, doc, score) in enumerate(results, 1):\n",
    "        print(f\"  {rank}. [{score:.3f}] {doc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-42",
   "metadata": {},
   "source": [
    "### Limitations and Considerations\n",
    "\n",
    "| Aspect | Consideration |\n",
    "|--------|---------------|\n",
    "| **Scale** | For millions of documents, use vector databases (Pinecone, Weaviate, etc.) |\n",
    "| **Freshness** | Embeddings must be recomputed when documents change |\n",
    "| **Domain** | General models may not work well for specialized domains |\n",
    "| **Length** | Very long texts may lose information - consider chunking |\n",
    "| **Languages** | Use multilingual models for non-English text |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-43",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 3: Clustering Similar Content (Independent)\n",
    "\n",
    "**Difficulty**: Advanced | **Time**: 15-20 minutes\n",
    "\n",
    "**Your task**: Build a simple clustering system that groups similar texts together based on their embeddings.\n",
    "\n",
    "**Requirements**:\n",
    "1. Calculate pairwise similarities\n",
    "2. Group texts above a threshold\n",
    "3. Display clusters with their members"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "class SimpleClustering:\n",
    "    \"\"\"\n",
    "    Simple clustering based on similarity threshold.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, extractor, threshold=0.7, pooling='mean'):\n",
    "        self.extractor = extractor\n",
    "        self.threshold = threshold\n",
    "        self.pooling = pooling\n",
    "    \n",
    "    def fit(self, texts):\n",
    "        \"\"\"\n",
    "        Cluster texts based on similarity.\n",
    "        \n",
    "        Returns:\n",
    "            list of clusters, where each cluster is a list of (index, text) tuples\n",
    "        \"\"\"\n",
    "        # Get all embeddings\n",
    "        embeddings = get_embeddings_batch(texts, self.extractor, self.pooling)\n",
    "        \n",
    "        n = len(texts)\n",
    "        assigned = [False] * n\n",
    "        clusters = []\n",
    "        \n",
    "        for i in range(n):\n",
    "            if assigned[i]:\n",
    "                continue\n",
    "            \n",
    "            # Start a new cluster with this text\n",
    "            cluster = [(i, texts[i])]\n",
    "            assigned[i] = True\n",
    "            \n",
    "            # Find all similar texts\n",
    "            for j in range(i + 1, n):\n",
    "                if assigned[j]:\n",
    "                    continue\n",
    "                \n",
    "                sim = cosine_similarity(embeddings[i], embeddings[j])\n",
    "                if sim >= self.threshold:\n",
    "                    cluster.append((j, texts[j]))\n",
    "                    assigned[j] = True\n",
    "            \n",
    "            clusters.append(cluster)\n",
    "        \n",
    "        return clusters\n",
    "    \n",
    "    def format_clusters(self, clusters):\n",
    "        \"\"\"\n",
    "        Format clusters for display.\n",
    "        \"\"\"\n",
    "        lines = []\n",
    "        lines.append(f\"Found {len(clusters)} clusters (threshold: {self.threshold})\")\n",
    "        lines.append(\"=\"*60)\n",
    "        \n",
    "        for i, cluster in enumerate(clusters, 1):\n",
    "            lines.append(f\"\\nCluster {i} ({len(cluster)} items):\")\n",
    "            for idx, text in cluster:\n",
    "                abbrev = text[:55] + '...' if len(text) > 55 else text\n",
    "                lines.append(f\"  [{idx}] {abbrev}\")\n",
    "        \n",
    "        return '\\n'.join(lines)\n",
    "\n",
    "\n",
    "# Test with diverse texts\n",
    "diverse_texts = [\n",
    "    # Technology cluster\n",
    "    \"Python is a popular programming language.\",\n",
    "    \"JavaScript is used for web development.\",\n",
    "    \"Coding in Python is enjoyable.\",\n",
    "    \n",
    "    # Food cluster\n",
    "    \"I love eating pizza.\",\n",
    "    \"Pizza is my favorite food.\",\n",
    "    \"Italian cuisine is delicious.\",\n",
    "    \n",
    "    # Sports cluster\n",
    "    \"Basketball is an exciting sport.\",\n",
    "    \"I enjoy watching football games.\",\n",
    "    \n",
    "    # Weather (singleton)\n",
    "    \"The weather is beautiful today.\",\n",
    "]\n",
    "\n",
    "# Cluster the texts\n",
    "clusterer = SimpleClustering(similarity_extractor, threshold=0.6)\n",
    "clusters = clusterer.fit(diverse_texts)\n",
    "\n",
    "print(clusterer.format_clusters(clusters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment with different thresholds\n",
    "print(\"Threshold Experiment:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for threshold in [0.5, 0.6, 0.7, 0.8]:\n",
    "    clusterer = SimpleClustering(similarity_extractor, threshold=threshold)\n",
    "    clusters = clusterer.fit(diverse_texts)\n",
    "    \n",
    "    multi_member_clusters = [c for c in clusters if len(c) > 1]\n",
    "    singletons = len([c for c in clusters if len(c) == 1])\n",
    "    \n",
    "    print(f\"\\nThreshold: {threshold}\")\n",
    "    print(f\"  Total clusters: {len(clusters)}\")\n",
    "    print(f\"  Multi-member clusters: {len(multi_member_clusters)}\")\n",
    "    print(f\"  Singletons: {singletons}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-46",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 5: Mini-Project\n",
    "\n",
    "## Project: Semantic FAQ Matcher\n",
    "\n",
    "**Scenario**: You're building a customer support system that matches user questions to the most relevant FAQ entries, providing instant answers.\n",
    "\n",
    "**Your goal**: Build a `SemanticFAQMatcher` class that:\n",
    "1. Stores FAQ entries with their answers\n",
    "2. Matches user queries to the best FAQ\n",
    "3. Provides confidence scores\n",
    "4. Handles the case when no good match is found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MINI-PROJECT: Semantic FAQ Matcher\n",
    "# ===================================\n",
    "\n",
    "class SemanticFAQMatcher:\n",
    "    \"\"\"\n",
    "    Matches user questions to FAQ entries using semantic similarity.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, extractor, confidence_threshold=0.6, pooling='mean'):\n",
    "        \"\"\"\n",
    "        Initialize the FAQ matcher.\n",
    "        \n",
    "        Args:\n",
    "            extractor: Feature extraction pipeline\n",
    "            confidence_threshold: Minimum similarity to return a match\n",
    "            pooling: Pooling strategy for embeddings\n",
    "        \"\"\"\n",
    "        self.extractor = extractor\n",
    "        self.confidence_threshold = confidence_threshold\n",
    "        self.pooling = pooling\n",
    "        \n",
    "        self.faqs = []  # List of {question, answer, category}\n",
    "        self.embeddings = None\n",
    "    \n",
    "    def add_faq(self, question, answer, category=\"General\"):\n",
    "        \"\"\"\n",
    "        Add a single FAQ entry.\n",
    "        \"\"\"\n",
    "        self.faqs.append({\n",
    "            'question': question,\n",
    "            'answer': answer,\n",
    "            'category': category\n",
    "        })\n",
    "        self.embeddings = None  # Mark for recalculation\n",
    "    \n",
    "    def add_faqs_bulk(self, faq_list):\n",
    "        \"\"\"\n",
    "        Add multiple FAQ entries.\n",
    "        \n",
    "        Args:\n",
    "            faq_list: List of dicts with 'question', 'answer', and optionally 'category'\n",
    "        \"\"\"\n",
    "        for faq in faq_list:\n",
    "            self.add_faq(\n",
    "                faq['question'], \n",
    "                faq['answer'], \n",
    "                faq.get('category', 'General')\n",
    "            )\n",
    "    \n",
    "    def build_index(self):\n",
    "        \"\"\"\n",
    "        Pre-compute embeddings for all FAQ questions.\n",
    "        \"\"\"\n",
    "        if not self.faqs:\n",
    "            raise ValueError(\"No FAQs added. Use add_faq() first.\")\n",
    "        \n",
    "        questions = [faq['question'] for faq in self.faqs]\n",
    "        self.embeddings = get_embeddings_batch(questions, self.extractor, self.pooling)\n",
    "        \n",
    "        print(f\"Index built with {len(self.faqs)} FAQ entries.\")\n",
    "    \n",
    "    def match(self, user_query, top_k=3):\n",
    "        \"\"\"\n",
    "        Find the best matching FAQ(s) for a user query.\n",
    "        \n",
    "        Args:\n",
    "            user_query: The user's question\n",
    "            top_k: Number of top matches to return\n",
    "            \n",
    "        Returns:\n",
    "            dict with best match info and alternatives\n",
    "        \"\"\"\n",
    "        # Build index if needed\n",
    "        if self.embeddings is None:\n",
    "            self.build_index()\n",
    "        \n",
    "        # Get query embedding\n",
    "        query_emb = get_embedding(user_query, self.extractor, self.pooling)\n",
    "        \n",
    "        # Calculate similarities\n",
    "        similarities = []\n",
    "        for i, faq_emb in enumerate(self.embeddings):\n",
    "            sim = cosine_similarity(query_emb, faq_emb)\n",
    "            similarities.append((i, sim))\n",
    "        \n",
    "        # Sort by similarity\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Get top matches\n",
    "        top_matches = []\n",
    "        for idx, sim in similarities[:top_k]:\n",
    "            top_matches.append({\n",
    "                'faq': self.faqs[idx],\n",
    "                'similarity': sim,\n",
    "                'confident': sim >= self.confidence_threshold\n",
    "            })\n",
    "        \n",
    "        # Determine overall result\n",
    "        best_match = top_matches[0] if top_matches else None\n",
    "        \n",
    "        return {\n",
    "            'query': user_query,\n",
    "            'found_match': best_match['confident'] if best_match else False,\n",
    "            'best_match': best_match,\n",
    "            'alternatives': top_matches[1:] if len(top_matches) > 1 else [],\n",
    "            'all_matches': top_matches,\n",
    "        }\n",
    "    \n",
    "    def get_answer(self, user_query):\n",
    "        \"\"\"\n",
    "        Get the answer to a user's question.\n",
    "        \n",
    "        Returns:\n",
    "            tuple of (answer_text, confidence) or (fallback_message, 0)\n",
    "        \"\"\"\n",
    "        result = self.match(user_query)\n",
    "        \n",
    "        if result['found_match']:\n",
    "            return (\n",
    "                result['best_match']['faq']['answer'],\n",
    "                result['best_match']['similarity']\n",
    "            )\n",
    "        else:\n",
    "            return (\n",
    "                \"I'm not sure about that. Would you like to speak with a support agent?\",\n",
    "                0.0\n",
    "            )\n",
    "    \n",
    "    def format_response(self, result):\n",
    "        \"\"\"\n",
    "        Format match result as a user-friendly response.\n",
    "        \"\"\"\n",
    "        lines = []\n",
    "        lines.append(\"┌\" + \"─\"*68 + \"┐\")\n",
    "        lines.append(f\"│ User Query: {result['query'][:52]:52s} │\")\n",
    "        lines.append(\"├\" + \"─\"*68 + \"┤\")\n",
    "        \n",
    "        if result['found_match']:\n",
    "            best = result['best_match']\n",
    "            lines.append(f\"│ ✓ Match Found (Confidence: {best['similarity']:.1%}){' '*30} │\")\n",
    "            lines.append(\"│\" + \" \"*68 + \"│\")\n",
    "            lines.append(f\"│ Category: {best['faq']['category']:57s} │\")\n",
    "            \n",
    "            # Wrap question\n",
    "            q = best['faq']['question']\n",
    "            lines.append(f\"│ Q: {q[:63]:63s} │\")\n",
    "            \n",
    "            # Wrap answer\n",
    "            a = best['faq']['answer']\n",
    "            lines.append(\"│\" + \" \"*68 + \"│\")\n",
    "            lines.append(f\"│ A: {a[:63]:63s} │\")\n",
    "            if len(a) > 63:\n",
    "                lines.append(f\"│    {a[63:126]:63s} │\")\n",
    "        else:\n",
    "            lines.append(f\"│ ✗ No confident match found{' '*40} │\")\n",
    "            lines.append(\"│\" + \" \"*68 + \"│\")\n",
    "            if result['all_matches']:\n",
    "                lines.append(f\"│ Closest match ({result['all_matches'][0]['similarity']:.1%} confidence):{' '*26} │\")\n",
    "                q = result['all_matches'][0]['faq']['question']\n",
    "                lines.append(f\"│   {q[:63]:63s} │\")\n",
    "        \n",
    "        # Show alternatives\n",
    "        if result['alternatives']:\n",
    "            lines.append(\"├\" + \"─\"*68 + \"┤\")\n",
    "            lines.append(f\"│ Other possibilities:{' '*46} │\")\n",
    "            for alt in result['alternatives'][:2]:\n",
    "                q = alt['faq']['question'][:50]\n",
    "                lines.append(f\"│   • {q:50s} ({alt['similarity']:.0%}) │\")\n",
    "        \n",
    "        lines.append(\"└\" + \"─\"*68 + \"┘\")\n",
    "        \n",
    "        return '\\n'.join(lines)\n",
    "\n",
    "\n",
    "# Create FAQ matcher\n",
    "matcher = SemanticFAQMatcher(similarity_extractor, confidence_threshold=0.65)\n",
    "\n",
    "print(\"Semantic FAQ Matcher initialized!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add sample FAQs\n",
    "sample_faqs = [\n",
    "    {\n",
    "        'question': 'How do I reset my password?',\n",
    "        'answer': 'Go to Settings > Account > Change Password. You can also use the \"Forgot Password\" link on the login page.',\n",
    "        'category': 'Account'\n",
    "    },\n",
    "    {\n",
    "        'question': 'What payment methods do you accept?',\n",
    "        'answer': 'We accept Visa, Mastercard, American Express, PayPal, and Apple Pay.',\n",
    "        'category': 'Billing'\n",
    "    },\n",
    "    {\n",
    "        'question': 'How long does shipping take?',\n",
    "        'answer': 'Standard shipping takes 5-7 business days. Express shipping delivers in 2-3 business days.',\n",
    "        'category': 'Shipping'\n",
    "    },\n",
    "    {\n",
    "        'question': 'What is your return policy?',\n",
    "        'answer': 'We offer free returns within 30 days of purchase. Items must be in original condition with tags attached.',\n",
    "        'category': 'Returns'\n",
    "    },\n",
    "    {\n",
    "        'question': 'How do I track my order?',\n",
    "        'answer': 'Log into your account and go to \"My Orders\" to see tracking information. You\\'ll also receive tracking emails.',\n",
    "        'category': 'Shipping'\n",
    "    },\n",
    "    {\n",
    "        'question': 'Do you offer international shipping?',\n",
    "        'answer': 'Yes! We ship to over 100 countries. International shipping typically takes 7-14 business days.',\n",
    "        'category': 'Shipping'\n",
    "    },\n",
    "    {\n",
    "        'question': 'How do I contact customer support?',\n",
    "        'answer': 'Email us at support@example.com or call 1-800-123-4567. Live chat is available 9 AM - 5 PM EST.',\n",
    "        'category': 'Support'\n",
    "    },\n",
    "    {\n",
    "        'question': 'How do I cancel my subscription?',\n",
    "        'answer': 'Go to Settings > Subscription > Cancel. Your access continues until the end of your billing period.',\n",
    "        'category': 'Account'\n",
    "    },\n",
    "    {\n",
    "        'question': 'Is there a mobile app?',\n",
    "        'answer': 'Yes! Download our free app from the App Store (iOS) or Google Play Store (Android).',\n",
    "        'category': 'General'\n",
    "    },\n",
    "    {\n",
    "        'question': 'How do I update my email address?',\n",
    "        'answer': 'Go to Settings > Account > Edit Profile. Click on your email address to change it.',\n",
    "        'category': 'Account'\n",
    "    },\n",
    "]\n",
    "\n",
    "matcher.add_faqs_bulk(sample_faqs)\n",
    "matcher.build_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with user queries\n",
    "test_queries = [\n",
    "    \"I forgot my password\",\n",
    "    \"where's my package?\",\n",
    "    \"can I pay with credit card?\",\n",
    "    \"I want to send something back\",\n",
    "    \"do you have an iPhone app?\",\n",
    "    \"what's the meaning of life?\",  # Should not match\n",
    "]\n",
    "\n",
    "print(\"FAQ Matcher Test Results:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for query in test_queries:\n",
    "    result = matcher.match(query)\n",
    "    print(\"\\n\" + matcher.format_response(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple chatbot interface\n",
    "def chatbot_response(user_input):\n",
    "    \"\"\"\n",
    "    Generate a chatbot response using the FAQ matcher.\n",
    "    \"\"\"\n",
    "    answer, confidence = matcher.get_answer(user_input)\n",
    "    \n",
    "    response = []\n",
    "    response.append(f\"User: {user_input}\")\n",
    "    response.append(\"\")\n",
    "    \n",
    "    if confidence > 0:\n",
    "        response.append(f\"Bot: {answer}\")\n",
    "        response.append(f\"     (Confidence: {confidence:.0%})\")\n",
    "    else:\n",
    "        response.append(f\"Bot: {answer}\")\n",
    "    \n",
    "    return '\\n'.join(response)\n",
    "\n",
    "\n",
    "# Simulate a conversation\n",
    "print(\"Customer Support Chatbot\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "conversation = [\n",
    "    \"Hi, I need help with my account\",\n",
    "    \"How can I change my password?\",\n",
    "    \"What if I want a refund?\",\n",
    "    \"Thanks for the help!\",\n",
    "]\n",
    "\n",
    "for msg in conversation:\n",
    "    print(\"\\n\" + chatbot_response(msg))\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try your own queries\n",
    "# Uncomment and modify:\n",
    "\n",
    "# my_query = \"Your question here\"\n",
    "# result = matcher.match(my_query)\n",
    "# print(matcher.format_response(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-52",
   "metadata": {},
   "source": [
    "### Extension Ideas\n",
    "\n",
    "If you want to extend this project further:\n",
    "\n",
    "1. **Synonyms handling**: Add alternate phrasings for each FAQ\n",
    "2. **Category filtering**: Let users filter by category\n",
    "3. **Feedback loop**: Track which answers are helpful to improve matching\n",
    "4. **Multi-turn context**: Remember conversation history\n",
    "5. **Hybrid search**: Combine semantic search with keyword matching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-53",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 6: Wrap-Up\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Embeddings represent text as numbers** that capture semantic meaning - similar texts have similar vectors\n",
    "\n",
    "2. **Pooling combines token vectors** into sentence vectors:\n",
    "   - Mean pooling (average) is usually best for similarity\n",
    "   - CLS pooling uses the special [CLS] token\n",
    "   - Max pooling captures the strongest signals\n",
    "\n",
    "3. **Cosine similarity** measures how alike two vectors are (0-1 scale)\n",
    "\n",
    "4. **Sentence-transformer models** are specifically trained for similarity tasks and work better than base models\n",
    "\n",
    "5. **Applications are everywhere**: search, duplicate detection, FAQ matching, recommendations, clustering\n",
    "\n",
    "## Common Mistakes to Avoid\n",
    "\n",
    "| Mistake | Why It's a Problem |\n",
    "|---------|-------------------|\n",
    "| Using base models for similarity | They're not optimized for this task |\n",
    "| Ignoring the threshold | Low-confidence matches may be wrong |\n",
    "| Not pre-computing embeddings | Slow for repeated searches |\n",
    "| Using CLS for similarity | Mean pooling usually works better |\n",
    "| Very long texts without chunking | Information gets lost |\n",
    "\n",
    "## What's Next?\n",
    "\n",
    "In **Notebook 9: Sentiment with Different Models**, you'll learn:\n",
    "- How different models approach sentiment analysis\n",
    "- Comparing binary vs. multi-class sentiment\n",
    "- Understanding model training data influence\n",
    "\n",
    "The embedding concepts from this notebook will help you understand why different sentiment models produce different results!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-54",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Solutions\n",
    "\n",
    "### Check Your Understanding (Quiz Answers)\n",
    "\n",
    "1. **B) To represent text as numbers that capture meaning** - Embeddings convert text into numerical vectors\n",
    "2. **B) The angle between two vectors** - Cosine similarity ignores magnitude, focusing on direction\n",
    "3. **B) They are very similar** - 0.95 indicates high similarity (close to 1.0)\n",
    "4. **A) Combining multiple token vectors into one sentence vector** - Pooling aggregates token-level representations\n",
    "\n",
    "### Exercise 2: Key Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key insights from duplicate detection:\n",
    "\n",
    "# 1. Threshold selection matters:\n",
    "#    - Too high (>0.9): Misses near-duplicates\n",
    "#    - Too low (<0.6): False positives\n",
    "#    - Sweet spot: 0.75-0.85 for most use cases\n",
    "\n",
    "# 2. Semantic vs. lexical duplicates:\n",
    "#    - \"How to reset password\" ↔ \"Forgot my password\" (semantic match)\n",
    "#    - \"Reset password\" ↔ \"Reset password\" (lexical match)\n",
    "#    - Embeddings catch semantic matches that keywords miss!\n",
    "\n",
    "# 3. Performance at scale:\n",
    "#    - Pre-compute embeddings once\n",
    "#    - Use efficient similarity search (FAISS, Annoy)\n",
    "#    - Consider approximate nearest neighbors for millions of docs\n",
    "\n",
    "threshold_guide = {\n",
    "    'Strict (0.90+)': 'Nearly identical content only',\n",
    "    'Standard (0.80)': 'Clear duplicates, different wording',\n",
    "    'Relaxed (0.70)': 'Same topic, related content',\n",
    "    'Loose (0.60)': 'Broadly similar themes',\n",
    "}\n",
    "\n",
    "print(\"Duplicate Detection Threshold Guide:\")\n",
    "print(\"=\"*50)\n",
    "for level, desc in threshold_guide.items():\n",
    "    print(f\"  {level:20s} → {desc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-56",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "- [Sentence-Transformers Documentation](https://www.sbert.net/)\n",
    "- [Hugging Face Embeddings Guide](https://huggingface.co/blog/getting-started-with-embeddings)\n",
    "- [Understanding Cosine Similarity](https://en.wikipedia.org/wiki/Cosine_similarity)\n",
    "- [FAISS Library](https://github.com/facebookresearch/faiss) - Efficient similarity search\n",
    "- [Vector Databases Comparison](https://www.pinecone.io/learn/vector-database/)\n",
    "- [Semantic Search Tutorial](https://www.sbert.net/examples/applications/semantic-search/README.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}